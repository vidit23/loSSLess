{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17ef0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import lightly\n",
    "import lightly.models as models\n",
    "import lightly.loss as loss\n",
    "import lightly.data as data\n",
    "from lightly.models.barlowtwins import BarlowTwins\n",
    "from lightly.models.simclr import SimCLR\n",
    "\n",
    "from simclr.modules.transformations import TransformsSimCLR\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "\n",
    "import resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a8c70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointDir = 'barlow-34'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cfdfd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split, transform, limit=0):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            root: Location of the dataset folder, usually it is /dataset\n",
    "            split: The split you want to used, it should be one of train, val or unlabeled.\n",
    "            transform: the transform you want to applied to the images.\n",
    "        \"\"\"\n",
    "\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_dir = os.path.join(root, split)\n",
    "        label_path = os.path.join(root, f\"{split}_label_tensor.pt\")\n",
    "\n",
    "        if limit == 0:\n",
    "            self.num_images = len(os.listdir(self.image_dir))\n",
    "        else:\n",
    "            self.num_images = limit\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            self.labels = torch.load(label_path)\n",
    "        else:\n",
    "            self.labels = -1 * torch.ones(self.num_images, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(os.path.join(self.image_dir, f\"{idx}.png\"), 'rb') as f:\n",
    "            img = Image.open(f).convert('RGB')\n",
    "            \n",
    "        if self.transform == None:\n",
    "            return img, self.labels[idx]            \n",
    "\n",
    "        return self.transform(img), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74985659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlur(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            sigma = random.random() * 1.9 + 0.1\n",
    "            return img.filter(ImageFilter.GaussianBlur(sigma))\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "\n",
    "class Solarization(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            return ImageOps.solarize(img)\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3061970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform:\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(96, interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                        saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            GaussianBlur(p=1.0),\n",
    "            Solarization(p=0.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.transform_prime = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(96, interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                        saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            GaussianBlur(p=0.1),\n",
    "            Solarization(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y1 = self.transform(x)\n",
    "        y2 = self.transform_prime(x)\n",
    "        return y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da242578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LARS(optim.Optimizer):\n",
    "    def __init__(self, params, lr, weight_decay=0, momentum=0.9, eta=0.001,\n",
    "                 weight_decay_filter=None, lars_adaptation_filter=None):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum,\n",
    "                        eta=eta, weight_decay_filter=weight_decay_filter,\n",
    "                        lars_adaptation_filter=lars_adaptation_filter)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for g in self.param_groups:\n",
    "            for p in g['params']:\n",
    "                dp = p.grad\n",
    "\n",
    "                if dp is None:\n",
    "                    continue\n",
    "\n",
    "                if g['weight_decay_filter'] is None or not g['weight_decay_filter'](p):\n",
    "                    dp = dp.add(p, alpha=g['weight_decay'])\n",
    "\n",
    "                if g['lars_adaptation_filter'] is None or not g['lars_adaptation_filter'](p):\n",
    "                    param_norm = torch.norm(p)\n",
    "                    update_norm = torch.norm(dp)\n",
    "                    one = torch.ones_like(param_norm)\n",
    "                    q = torch.where(param_norm > 0.,\n",
    "                                    torch.where(update_norm > 0,\n",
    "                                                (g['eta'] * param_norm / update_norm), one), one)\n",
    "                    dp = dp.mul(q)\n",
    "\n",
    "                param_state = self.state[p]\n",
    "                if 'mu' not in param_state:\n",
    "                    param_state['mu'] = torch.zeros_like(p)\n",
    "                mu = param_state['mu']\n",
    "                mu.mul_(g['momentum']).add_(dp)\n",
    "\n",
    "                p.add_(mu, alpha=-g['lr'])\n",
    "\n",
    "\n",
    "def exclude_bias_and_norm(p):\n",
    "    return p.ndim == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fcfc2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset from your image folder\n",
    "dataset = CustomDataset(root='/dataset', split='unlabeled', transform=Transform())\n",
    "\n",
    "# build a PyTorch dataloader\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1024, shuffle=True, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "efed9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarlowTwins(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.resnet34(zero_init_residual=True)\n",
    "#         self.backbone = resnet.get_custom_resnet34()\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        # projector\n",
    "        sizes = [512] + list(map(int, '1024-1024-1024'.split('-')))\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=False))\n",
    "            layers.append(nn.BatchNorm1d(sizes[i + 1]))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Linear(sizes[-2], sizes[-1], bias=False))\n",
    "        self.projector = nn.Sequential(*layers)\n",
    "\n",
    "        # normalization layer for the representations z1 and z2\n",
    "        self.bn = nn.BatchNorm1d(sizes[-1], affine=False)\n",
    "\n",
    "    def forward(self, y1, y2):\n",
    "        z1 = self.projector(self.backbone(y1))\n",
    "        z2 = self.projector(self.backbone(y2))\n",
    "\n",
    "        # empirical cross-correlation matrix\n",
    "        c = self.bn(z1).T @ self.bn(z2)\n",
    "\n",
    "        # sum the cross-correlation matrix between all gpus\n",
    "        c.div_(1024)\n",
    "#         torch.distributed.all_reduce(c)\n",
    "\n",
    "        # use --scale-loss to multiply the loss by a constant factor\n",
    "        # see the Issues section of the readme\n",
    "        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum().mul(1/32)\n",
    "        off_diag = off_diagonal(c).pow_(2).sum().mul(1/32)\n",
    "        loss = on_diag + 3.9e-3 * off_diag\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b836e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    # return a flattened view of the off-diagonal elements of a square matrix\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "944c8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, loader, step):\n",
    "    max_steps = 300 * len(loader)\n",
    "    warmup_steps = 10 * len(loader)\n",
    "    base_lr = 0.2 * 1024 / 256\n",
    "    if step < warmup_steps:\n",
    "        lr = base_lr * step / warmup_steps\n",
    "    else:\n",
    "        step -= warmup_steps\n",
    "        max_steps -= warmup_steps\n",
    "        q = 0.5 * (1 + math.cos(math.pi * step / max_steps))\n",
    "        end_lr = base_lr * 0.001\n",
    "        lr = base_lr * q + end_lr * (1 - q)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "220d2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = BarlowTwins().cuda()\n",
    "# model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "optimizer = LARS(model.parameters(), lr=0, weight_decay=1e-6,\n",
    "                 weight_decay_filter=exclude_bias_and_norm,\n",
    "                 lars_adaptation_filter=exclude_bias_and_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee441fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically resume from checkpoint if it exists\n",
    "if os.path.isfile('/scratch/vvb238/' + checkpointDir + '/checkpoint.pth'):\n",
    "    ckpt = torch.load('/scratch/vvb238/' + checkpointDir + '/checkpoint.pth',\n",
    "                      map_location='cpu')\n",
    "    start_epoch = ckpt['epoch']\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "else:\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410a2ed",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in range(start_epoch, 300):\n",
    "#     sampler.set_epoch(epoch)\n",
    "    for step, ((y1, y2), _) in enumerate(loader, start=epoch * len(loader)):\n",
    "        y1 = y1.cuda()\n",
    "        y2 = y2.cuda()\n",
    "        lr = adjust_learning_rate(optimizer, loader, step)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss = model.forward(y1, y2)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if step % 10 == 0:\n",
    "#             torch.distributed.reduce(loss.div_(args.world_size), 0)\n",
    "#             if args.rank == 0:\n",
    "            stats = dict(epoch=epoch, step=step, learning_rate=lr,\n",
    "                         loss=loss.item(),\n",
    "                         time=int(time.time() - start_time))\n",
    "            print(json.dumps(stats))\n",
    "#                 print(json.dumps(stats), file=stats_file)\n",
    "        # save checkpoint\n",
    "    state = dict(epoch=epoch + 1, model=model.state_dict(),\n",
    "                 optimizer=optimizer.state_dict())\n",
    "    torch.save(state, '/scratch/vvb238/' + checkpointDir + '/checkpoint.pth')\n",
    "    \n",
    "torch.save(model.backbone.state_dict(),\n",
    "           '/scratch/vvb238/' + checkpointDir + '/resnet50.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8039162d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc9ff3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS PART TAKES THE MODEL IN THE MIDDLE AND USES IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "739a333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlur(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            sigma = random.random() * 1.9 + 0.1\n",
    "            return img.filter(ImageFilter.GaussianBlur(sigma))\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "\n",
    "class Solarization(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            return ImageOps.solarize(img)\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f649916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYUImageNetDataModule(pl.LightningDataModule):\n",
    "  \n",
    "    def train_dataloader(self):\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(96, interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                        saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            GaussianBlur(p=1.0),\n",
    "            Solarization(p=0.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        trainset = CustomDataset(root='/dataset', split=\"train\", transform=train_transform)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        eval_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        evalset = CustomDataset(root='/dataset', split=\"val\", transform=eval_transform)\n",
    "        eval_loader = torch.utils.data.DataLoader(evalset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        return eval_loader\n",
    "    \n",
    "nyudata = NYUImageNetDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "98574a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('/scratch/vvb238/' + checkpointDir + '/checkpoint.pth'):\n",
    "    ckpt = torch.load('/scratch/vvb238/' + checkpointDir + '/checkpoint.pth',\n",
    "                      map_location='cpu')\n",
    "    model.load_state_dict(ckpt['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e07afdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0183ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simclr.modules.identity import Identity\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "class ResNetClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.resnet34(zero_init_residual=True)\n",
    "#         self.backbone = resnet.get_custom_resnet34()\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.backbone.load_state_dict(model.backbone.state_dict())\n",
    "        \n",
    "        self.lastLayer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(1024, 800),\n",
    "        )\n",
    "#         self.lastLayer = torch.nn.Linear(512, 800)\n",
    "        for layer in self.lastLayer.modules():\n",
    "           if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.01)\n",
    "                layer.bias.data.zero_()\n",
    "        \n",
    "        self.param_groups = [dict(params=self.lastLayer.parameters(), lr=0.01)]\n",
    "        self.param_groups.append(dict(params=model.parameters(), lr=0.0005))\n",
    "        \n",
    "        self.criterion=torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "#         x = self.relu(self.projector(x))\n",
    "        x = self.lastLayer(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, label = batch\n",
    "        classProbs = self.forward(data)\n",
    "        loss = self.criterion(classProbs, label)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def _evaluate(self, batch, batch_idx, stage=None):\n",
    "        x, y = batch\n",
    "        out = self.forward(x)\n",
    "        logits = F.log_softmax(out, dim=-1)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f'{stage}_loss', loss, prog_bar=True)\n",
    "            self.log(f'{stage}_acc', acc, prog_bar=True)\n",
    "\n",
    "        return loss, acc\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        self._evaluate(batch, batch_idx, 'val')[0]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.param_groups, 0, momentum=0.9, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS, verbose=True)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d4e61a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 60\n",
    "classifier = ResNetClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a4cfc1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', save_last=True)\n",
    "classifier_trainer = Trainer(gpus=1,deterministic=True, max_epochs=EPOCHS, default_root_dir='/scratch/vvb238/classifier-' + checkpointDir, profiler=\"simple\",\n",
    "                     limit_val_batches= 0.5, benchmark=True, callbacks=[checkpoint_callback], fast_dev_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "00a5d964",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Adjusting learning rate of group 1 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | backbone  | ResNet           | 21.3 M\n",
      "1 | lastLayer | Sequential       | 1.3 M \n",
      "2 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "22.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.6 M    Total params\n",
      "90.520    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  67%|██████▋   | 201/300 [00:19<00:09, 10.05it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  68%|██████▊   | 203/300 [00:20<00:09,  9.90it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  69%|██████▉   | 207/300 [00:20<00:09, 10.04it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Validating:   7%|▋         | 7/100 [00:00<00:06, 14.62it/s]\u001b[A\n",
      "Epoch 0:  70%|███████   | 211/300 [00:20<00:08, 10.14it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  72%|███████▏  | 215/300 [00:20<00:08, 10.27it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  73%|███████▎  | 219/300 [00:21<00:07, 10.40it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  74%|███████▍  | 223/300 [00:21<00:07, 10.53it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  76%|███████▌  | 227/300 [00:21<00:06, 10.64it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  77%|███████▋  | 231/300 [00:21<00:06, 10.77it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  78%|███████▊  | 235/300 [00:21<00:05, 10.88it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  80%|███████▉  | 239/300 [00:21<00:05, 10.99it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  81%|████████  | 243/300 [00:21<00:05, 11.10it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  82%|████████▏ | 247/300 [00:22<00:04, 11.21it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  84%|████████▎ | 251/300 [00:22<00:04, 11.33it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  85%|████████▌ | 255/300 [00:22<00:03, 11.44it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Validating:  55%|█████▌    | 55/100 [00:02<00:01, 29.50it/s]\u001b[A\n",
      "Epoch 0:  86%|████████▋ | 259/300 [00:22<00:03, 11.55it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  88%|████████▊ | 263/300 [00:22<00:03, 11.65it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  89%|████████▉ | 268/300 [00:22<00:02, 11.81it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  91%|█████████ | 273/300 [00:22<00:02, 11.94it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  93%|█████████▎| 278/300 [00:23<00:01, 12.06it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  94%|█████████▍| 283/300 [00:23<00:01, 12.18it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Validating:  83%|████████▎ | 83/100 [00:03<00:00, 28.74it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 288/300 [00:23<00:00, 12.30it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  98%|█████████▊| 293/300 [00:23<00:00, 12.43it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Epoch 0:  99%|█████████▉| 298/300 [00:23<00:00, 12.55it/s, loss=6.54, v_num=2, val_loss=6.700, val_acc=0.000]\n",
      "Validating: 100%|██████████| 100/100 [00:03<00:00, 31.84it/s]\u001b[AAdjusting learning rate of group 0 to 9.9931e-03.\n",
      "Adjusting learning rate of group 1 to 4.9966e-04.\n",
      "Epoch 0: 100%|██████████| 300/300 [00:23<00:00, 12.51it/s, loss=6.54, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Epoch 1:  67%|██████▋   | 200/300 [00:19<00:09, 10.15it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 205/300 [00:20<00:09, 10.09it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Validating:   5%|▌         | 5/100 [00:00<00:09,  9.86it/s]\u001b[A\n",
      "Epoch 1:  70%|███████   | 210/300 [00:20<00:08, 10.26it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Epoch 1:  72%|███████▏  | 215/300 [00:20<00:08, 10.43it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Epoch 1:  73%|███████▎  | 220/300 [00:20<00:07, 10.58it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Epoch 1:  75%|███████▌  | 225/300 [00:20<00:06, 10.76it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Validating:  25%|██▌       | 25/100 [00:01<00:02, 27.84it/s]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 230/300 [00:21<00:06, 10.87it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Epoch 1:  78%|███████▊  | 235/300 [00:21<00:05, 11.05it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Epoch 1:  80%|████████  | 240/300 [00:21<00:05, 11.15it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Epoch 1:  82%|████████▏ | 245/300 [00:21<00:04, 11.29it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Validating:  46%|████▌     | 46/100 [00:02<00:02, 26.92it/s]\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 250/300 [00:21<00:04, 11.44it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Epoch 1:  85%|████████▌ | 255/300 [00:22<00:03, 11.58it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Validating:  55%|█████▌    | 55/100 [00:02<00:01, 27.89it/s]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 260/300 [00:22<00:03, 11.70it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Epoch 1:  88%|████████▊ | 265/300 [00:22<00:02, 11.84it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Epoch 1:  90%|█████████ | 270/300 [00:22<00:02, 11.98it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Validating:  70%|███████   | 70/100 [00:02<00:00, 30.43it/s]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 275/300 [00:22<00:02, 12.11it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Epoch 1:  93%|█████████▎| 280/300 [00:22<00:01, 12.24it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Validating:  81%|████████  | 81/100 [00:03<00:00, 26.62it/s]\u001b[A\n",
      "Epoch 1:  95%|█████████▌| 285/300 [00:23<00:01, 12.34it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Epoch 1:  97%|█████████▋| 290/300 [00:23<00:00, 12.49it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Epoch 1:  98%|█████████▊| 295/300 [00:23<00:00, 12.59it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Epoch 1: 100%|██████████| 300/300 [00:23<00:00, 12.72it/s, loss=5.86, v_num=2, val_loss=6.440, val_acc=0.0634]\n",
      "Validating: 100%|██████████| 100/100 [00:03<00:00, 28.98it/s]\u001b[AAdjusting learning rate of group 0 to 9.9726e-03.\n",
      "Adjusting learning rate of group 1 to 4.9863e-04.\n",
      "Epoch 1: 100%|██████████| 300/300 [00:23<00:00, 12.60it/s, loss=5.86, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2:  67%|██████▋   | 200/300 [00:19<00:09, 10.07it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 205/300 [00:20<00:09, 10.01it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Validating:   5%|▌         | 5/100 [00:00<00:09, 10.07it/s]\u001b[A\n",
      "Epoch 2:  70%|███████   | 210/300 [00:20<00:08, 10.18it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2:  72%|███████▏  | 215/300 [00:20<00:08, 10.33it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2:  73%|███████▎  | 220/300 [00:20<00:07, 10.50it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2:  75%|███████▌  | 225/300 [00:21<00:07, 10.64it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2:  77%|███████▋  | 230/300 [00:21<00:06, 10.81it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Validating:  30%|███       | 30/100 [00:01<00:02, 29.28it/s]\u001b[A\n",
      "Epoch 2:  78%|███████▊  | 235/300 [00:21<00:05, 10.92it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2:  80%|████████  | 240/300 [00:21<00:05, 11.08it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2:  82%|████████▏ | 245/300 [00:21<00:04, 11.22it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2:  83%|████████▎ | 250/300 [00:22<00:04, 11.36it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Validating:  50%|█████     | 50/100 [00:02<00:01, 26.88it/s]\u001b[A\n",
      "Epoch 2:  85%|████████▌ | 255/300 [00:22<00:03, 11.48it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2:  87%|████████▋ | 260/300 [00:22<00:03, 11.64it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2:  88%|████████▊ | 265/300 [00:22<00:02, 11.75it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2:  90%|█████████ | 270/300 [00:22<00:02, 11.88it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Validating:  70%|███████   | 70/100 [00:02<00:01, 27.73it/s]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 275/300 [00:22<00:02, 12.01it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2:  93%|█████████▎| 280/300 [00:23<00:01, 12.15it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2:  95%|█████████▌| 285/300 [00:23<00:01, 12.27it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2:  97%|█████████▋| 290/300 [00:23<00:00, 12.42it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Validating:  90%|█████████ | 90/100 [00:03<00:00, 28.06it/s]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 295/300 [00:23<00:00, 12.49it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]\n",
      "Epoch 2: 100%|██████████| 300/300 [00:23<00:00, 12.62it/s, loss=5.17, v_num=2, val_loss=5.540, val_acc=0.0694]Adjusting learning rate of group 0 to 9.9384e-03.\n",
      "Adjusting learning rate of group 1 to 4.9692e-04.\n",
      "Epoch 2: 100%|██████████| 300/300 [00:23<00:00, 12.52it/s, loss=5.17, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  67%|██████▋   | 200/300 [00:20<00:10,  9.91it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/100 [00:00<00:44,  2.23it/s]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 205/300 [00:20<00:09,  9.84it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  70%|███████   | 210/300 [00:20<00:08, 10.01it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  72%|███████▏  | 215/300 [00:21<00:08, 10.13it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Validating:  15%|█▌        | 15/100 [00:01<00:04, 20.40it/s]\u001b[A\n",
      "Epoch 3:  73%|███████▎  | 220/300 [00:21<00:07, 10.29it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  75%|███████▌  | 225/300 [00:21<00:07, 10.46it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  77%|███████▋  | 230/300 [00:21<00:06, 10.59it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  78%|███████▊  | 235/300 [00:21<00:06, 10.76it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  80%|████████  | 240/300 [00:22<00:05, 10.90it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Validating:  40%|████      | 40/100 [00:01<00:02, 27.61it/s]\u001b[A\n",
      "Epoch 3:  82%|████████▏ | 245/300 [00:22<00:04, 11.03it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  83%|████████▎ | 250/300 [00:22<00:04, 11.16it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  85%|████████▌ | 255/300 [00:22<00:03, 11.31it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  87%|████████▋ | 260/300 [00:22<00:03, 11.42it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Validating:  60%|██████    | 60/100 [00:02<00:01, 27.42it/s]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 265/300 [00:22<00:03, 11.55it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  90%|█████████ | 270/300 [00:23<00:02, 11.71it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  92%|█████████▏| 275/300 [00:23<00:02, 11.81it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Validating:  75%|███████▌  | 75/100 [00:03<00:00, 27.57it/s]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 280/300 [00:23<00:01, 11.95it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  95%|█████████▌| 285/300 [00:23<00:01, 12.06it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  97%|█████████▋| 290/300 [00:23<00:00, 12.19it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Epoch 3:  98%|█████████▊| 295/300 [00:23<00:00, 12.30it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 28.02it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 300/300 [00:24<00:00, 12.42it/s, loss=4.86, v_num=2, val_loss=4.870, val_acc=0.0981]Adjusting learning rate of group 0 to 9.8907e-03.\n",
      "Adjusting learning rate of group 1 to 4.9454e-04.\n",
      "Epoch 3: 100%|██████████| 300/300 [00:24<00:00, 12.31it/s, loss=4.86, v_num=2, val_loss=4.490, val_acc=0.132] \n",
      "Epoch 4:  67%|██████▋   | 200/300 [00:19<00:09, 10.10it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  68%|██████▊   | 205/300 [00:20<00:09, 10.05it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Validating:   6%|▌         | 6/100 [00:00<00:07, 12.10it/s]\u001b[A\n",
      "Epoch 4:  70%|███████   | 210/300 [00:20<00:08, 10.21it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Epoch 4:  72%|███████▏  | 215/300 [00:20<00:08, 10.35it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Validating:  15%|█▌        | 15/100 [00:00<00:04, 20.56it/s]\u001b[A\n",
      "Epoch 4:  73%|███████▎  | 220/300 [00:20<00:07, 10.51it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Epoch 4:  75%|███████▌  | 225/300 [00:21<00:07, 10.67it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Epoch 4:  77%|███████▋  | 230/300 [00:21<00:06, 10.82it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Validating:  30%|███       | 30/100 [00:01<00:02, 28.02it/s]\u001b[A\n",
      "Epoch 4:  78%|███████▊  | 235/300 [00:21<00:05, 10.97it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Epoch 4:  80%|████████  | 240/300 [00:21<00:05, 11.12it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Epoch 4:  82%|████████▏ | 245/300 [00:21<00:04, 11.27it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Epoch 4:  83%|████████▎ | 250/300 [00:21<00:04, 11.40it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Validating:  50%|█████     | 50/100 [00:02<00:01, 29.88it/s]\u001b[A\n",
      "Epoch 4:  85%|████████▌ | 255/300 [00:22<00:03, 11.55it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Epoch 4:  87%|████████▋ | 260/300 [00:22<00:03, 11.67it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Validating:  61%|██████    | 61/100 [00:02<00:01, 28.72it/s]\u001b[A\n",
      "Epoch 4:  88%|████████▊ | 265/300 [00:22<00:02, 11.80it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Epoch 4:  90%|█████████ | 270/300 [00:22<00:02, 11.94it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Epoch 4:  92%|█████████▏| 275/300 [00:22<00:02, 12.05it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Epoch 4:  93%|█████████▎| 280/300 [00:22<00:01, 12.21it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Epoch 4:  95%|█████████▌| 285/300 [00:23<00:01, 12.31it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 27.25it/s]\u001b[A\n",
      "Epoch 4:  97%|█████████▋| 290/300 [00:23<00:00, 12.43it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Epoch 4:  98%|█████████▊| 295/300 [00:23<00:00, 12.56it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 29.90it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 300/300 [00:23<00:00, 12.69it/s, loss=4.68, v_num=2, val_loss=4.490, val_acc=0.132]Adjusting learning rate of group 0 to 9.8296e-03.\n",
      "Adjusting learning rate of group 1 to 4.9148e-04.\n",
      "Epoch 4: 100%|██████████| 300/300 [00:23<00:00, 12.56it/s, loss=4.68, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Epoch 5:  67%|██████▋   | 200/300 [00:19<00:09, 10.10it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  68%|██████▊   | 205/300 [00:20<00:09, 10.03it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Validating:   5%|▌         | 5/100 [00:00<00:09,  9.77it/s]\u001b[A\n",
      "Epoch 5:  70%|███████   | 210/300 [00:20<00:08, 10.19it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Epoch 5:  72%|███████▏  | 215/300 [00:20<00:08, 10.38it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Epoch 5:  73%|███████▎  | 220/300 [00:20<00:07, 10.50it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Validating:  20%|██        | 20/100 [00:01<00:03, 22.70it/s]\u001b[A\n",
      "Epoch 5:  75%|███████▌  | 225/300 [00:21<00:07, 10.66it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Epoch 5:  77%|███████▋  | 230/300 [00:21<00:06, 10.81it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Epoch 5:  78%|███████▊  | 235/300 [00:21<00:05, 10.96it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Validating:  35%|███▌      | 35/100 [00:01<00:02, 26.78it/s]\u001b[A\n",
      "Epoch 5:  80%|████████  | 240/300 [00:21<00:05, 11.09it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Epoch 5:  82%|████████▏ | 245/300 [00:21<00:04, 11.25it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Epoch 5:  83%|████████▎ | 250/300 [00:21<00:04, 11.38it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Epoch 5:  85%|████████▌ | 255/300 [00:22<00:03, 11.52it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Validating:  55%|█████▌    | 55/100 [00:02<00:01, 28.88it/s]\u001b[A\n",
      "Epoch 5:  87%|████████▋ | 260/300 [00:22<00:03, 11.64it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Epoch 5:  88%|████████▊ | 265/300 [00:22<00:02, 11.79it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Validating:  65%|██████▌   | 65/100 [00:02<00:01, 28.94it/s]\u001b[A\n",
      "Epoch 5:  90%|█████████ | 270/300 [00:22<00:02, 11.91it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Epoch 5:  92%|█████████▏| 275/300 [00:22<00:02, 12.04it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Validating:  75%|███████▌  | 75/100 [00:03<00:00, 27.95it/s]\u001b[A\n",
      "Epoch 5:  93%|█████████▎| 280/300 [00:22<00:01, 12.19it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Epoch 5:  95%|█████████▌| 285/300 [00:23<00:01, 12.30it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Epoch 5:  97%|█████████▋| 290/300 [00:23<00:00, 12.42it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Epoch 5:  98%|█████████▊| 295/300 [00:23<00:00, 12.56it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 29.70it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 300/300 [00:23<00:00, 12.66it/s, loss=4.47, v_num=2, val_loss=4.280, val_acc=0.154]Adjusting learning rate of group 0 to 9.7553e-03.\n",
      "Adjusting learning rate of group 1 to 4.8776e-04.\n",
      "Epoch 5: 100%|██████████| 300/300 [00:23<00:00, 12.54it/s, loss=4.47, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6:  67%|██████▋   | 200/300 [00:19<00:09, 10.12it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  68%|██████▊   | 205/300 [00:20<00:09, 10.06it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6:  70%|███████   | 210/300 [00:20<00:08, 10.24it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Validating:  10%|█         | 10/100 [00:00<00:05, 16.46it/s]\u001b[A\n",
      "Epoch 6:  72%|███████▏  | 215/300 [00:20<00:08, 10.37it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6:  73%|███████▎  | 220/300 [00:20<00:07, 10.54it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Validating:  21%|██        | 21/100 [00:01<00:03, 25.15it/s]\u001b[A\n",
      "Epoch 6:  75%|███████▌  | 225/300 [00:21<00:07, 10.67it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6:  77%|███████▋  | 230/300 [00:21<00:06, 10.84it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6:  78%|███████▊  | 235/300 [00:21<00:05, 10.98it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6:  80%|████████  | 240/300 [00:21<00:05, 11.11it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Validating:  40%|████      | 40/100 [00:01<00:02, 26.96it/s]\u001b[A\n",
      "Epoch 6:  82%|████████▏ | 245/300 [00:21<00:04, 11.27it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6:  83%|████████▎ | 250/300 [00:21<00:04, 11.39it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6:  85%|████████▌ | 255/300 [00:22<00:03, 11.55it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6:  87%|████████▋ | 260/300 [00:22<00:03, 11.67it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6:  88%|████████▊ | 265/300 [00:22<00:02, 11.83it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Validating:  65%|██████▌   | 65/100 [00:02<00:01, 29.51it/s]\u001b[A\n",
      "Epoch 6:  90%|█████████ | 270/300 [00:22<00:02, 11.93it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6:  92%|█████████▏| 275/300 [00:22<00:02, 12.07it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6:  93%|█████████▎| 280/300 [00:22<00:01, 12.20it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Validating:  81%|████████  | 81/100 [00:03<00:00, 29.27it/s]\u001b[A\n",
      "Epoch 6:  95%|█████████▌| 285/300 [00:23<00:01, 12.32it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6:  97%|█████████▋| 290/300 [00:23<00:00, 12.46it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6:  98%|█████████▊| 295/300 [00:23<00:00, 12.56it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Epoch 6: 100%|██████████| 300/300 [00:23<00:00, 12.68it/s, loss=4.36, v_num=2, val_loss=4.130, val_acc=0.171]\n",
      "Validating: 100%|██████████| 100/100 [00:03<00:00, 28.17it/s]\u001b[AAdjusting learning rate of group 0 to 9.6679e-03.\n",
      "Adjusting learning rate of group 1 to 4.8340e-04.\n",
      "Epoch 6: 100%|██████████| 300/300 [00:23<00:00, 12.55it/s, loss=4.36, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7:  67%|██████▋   | 200/300 [00:19<00:09, 10.16it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  68%|██████▊   | 205/300 [00:20<00:09, 10.15it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7:  70%|███████   | 210/300 [00:20<00:08, 10.28it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Validating:  10%|█         | 10/100 [00:00<00:05, 17.03it/s]\u001b[A\n",
      "Epoch 7:  72%|███████▏  | 215/300 [00:20<00:08, 10.42it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7:  73%|███████▎  | 220/300 [00:20<00:07, 10.59it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Validating:  21%|██        | 21/100 [00:01<00:03, 24.44it/s]\u001b[A\n",
      "Epoch 7:  75%|███████▌  | 225/300 [00:21<00:07, 10.71it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7:  77%|███████▋  | 230/300 [00:21<00:06, 10.88it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7:  78%|███████▊  | 235/300 [00:21<00:05, 11.03it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Validating:  35%|███▌      | 35/100 [00:01<00:02, 28.82it/s]\u001b[A\n",
      "Epoch 7:  80%|████████  | 240/300 [00:21<00:05, 11.17it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7:  82%|████████▏ | 245/300 [00:21<00:04, 11.35it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7:  83%|████████▎ | 250/300 [00:21<00:04, 11.45it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7:  85%|████████▌ | 255/300 [00:22<00:03, 11.59it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7:  87%|████████▋ | 260/300 [00:22<00:03, 11.72it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Validating:  60%|██████    | 60/100 [00:02<00:01, 28.05it/s]\u001b[A\n",
      "Epoch 7:  88%|████████▊ | 265/300 [00:22<00:02, 11.84it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7:  90%|█████████ | 270/300 [00:22<00:02, 12.01it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7:  92%|█████████▏| 275/300 [00:22<00:02, 12.10it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7:  93%|█████████▎| 280/300 [00:22<00:01, 12.24it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7:  95%|█████████▌| 285/300 [00:23<00:01, 12.36it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 28.30it/s]\u001b[A\n",
      "Epoch 7:  97%|█████████▋| 290/300 [00:23<00:00, 12.49it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7:  98%|█████████▊| 295/300 [00:23<00:00, 12.63it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]\n",
      "Epoch 7: 100%|██████████| 300/300 [00:23<00:00, 12.74it/s, loss=4.26, v_num=2, val_loss=4.040, val_acc=0.175]Adjusting learning rate of group 0 to 9.5677e-03.\n",
      "Adjusting learning rate of group 1 to 4.7839e-04.\n",
      "Epoch 7: 100%|██████████| 300/300 [00:23<00:00, 12.61it/s, loss=4.26, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8:  67%|██████▋   | 200/300 [00:19<00:09, 10.05it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  68%|██████▊   | 205/300 [00:20<00:09, 10.00it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8:  70%|███████   | 210/300 [00:20<00:08, 10.19it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Validating:  10%|█         | 10/100 [00:00<00:05, 17.71it/s]\u001b[A\n",
      "Epoch 8:  72%|███████▏  | 215/300 [00:20<00:08, 10.31it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8:  73%|███████▎  | 220/300 [00:21<00:07, 10.47it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Validating:  20%|██        | 20/100 [00:01<00:03, 22.68it/s]\u001b[A\n",
      "Epoch 8:  75%|███████▌  | 225/300 [00:21<00:07, 10.62it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8:  77%|███████▋  | 230/300 [00:21<00:06, 10.77it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8:  78%|███████▊  | 235/300 [00:21<00:05, 10.93it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8:  80%|████████  | 240/300 [00:21<00:05, 11.08it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Validating:  40%|████      | 40/100 [00:01<00:02, 27.74it/s]\u001b[A\n",
      "Epoch 8:  82%|████████▏ | 245/300 [00:21<00:04, 11.20it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8:  83%|████████▎ | 250/300 [00:22<00:04, 11.35it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8:  85%|████████▌ | 255/300 [00:22<00:03, 11.52it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Validating:  55%|█████▌    | 55/100 [00:02<00:01, 32.70it/s]\u001b[A\n",
      "Epoch 8:  87%|████████▋ | 260/300 [00:22<00:03, 11.62it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8:  88%|████████▊ | 265/300 [00:22<00:02, 11.78it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8:  90%|█████████ | 270/300 [00:22<00:02, 11.89it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8:  92%|█████████▏| 275/300 [00:22<00:02, 12.03it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Validating:  75%|███████▌  | 75/100 [00:02<00:00, 29.44it/s]\u001b[A\n",
      "Epoch 8:  93%|█████████▎| 280/300 [00:23<00:01, 12.15it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8:  95%|█████████▌| 285/300 [00:23<00:01, 12.28it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8:  97%|█████████▋| 290/300 [00:23<00:00, 12.41it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8:  98%|█████████▊| 295/300 [00:23<00:00, 12.56it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Epoch 8: 100%|██████████| 300/300 [00:23<00:00, 12.64it/s, loss=4.22, v_num=2, val_loss=3.950, val_acc=0.194]\n",
      "Validating: 100%|██████████| 100/100 [00:03<00:00, 29.56it/s]\u001b[AAdjusting learning rate of group 0 to 9.4550e-03.\n",
      "Adjusting learning rate of group 1 to 4.7275e-04.\n",
      "Epoch 8: 100%|██████████| 300/300 [00:23<00:00, 12.52it/s, loss=4.22, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9:  67%|██████▋   | 200/300 [00:19<00:09, 10.16it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  68%|██████▊   | 205/300 [00:20<00:09, 10.11it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Validating:   6%|▌         | 6/100 [00:00<00:07, 12.31it/s]\u001b[A\n",
      "Epoch 9:  70%|███████   | 210/300 [00:20<00:08, 10.25it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9:  72%|███████▏  | 215/300 [00:20<00:08, 10.43it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9:  73%|███████▎  | 220/300 [00:20<00:07, 10.57it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Validating:  20%|██        | 20/100 [00:01<00:03, 23.95it/s]\u001b[A\n",
      "Epoch 9:  75%|███████▌  | 225/300 [00:20<00:06, 10.73it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9:  77%|███████▋  | 230/300 [00:21<00:06, 10.88it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Validating:  30%|███       | 30/100 [00:01<00:02, 26.77it/s]\u001b[A\n",
      "Epoch 9:  78%|███████▊  | 235/300 [00:21<00:05, 11.03it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9:  80%|████████  | 240/300 [00:21<00:05, 11.17it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9:  82%|████████▏ | 245/300 [00:21<00:04, 11.30it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9:  83%|████████▎ | 250/300 [00:21<00:04, 11.46it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9:  85%|████████▌ | 255/300 [00:21<00:03, 11.60it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9:  87%|████████▋ | 260/300 [00:22<00:03, 11.74it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Validating:  60%|██████    | 60/100 [00:02<00:01, 27.19it/s]\u001b[A\n",
      "Epoch 9:  88%|████████▊ | 265/300 [00:22<00:02, 11.86it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9:  90%|█████████ | 270/300 [00:22<00:02, 12.02it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9:  92%|█████████▏| 275/300 [00:22<00:02, 12.11it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9:  93%|█████████▎| 280/300 [00:22<00:01, 12.24it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9:  95%|█████████▌| 285/300 [00:23<00:01, 12.36it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 28.26it/s]\u001b[A\n",
      "Epoch 9:  97%|█████████▋| 290/300 [00:23<00:00, 12.48it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9:  98%|█████████▊| 295/300 [00:23<00:00, 12.61it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]\n",
      "Epoch 9: 100%|██████████| 300/300 [00:23<00:00, 12.75it/s, loss=4.18, v_num=2, val_loss=3.900, val_acc=0.198]Adjusting learning rate of group 0 to 9.3301e-03.\n",
      "Adjusting learning rate of group 1 to 4.6651e-04.\n",
      "Epoch 9: 100%|██████████| 300/300 [00:23<00:00, 12.62it/s, loss=4.18, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Epoch 10:  67%|██████▋   | 200/300 [00:20<00:10,  9.99it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  68%|██████▊   | 205/300 [00:20<00:09,  9.93it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Validating:   5%|▌         | 5/100 [00:00<00:09,  9.68it/s]\u001b[A\n",
      "Epoch 10:  70%|███████   | 210/300 [00:20<00:08, 10.09it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Epoch 10:  72%|███████▏  | 215/300 [00:20<00:08, 10.24it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Epoch 10:  73%|███████▎  | 220/300 [00:21<00:07, 10.41it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Validating:  20%|██        | 20/100 [00:01<00:03, 24.45it/s]\u001b[A\n",
      "Epoch 10:  75%|███████▌  | 225/300 [00:21<00:07, 10.56it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Epoch 10:  77%|███████▋  | 230/300 [00:21<00:06, 10.72it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Epoch 10:  78%|███████▊  | 235/300 [00:21<00:05, 10.86it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Epoch 10:  80%|████████  | 240/300 [00:21<00:05, 11.00it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Validating:  40%|████      | 40/100 [00:01<00:02, 28.09it/s]\u001b[A\n",
      "Epoch 10:  82%|████████▏ | 245/300 [00:21<00:04, 11.14it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Epoch 10:  83%|████████▎ | 250/300 [00:22<00:04, 11.28it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Validating:  50%|█████     | 50/100 [00:02<00:01, 29.52it/s]\u001b[A\n",
      "Epoch 10:  85%|████████▌ | 255/300 [00:22<00:03, 11.42it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Epoch 10:  87%|████████▋ | 260/300 [00:22<00:03, 11.54it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Validating:  60%|██████    | 60/100 [00:02<00:01, 27.47it/s]\u001b[A\n",
      "Epoch 10:  88%|████████▊ | 265/300 [00:22<00:02, 11.67it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Epoch 10:  90%|█████████ | 270/300 [00:22<00:02, 11.81it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Validating:  71%|███████   | 71/100 [00:02<00:01, 26.48it/s]\u001b[A\n",
      "Epoch 10:  92%|█████████▏| 275/300 [00:23<00:02, 11.92it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Epoch 10:  93%|█████████▎| 280/300 [00:23<00:01, 12.05it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Epoch 10:  95%|█████████▌| 285/300 [00:23<00:01, 12.18it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Epoch 10:  97%|█████████▋| 290/300 [00:23<00:00, 12.30it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Epoch 10:  98%|█████████▊| 295/300 [00:23<00:00, 12.43it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 30.52it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 300/300 [00:23<00:00, 12.54it/s, loss=4.08, v_num=2, val_loss=3.870, val_acc=0.202]Adjusting learning rate of group 0 to 9.1934e-03.\n",
      "Adjusting learning rate of group 1 to 4.5967e-04.\n",
      "Epoch 10: 100%|██████████| 300/300 [00:24<00:00, 12.41it/s, loss=4.08, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Epoch 11:  67%|██████▋   | 200/300 [00:19<00:09, 10.06it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  68%|██████▊   | 205/300 [00:20<00:09, 10.02it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Validating:   6%|▌         | 6/100 [00:00<00:07, 12.20it/s]\u001b[A\n",
      "Epoch 11:  70%|███████   | 210/300 [00:20<00:08, 10.18it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Epoch 11:  72%|███████▏  | 215/300 [00:20<00:08, 10.32it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Epoch 11:  73%|███████▎  | 220/300 [00:20<00:07, 10.49it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Validating:  20%|██        | 20/100 [00:01<00:03, 25.90it/s]\u001b[A\n",
      "Epoch 11:  75%|███████▌  | 225/300 [00:21<00:07, 10.65it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Epoch 11:  77%|███████▋  | 230/300 [00:21<00:06, 10.77it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Epoch 11:  78%|███████▊  | 235/300 [00:21<00:05, 10.93it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Validating:  35%|███▌      | 35/100 [00:01<00:02, 26.68it/s]\u001b[A\n",
      "Epoch 11:  80%|████████  | 240/300 [00:21<00:05, 11.07it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Epoch 11:  82%|████████▏ | 245/300 [00:21<00:04, 11.21it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Epoch 11:  83%|████████▎ | 250/300 [00:22<00:04, 11.35it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Validating:  50%|█████     | 50/100 [00:02<00:01, 28.94it/s]\u001b[A\n",
      "Epoch 11:  85%|████████▌ | 255/300 [00:22<00:03, 11.49it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Epoch 11:  87%|████████▋ | 260/300 [00:22<00:03, 11.64it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Epoch 11:  88%|████████▊ | 265/300 [00:22<00:02, 11.77it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Epoch 11:  90%|█████████ | 270/300 [00:22<00:02, 11.91it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Epoch 11:  92%|█████████▏| 275/300 [00:22<00:02, 12.06it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Validating:  75%|███████▌  | 75/100 [00:03<00:00, 28.09it/s]\u001b[A\n",
      "Epoch 11:  93%|█████████▎| 280/300 [00:23<00:01, 12.13it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Epoch 11:  95%|█████████▌| 285/300 [00:23<00:01, 12.26it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Epoch 11:  97%|█████████▋| 290/300 [00:23<00:00, 12.37it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Validating:  90%|█████████ | 90/100 [00:03<00:00, 26.37it/s]\u001b[A\n",
      "Epoch 11:  98%|█████████▊| 295/300 [00:23<00:00, 12.51it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]\n",
      "Epoch 11: 100%|██████████| 300/300 [00:23<00:00, 12.62it/s, loss=4.06, v_num=2, val_loss=3.800, val_acc=0.214]Adjusting learning rate of group 0 to 9.0451e-03.\n",
      "Adjusting learning rate of group 1 to 4.5225e-04.\n",
      "Epoch 11: 100%|██████████| 300/300 [00:24<00:00, 12.48it/s, loss=4.06, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Epoch 12:  67%|██████▋   | 200/300 [00:19<00:09, 10.07it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/100 [00:00<00:43,  2.30it/s]\u001b[A\n",
      "Epoch 12:  68%|██████▊   | 205/300 [00:20<00:09, 10.04it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Epoch 12:  70%|███████   | 210/300 [00:20<00:08, 10.18it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Validating:  10%|█         | 10/100 [00:00<00:05, 17.94it/s]\u001b[A\n",
      "Epoch 12:  72%|███████▏  | 215/300 [00:20<00:08, 10.35it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Epoch 12:  73%|███████▎  | 220/300 [00:20<00:07, 10.50it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Validating:  21%|██        | 21/100 [00:01<00:02, 26.49it/s]\u001b[A\n",
      "Epoch 12:  75%|███████▌  | 225/300 [00:21<00:07, 10.65it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Epoch 12:  77%|███████▋  | 230/300 [00:21<00:06, 10.79it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Epoch 12:  78%|███████▊  | 235/300 [00:21<00:05, 10.95it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Validating:  35%|███▌      | 35/100 [00:01<00:02, 28.42it/s]\u001b[A\n",
      "Epoch 12:  80%|████████  | 240/300 [00:21<00:05, 11.08it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Epoch 12:  82%|████████▏ | 245/300 [00:21<00:04, 11.23it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Validating:  45%|████▌     | 45/100 [00:01<00:01, 29.01it/s]\u001b[A\n",
      "Epoch 12:  83%|████████▎ | 250/300 [00:21<00:04, 11.37it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Epoch 12:  85%|████████▌ | 255/300 [00:22<00:03, 11.54it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Epoch 12:  87%|████████▋ | 260/300 [00:22<00:03, 11.64it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Epoch 12:  88%|████████▊ | 265/300 [00:22<00:02, 11.78it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Epoch 12:  90%|█████████ | 270/300 [00:22<00:02, 11.93it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Epoch 12:  92%|█████████▏| 275/300 [00:22<00:02, 12.04it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Validating:  75%|███████▌  | 75/100 [00:03<00:00, 29.23it/s]\u001b[A\n",
      "Epoch 12:  93%|█████████▎| 280/300 [00:23<00:01, 12.15it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Epoch 12:  95%|█████████▌| 285/300 [00:23<00:01, 12.28it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Epoch 12:  97%|█████████▋| 290/300 [00:23<00:00, 12.39it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Validating:  90%|█████████ | 90/100 [00:03<00:00, 27.89it/s]\u001b[A\n",
      "Epoch 12:  98%|█████████▊| 295/300 [00:23<00:00, 12.52it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]\n",
      "Epoch 12: 100%|██████████| 300/300 [00:23<00:00, 12.64it/s, loss=3.96, v_num=2, val_loss=3.770, val_acc=0.223]Adjusting learning rate of group 0 to 8.8857e-03.\n",
      "Adjusting learning rate of group 1 to 4.4429e-04.\n",
      "Epoch 12: 100%|██████████| 300/300 [00:23<00:00, 12.52it/s, loss=3.96, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Epoch 13:  67%|██████▋   | 200/300 [00:19<00:09, 10.10it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  68%|██████▊   | 205/300 [00:20<00:09, 10.04it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Validating:   6%|▌         | 6/100 [00:00<00:07, 12.08it/s]\u001b[A\n",
      "Epoch 13:  70%|███████   | 210/300 [00:20<00:08, 10.21it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Epoch 13:  72%|███████▏  | 215/300 [00:20<00:08, 10.35it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Epoch 13:  73%|███████▎  | 220/300 [00:20<00:07, 10.51it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Validating:  20%|██        | 20/100 [00:01<00:03, 24.84it/s]\u001b[A\n",
      "Epoch 13:  75%|███████▌  | 225/300 [00:21<00:07, 10.65it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Epoch 13:  77%|███████▋  | 230/300 [00:21<00:06, 10.83it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Epoch 13:  78%|███████▊  | 235/300 [00:21<00:05, 10.95it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Validating:  35%|███▌      | 35/100 [00:01<00:02, 27.12it/s]\u001b[A\n",
      "Epoch 13:  80%|████████  | 240/300 [00:21<00:05, 11.10it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Epoch 13:  82%|████████▏ | 245/300 [00:21<00:04, 11.25it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Epoch 13:  83%|████████▎ | 250/300 [00:21<00:04, 11.38it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Epoch 13:  85%|████████▌ | 255/300 [00:22<00:03, 11.51it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Epoch 13:  87%|████████▋ | 260/300 [00:22<00:03, 11.68it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Validating:  60%|██████    | 60/100 [00:02<00:01, 31.86it/s]\u001b[A\n",
      "Epoch 13:  88%|████████▊ | 265/300 [00:22<00:02, 11.78it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Epoch 13:  90%|█████████ | 270/300 [00:22<00:02, 11.92it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Epoch 13:  92%|█████████▏| 275/300 [00:22<00:02, 12.06it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Epoch 13:  93%|█████████▎| 280/300 [00:23<00:01, 12.15it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Validating:  80%|████████  | 80/100 [00:03<00:00, 26.70it/s]\u001b[A\n",
      "Epoch 13:  95%|█████████▌| 285/300 [00:23<00:01, 12.30it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Epoch 13:  97%|█████████▋| 290/300 [00:23<00:00, 12.43it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Epoch 13:  98%|█████████▊| 295/300 [00:23<00:00, 12.54it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]\n",
      "Validating:  96%|█████████▌| 96/100 [00:03<00:00, 27.52it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 300/300 [00:23<00:00, 12.64it/s, loss=3.9, v_num=2, val_loss=3.760, val_acc=0.223]Adjusting learning rate of group 0 to 8.7157e-03.\n",
      "Adjusting learning rate of group 1 to 4.3579e-04.\n",
      "Epoch 13: 100%|██████████| 300/300 [00:23<00:00, 12.52it/s, loss=3.9, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14:  67%|██████▋   | 200/300 [00:19<00:09, 10.12it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  68%|██████▊   | 205/300 [00:20<00:09, 10.05it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Validating:   5%|▌         | 5/100 [00:00<00:09,  9.65it/s]\u001b[A\n",
      "Epoch 14:  70%|███████   | 210/300 [00:20<00:08, 10.21it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14:  72%|███████▏  | 215/300 [00:20<00:08, 10.40it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14:  73%|███████▎  | 220/300 [00:20<00:07, 10.54it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14:  75%|███████▌  | 225/300 [00:21<00:07, 10.69it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Validating:  25%|██▌       | 25/100 [00:01<00:02, 26.51it/s]\u001b[A\n",
      "Epoch 14:  77%|███████▋  | 230/300 [00:21<00:06, 10.83it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14:  78%|███████▊  | 235/300 [00:21<00:05, 11.01it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14:  80%|████████  | 240/300 [00:21<00:05, 11.12it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14:  82%|████████▏ | 245/300 [00:21<00:04, 11.29it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14:  83%|████████▎ | 250/300 [00:21<00:04, 11.39it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14:  85%|████████▌ | 255/300 [00:22<00:03, 11.56it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Validating:  55%|█████▌    | 55/100 [00:02<00:01, 30.43it/s]\u001b[A\n",
      "Epoch 14:  87%|████████▋ | 260/300 [00:22<00:03, 11.69it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14:  88%|████████▊ | 265/300 [00:22<00:02, 11.82it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14:  90%|█████████ | 270/300 [00:22<00:02, 11.94it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Validating:  71%|███████   | 71/100 [00:02<00:01, 28.29it/s]\u001b[A\n",
      "Epoch 14:  92%|█████████▏| 275/300 [00:22<00:02, 12.07it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14:  93%|█████████▎| 280/300 [00:22<00:01, 12.21it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14:  95%|█████████▌| 285/300 [00:23<00:01, 12.35it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14:  97%|█████████▋| 290/300 [00:23<00:00, 12.45it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Validating:  90%|█████████ | 90/100 [00:03<00:00, 27.96it/s]\u001b[A\n",
      "Epoch 14:  98%|█████████▊| 295/300 [00:23<00:00, 12.56it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Epoch 14: 100%|██████████| 300/300 [00:23<00:00, 12.69it/s, loss=3.88, v_num=2, val_loss=3.710, val_acc=0.232]\n",
      "Validating: 100%|██████████| 100/100 [00:03<00:00, 28.86it/s]\u001b[AAdjusting learning rate of group 0 to 8.5355e-03.\n",
      "Adjusting learning rate of group 1 to 4.2678e-04.\n",
      "Epoch 14: 100%|██████████| 300/300 [00:23<00:00, 12.55it/s, loss=3.88, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Epoch 15:  67%|██████▋   | 200/300 [00:20<00:10, 10.00it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  68%|██████▊   | 205/300 [00:20<00:09,  9.95it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Epoch 15:  70%|███████   | 210/300 [00:20<00:08, 10.13it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Validating:  10%|█         | 10/100 [00:00<00:05, 15.58it/s]\u001b[A\n",
      "Epoch 15:  72%|███████▏  | 215/300 [00:20<00:08, 10.25it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Epoch 15:  73%|███████▎  | 220/300 [00:21<00:07, 10.41it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Epoch 15:  75%|███████▌  | 225/300 [00:21<00:07, 10.59it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Epoch 15:  77%|███████▋  | 230/300 [00:21<00:06, 10.73it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Epoch 15:  78%|███████▊  | 235/300 [00:21<00:05, 10.88it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Validating:  35%|███▌      | 35/100 [00:01<00:02, 30.64it/s]\u001b[A\n",
      "Epoch 15:  80%|████████  | 240/300 [00:21<00:05, 11.01it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Epoch 15:  82%|████████▏ | 245/300 [00:22<00:04, 11.12it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Epoch 15:  83%|████████▎ | 250/300 [00:22<00:04, 11.27it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Validating:  50%|█████     | 50/100 [00:02<00:01, 28.20it/s]\u001b[A\n",
      "Epoch 15:  85%|████████▌ | 255/300 [00:22<00:03, 11.41it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Epoch 15:  87%|████████▋ | 260/300 [00:22<00:03, 11.53it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Validating:  61%|██████    | 61/100 [00:02<00:01, 28.26it/s]\u001b[A\n",
      "Epoch 15:  88%|████████▊ | 265/300 [00:22<00:03, 11.67it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Epoch 15:  90%|█████████ | 270/300 [00:22<00:02, 11.81it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Validating:  71%|███████   | 71/100 [00:02<00:01, 28.15it/s]\u001b[A\n",
      "Epoch 15:  92%|█████████▏| 275/300 [00:23<00:02, 11.92it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Epoch 15:  93%|█████████▎| 280/300 [00:23<00:01, 12.05it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Epoch 15:  95%|█████████▌| 285/300 [00:23<00:01, 12.20it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 28.62it/s]\u001b[A\n",
      "Epoch 15:  97%|█████████▋| 290/300 [00:23<00:00, 12.30it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Epoch 15:  98%|█████████▊| 295/300 [00:23<00:00, 12.42it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 28.92it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 300/300 [00:23<00:00, 12.54it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.239]Adjusting learning rate of group 0 to 8.3457e-03.\n",
      "Adjusting learning rate of group 1 to 4.1728e-04.\n",
      "Epoch 15: 100%|██████████| 300/300 [00:24<00:00, 12.42it/s, loss=3.85, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16:  67%|██████▋   | 200/300 [00:20<00:10, 10.00it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  68%|██████▊   | 205/300 [00:20<00:09,  9.94it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16:  70%|███████   | 210/300 [00:20<00:08, 10.13it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Validating:  10%|█         | 10/100 [00:00<00:05, 16.41it/s]\u001b[A\n",
      "Epoch 16:  72%|███████▏  | 215/300 [00:20<00:08, 10.26it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16:  73%|███████▎  | 220/300 [00:21<00:07, 10.41it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Validating:  20%|██        | 20/100 [00:01<00:03, 24.30it/s]\u001b[A\n",
      "Epoch 16:  75%|███████▌  | 225/300 [00:21<00:07, 10.55it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16:  77%|███████▋  | 230/300 [00:21<00:06, 10.72it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16:  78%|███████▊  | 235/300 [00:21<00:05, 10.85it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16:  80%|████████  | 240/300 [00:21<00:05, 11.00it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Validating:  40%|████      | 40/100 [00:01<00:02, 27.65it/s]\u001b[A\n",
      "Epoch 16:  82%|████████▏ | 245/300 [00:21<00:04, 11.16it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16:  83%|████████▎ | 250/300 [00:22<00:04, 11.27it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16:  85%|████████▌ | 255/300 [00:22<00:03, 11.42it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16:  87%|████████▋ | 260/300 [00:22<00:03, 11.55it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16:  88%|████████▊ | 265/300 [00:22<00:02, 11.72it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Validating:  65%|██████▌   | 65/100 [00:02<00:01, 33.71it/s]\u001b[A\n",
      "Epoch 16:  90%|█████████ | 270/300 [00:22<00:02, 11.83it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16:  92%|█████████▏| 275/300 [00:23<00:02, 11.94it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16:  93%|█████████▎| 280/300 [00:23<00:01, 12.07it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16:  95%|█████████▌| 285/300 [00:23<00:01, 12.19it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16:  97%|█████████▋| 290/300 [00:23<00:00, 12.31it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Validating:  90%|█████████ | 90/100 [00:03<00:00, 27.73it/s]\u001b[A\n",
      "Epoch 16:  98%|█████████▊| 295/300 [00:23<00:00, 12.43it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]\n",
      "Epoch 16: 100%|██████████| 300/300 [00:23<00:00, 12.56it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.236]Adjusting learning rate of group 0 to 8.1466e-03.\n",
      "Adjusting learning rate of group 1 to 4.0733e-04.\n",
      "Epoch 16: 100%|██████████| 300/300 [00:24<00:00, 12.45it/s, loss=3.86, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Epoch 17:  67%|██████▋   | 200/300 [00:20<00:10,  9.98it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  68%|██████▊   | 205/300 [00:20<00:09,  9.92it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Validating:   5%|▌         | 5/100 [00:00<00:09,  9.56it/s]\u001b[A\n",
      "Epoch 17:  70%|███████   | 210/300 [00:20<00:08, 10.08it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Epoch 17:  72%|███████▏  | 215/300 [00:20<00:08, 10.25it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Epoch 17:  73%|███████▎  | 220/300 [00:21<00:07, 10.41it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Epoch 17:  75%|███████▌  | 225/300 [00:21<00:07, 10.58it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Epoch 17:  77%|███████▋  | 230/300 [00:21<00:06, 10.70it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Epoch 17:  78%|███████▊  | 235/300 [00:21<00:05, 10.87it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Validating:  35%|███▌      | 35/100 [00:01<00:02, 30.12it/s]\u001b[A\n",
      "Epoch 17:  80%|████████  | 240/300 [00:21<00:05, 11.01it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Epoch 17:  82%|████████▏ | 245/300 [00:22<00:04, 11.13it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Epoch 17:  83%|████████▎ | 250/300 [00:22<00:04, 11.27it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Validating:  50%|█████     | 50/100 [00:02<00:01, 28.04it/s]\u001b[A\n",
      "Epoch 17:  85%|████████▌ | 255/300 [00:22<00:03, 11.41it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Epoch 17:  87%|████████▋ | 260/300 [00:22<00:03, 11.55it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Validating:  60%|██████    | 60/100 [00:02<00:01, 28.61it/s]\u001b[A\n",
      "Epoch 17:  88%|████████▊ | 265/300 [00:22<00:03, 11.67it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Epoch 17:  90%|█████████ | 270/300 [00:22<00:02, 11.82it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Validating:  71%|███████   | 71/100 [00:02<00:01, 28.53it/s]\u001b[A\n",
      "Epoch 17:  92%|█████████▏| 275/300 [00:23<00:02, 11.92it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Epoch 17:  93%|█████████▎| 280/300 [00:23<00:01, 12.07it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Epoch 17:  95%|█████████▌| 285/300 [00:23<00:01, 12.20it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Epoch 17:  97%|█████████▋| 290/300 [00:23<00:00, 12.32it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Epoch 17:  98%|█████████▊| 295/300 [00:23<00:00, 12.45it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 27.52it/s]\u001b[A\n",
      "Epoch 17: 100%|██████████| 300/300 [00:23<00:00, 12.54it/s, loss=3.78, v_num=2, val_loss=3.690, val_acc=0.237]Adjusting learning rate of group 0 to 7.9389e-03.\n",
      "Adjusting learning rate of group 1 to 3.9695e-04.\n",
      "Epoch 17: 100%|██████████| 300/300 [00:24<00:00, 12.43it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Epoch 18:  67%|██████▋   | 200/300 [00:19<00:09, 10.05it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/100 [00:00<00:46,  2.14it/s]\u001b[A\n",
      "Epoch 18:  68%|██████▊   | 205/300 [00:20<00:09, 10.00it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Epoch 18:  70%|███████   | 210/300 [00:20<00:08, 10.16it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Validating:  10%|█         | 10/100 [00:00<00:05, 16.76it/s]\u001b[A\n",
      "Epoch 18:  72%|███████▏  | 215/300 [00:20<00:08, 10.31it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Epoch 18:  73%|███████▎  | 220/300 [00:21<00:07, 10.47it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Epoch 18:  75%|███████▌  | 225/300 [00:21<00:07, 10.62it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Validating:  25%|██▌       | 25/100 [00:01<00:02, 26.80it/s]\u001b[A\n",
      "Epoch 18:  77%|███████▋  | 230/300 [00:21<00:06, 10.77it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Epoch 18:  78%|███████▊  | 235/300 [00:21<00:05, 10.92it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Epoch 18:  80%|████████  | 240/300 [00:21<00:05, 11.07it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Validating:  40%|████      | 40/100 [00:01<00:02, 28.98it/s]\u001b[A\n",
      "Epoch 18:  82%|████████▏ | 245/300 [00:21<00:04, 11.20it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Epoch 18:  83%|████████▎ | 250/300 [00:22<00:04, 11.35it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Epoch 18:  85%|████████▌ | 255/300 [00:22<00:03, 11.49it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Epoch 18:  87%|████████▋ | 260/300 [00:22<00:03, 11.64it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Validating:  60%|██████    | 60/100 [00:02<00:01, 27.72it/s]\u001b[A\n",
      "Epoch 18:  88%|████████▊ | 265/300 [00:22<00:02, 11.74it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Epoch 18:  90%|█████████ | 270/300 [00:22<00:02, 11.88it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Epoch 18:  92%|█████████▏| 275/300 [00:22<00:02, 12.01it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Validating:  75%|███████▌  | 75/100 [00:03<00:00, 30.05it/s]\u001b[A\n",
      "Epoch 18:  93%|█████████▎| 280/300 [00:23<00:01, 12.14it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Epoch 18:  95%|█████████▌| 285/300 [00:23<00:01, 12.28it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Epoch 18:  97%|█████████▋| 290/300 [00:23<00:00, 12.37it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Validating:  90%|█████████ | 90/100 [00:03<00:00, 27.49it/s]\u001b[A\n",
      "Epoch 18:  98%|█████████▊| 295/300 [00:23<00:00, 12.51it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]\n",
      "Epoch 18: 100%|██████████| 300/300 [00:23<00:00, 12.61it/s, loss=3.78, v_num=2, val_loss=3.650, val_acc=0.248]Adjusting learning rate of group 0 to 7.7232e-03.\n",
      "Adjusting learning rate of group 1 to 3.8616e-04.\n",
      "Epoch 18: 100%|██████████| 300/300 [00:23<00:00, 12.50it/s, loss=3.78, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Epoch 19:  67%|██████▋   | 200/300 [00:19<00:09, 10.01it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  68%|██████▊   | 205/300 [00:20<00:09,  9.98it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Validating:   5%|▌         | 5/100 [00:00<00:09,  9.65it/s]\u001b[A\n",
      "Epoch 19:  70%|███████   | 210/300 [00:20<00:08, 10.12it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Epoch 19:  72%|███████▏  | 215/300 [00:20<00:08, 10.27it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Epoch 19:  73%|███████▎  | 220/300 [00:21<00:07, 10.44it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Epoch 19:  75%|███████▌  | 225/300 [00:21<00:07, 10.58it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Validating:  25%|██▌       | 25/100 [00:01<00:02, 25.42it/s]\u001b[A\n",
      "Epoch 19:  77%|███████▋  | 230/300 [00:21<00:06, 10.74it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Validating:  31%|███       | 31/100 [00:01<00:02, 27.26it/s]\u001b[A\n",
      "Epoch 19:  78%|███████▊  | 235/300 [00:21<00:05, 10.87it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Epoch 19:  80%|████████  | 240/300 [00:21<00:05, 11.02it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Epoch 19:  82%|████████▏ | 245/300 [00:21<00:04, 11.16it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Validating:  45%|████▌     | 45/100 [00:01<00:01, 28.36it/s]\u001b[A\n",
      "Epoch 19:  83%|████████▎ | 250/300 [00:22<00:04, 11.30it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Epoch 19:  85%|████████▌ | 255/300 [00:22<00:03, 11.45it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Epoch 19:  87%|████████▋ | 260/300 [00:22<00:03, 11.58it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Epoch 19:  88%|████████▊ | 265/300 [00:22<00:02, 11.72it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Epoch 19:  90%|█████████ | 270/300 [00:22<00:02, 11.88it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Validating:  70%|███████   | 70/100 [00:02<00:01, 27.07it/s]\u001b[A\n",
      "Epoch 19:  92%|█████████▏| 275/300 [00:22<00:02, 11.96it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Epoch 19:  93%|█████████▎| 280/300 [00:23<00:01, 12.09it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Epoch 19:  95%|█████████▌| 285/300 [00:23<00:01, 12.25it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 33.51it/s]\u001b[A\n",
      "Epoch 19:  97%|█████████▋| 290/300 [00:23<00:00, 12.35it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Epoch 19:  98%|█████████▊| 295/300 [00:23<00:00, 12.45it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]\n",
      "Epoch 19: 100%|██████████| 300/300 [00:23<00:00, 12.58it/s, loss=3.71, v_num=2, val_loss=3.660, val_acc=0.242]Adjusting learning rate of group 0 to 7.5000e-03.\n",
      "Adjusting learning rate of group 1 to 3.7500e-04.\n",
      "Epoch 19: 100%|██████████| 300/300 [00:24<00:00, 12.47it/s, loss=3.71, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Epoch 20:  67%|██████▋   | 200/300 [00:19<00:09, 10.07it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  68%|██████▊   | 205/300 [00:20<00:09, 10.02it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Validating:   6%|▌         | 6/100 [00:00<00:07, 12.03it/s]\u001b[A\n",
      "Epoch 20:  70%|███████   | 210/300 [00:20<00:08, 10.18it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Epoch 20:  72%|███████▏  | 215/300 [00:20<00:08, 10.33it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Validating:  15%|█▌        | 15/100 [00:00<00:04, 20.41it/s]\u001b[A\n",
      "Epoch 20:  73%|███████▎  | 220/300 [00:20<00:07, 10.48it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Epoch 20:  75%|███████▌  | 225/300 [00:21<00:07, 10.64it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Validating:  26%|██▌       | 26/100 [00:01<00:02, 27.02it/s]\u001b[A\n",
      "Epoch 20:  77%|███████▋  | 230/300 [00:21<00:06, 10.78it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Epoch 20:  78%|███████▊  | 235/300 [00:21<00:05, 10.93it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Epoch 20:  80%|████████  | 240/300 [00:21<00:05, 11.08it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Epoch 20:  82%|████████▏ | 245/300 [00:21<00:04, 11.23it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Validating:  45%|████▌     | 45/100 [00:01<00:01, 30.31it/s]\u001b[A\n",
      "Epoch 20:  83%|████████▎ | 250/300 [00:22<00:04, 11.36it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Epoch 20:  85%|████████▌ | 255/300 [00:22<00:03, 11.52it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Epoch 20:  87%|████████▋ | 260/300 [00:22<00:03, 11.64it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Validating:  61%|██████    | 61/100 [00:02<00:01, 29.02it/s]\u001b[A\n",
      "Epoch 20:  88%|████████▊ | 265/300 [00:22<00:02, 11.77it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Epoch 20:  90%|█████████ | 270/300 [00:22<00:02, 11.90it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Epoch 20:  92%|█████████▏| 275/300 [00:22<00:02, 12.03it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Validating:  75%|███████▌  | 75/100 [00:03<00:00, 28.64it/s]\u001b[A\n",
      "Epoch 20:  93%|█████████▎| 280/300 [00:23<00:01, 12.15it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Epoch 20:  95%|█████████▌| 285/300 [00:23<00:01, 12.29it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Epoch 20:  97%|█████████▋| 290/300 [00:23<00:00, 12.41it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Validating:  90%|█████████ | 90/100 [00:03<00:00, 29.42it/s]\u001b[A\n",
      "Epoch 20:  98%|█████████▊| 295/300 [00:23<00:00, 12.52it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]\n",
      "Epoch 20: 100%|██████████| 300/300 [00:23<00:00, 12.67it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.244]Adjusting learning rate of group 0 to 7.2700e-03.\n",
      "Adjusting learning rate of group 1 to 3.6350e-04.\n",
      "Epoch 20: 100%|██████████| 300/300 [00:23<00:00, 12.54it/s, loss=3.68, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Epoch 21:  67%|██████▋   | 200/300 [00:19<00:09, 10.10it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  68%|██████▊   | 205/300 [00:20<00:09, 10.06it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Epoch 21:  70%|███████   | 210/300 [00:20<00:08, 10.25it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Epoch 21:  72%|███████▏  | 215/300 [00:20<00:08, 10.37it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Validating:  15%|█▌        | 15/100 [00:00<00:04, 20.12it/s]\u001b[A\n",
      "Epoch 21:  73%|███████▎  | 220/300 [00:20<00:07, 10.52it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Epoch 21:  75%|███████▌  | 225/300 [00:21<00:07, 10.66it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Epoch 21:  77%|███████▋  | 230/300 [00:21<00:06, 10.81it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Validating:  30%|███       | 30/100 [00:01<00:02, 26.54it/s]\u001b[A\n",
      "Epoch 21:  78%|███████▊  | 235/300 [00:21<00:05, 10.96it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Epoch 21:  80%|████████  | 240/300 [00:21<00:05, 11.12it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Validating:  40%|████      | 40/100 [00:01<00:02, 28.39it/s]\u001b[A\n",
      "Epoch 21:  82%|████████▏ | 245/300 [00:21<00:04, 11.25it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Epoch 21:  83%|████████▎ | 250/300 [00:21<00:04, 11.39it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Validating:  50%|█████     | 50/100 [00:02<00:01, 28.44it/s]\u001b[A\n",
      "Epoch 21:  85%|████████▌ | 255/300 [00:22<00:03, 11.52it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Epoch 21:  87%|████████▋ | 260/300 [00:22<00:03, 11.66it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Validating:  60%|██████    | 60/100 [00:02<00:01, 28.80it/s]\u001b[A\n",
      "Epoch 21:  88%|████████▊ | 265/300 [00:22<00:02, 11.79it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Epoch 21:  90%|█████████ | 270/300 [00:22<00:02, 11.93it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Epoch 21:  92%|█████████▏| 275/300 [00:22<00:02, 12.06it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Validating:  75%|███████▌  | 75/100 [00:03<00:00, 29.70it/s]\u001b[A\n",
      "Epoch 21:  93%|█████████▎| 280/300 [00:22<00:01, 12.20it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Epoch 21:  95%|█████████▌| 285/300 [00:23<00:01, 12.30it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Validating:  86%|████████▌ | 86/100 [00:03<00:00, 28.06it/s]\u001b[A\n",
      "Epoch 21:  97%|█████████▋| 290/300 [00:23<00:00, 12.43it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Epoch 21:  98%|█████████▊| 295/300 [00:23<00:00, 12.55it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 28.19it/s]\u001b[A\n",
      "Epoch 21: 100%|██████████| 300/300 [00:23<00:00, 12.66it/s, loss=3.66, v_num=2, val_loss=3.630, val_acc=0.246]Adjusting learning rate of group 0 to 7.0337e-03.\n",
      "Adjusting learning rate of group 1 to 3.5168e-04.\n",
      "Epoch 21: 100%|██████████| 300/300 [00:23<00:00, 12.56it/s, loss=3.66, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Epoch 22:  67%|██████▋   | 200/300 [00:19<00:09, 10.12it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  68%|██████▊   | 205/300 [00:20<00:09, 10.06it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Epoch 22:  70%|███████   | 210/300 [00:20<00:08, 10.25it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Epoch 22:  72%|███████▏  | 215/300 [00:20<00:08, 10.37it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Validating:  15%|█▌        | 15/100 [00:00<00:04, 20.09it/s]\u001b[A\n",
      "Epoch 22:  73%|███████▎  | 220/300 [00:20<00:07, 10.52it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Epoch 22:  75%|███████▌  | 225/300 [00:21<00:07, 10.66it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Epoch 22:  77%|███████▋  | 230/300 [00:21<00:06, 10.84it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Validating:  30%|███       | 30/100 [00:01<00:02, 25.55it/s]\u001b[A\n",
      "Epoch 22:  78%|███████▊  | 235/300 [00:21<00:05, 10.95it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Epoch 22:  80%|████████  | 240/300 [00:21<00:05, 11.11it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Validating:  41%|████      | 41/100 [00:01<00:02, 28.20it/s]\u001b[A\n",
      "Epoch 22:  82%|████████▏ | 245/300 [00:21<00:04, 11.25it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Epoch 22:  83%|████████▎ | 250/300 [00:21<00:04, 11.39it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Epoch 22:  85%|████████▌ | 255/300 [00:22<00:03, 11.53it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Epoch 22:  87%|████████▋ | 260/300 [00:22<00:03, 11.66it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Validating:  60%|██████    | 60/100 [00:02<00:01, 29.77it/s]\u001b[A\n",
      "Epoch 22:  88%|████████▊ | 265/300 [00:22<00:02, 11.79it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Epoch 22:  90%|█████████ | 270/300 [00:22<00:02, 11.94it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Epoch 22:  92%|█████████▏| 275/300 [00:22<00:02, 12.06it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Epoch 22:  93%|█████████▎| 280/300 [00:22<00:01, 12.18it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Validating:  80%|████████  | 80/100 [00:03<00:00, 28.47it/s]\u001b[A\n",
      "Epoch 22:  95%|█████████▌| 285/300 [00:23<00:01, 12.31it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Epoch 22:  97%|█████████▋| 290/300 [00:23<00:00, 12.43it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Validating:  91%|█████████ | 91/100 [00:03<00:00, 28.49it/s]\u001b[A\n",
      "Epoch 22:  98%|█████████▊| 295/300 [00:23<00:00, 12.55it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]\n",
      "Epoch 22: 100%|██████████| 300/300 [00:23<00:00, 12.67it/s, loss=3.65, v_num=2, val_loss=3.610, val_acc=0.253]Adjusting learning rate of group 0 to 6.7918e-03.\n",
      "Adjusting learning rate of group 1 to 3.3959e-04.\n",
      "Epoch 22: 100%|██████████| 300/300 [00:23<00:00, 12.54it/s, loss=3.65, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23:  67%|██████▋   | 200/300 [00:19<00:09, 10.02it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  68%|██████▊   | 205/300 [00:20<00:09,  9.98it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Validating:   5%|▌         | 5/100 [00:00<00:09,  9.66it/s]\u001b[A\n",
      "Epoch 23:  70%|███████   | 210/300 [00:20<00:08, 10.11it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23:  72%|███████▏  | 215/300 [00:20<00:08, 10.30it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23:  73%|███████▎  | 220/300 [00:20<00:07, 10.48it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23:  75%|███████▌  | 225/300 [00:21<00:07, 10.59it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Validating:  25%|██▌       | 25/100 [00:01<00:03, 23.66it/s]\u001b[A\n",
      "Epoch 23:  77%|███████▋  | 230/300 [00:21<00:06, 10.73it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23:  78%|███████▊  | 235/300 [00:21<00:05, 10.88it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23:  80%|████████  | 240/300 [00:21<00:05, 11.04it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23:  82%|████████▏ | 245/300 [00:21<00:04, 11.18it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Validating:  45%|████▌     | 45/100 [00:01<00:01, 29.13it/s]\u001b[A\n",
      "Epoch 23:  83%|████████▎ | 250/300 [00:22<00:04, 11.31it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23:  85%|████████▌ | 255/300 [00:22<00:03, 11.46it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23:  87%|████████▋ | 260/300 [00:22<00:03, 11.60it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Validating:  60%|██████    | 60/100 [00:02<00:01, 27.75it/s]\u001b[A\n",
      "Epoch 23:  88%|████████▊ | 265/300 [00:22<00:02, 11.72it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23:  90%|█████████ | 270/300 [00:22<00:02, 11.85it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23:  92%|█████████▏| 275/300 [00:22<00:02, 11.99it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23:  93%|█████████▎| 280/300 [00:23<00:01, 12.11it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23:  95%|█████████▌| 285/300 [00:23<00:01, 12.27it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 33.50it/s]\u001b[A\n",
      "Epoch 23:  97%|█████████▋| 290/300 [00:23<00:00, 12.34it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23:  98%|█████████▊| 295/300 [00:23<00:00, 12.48it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]\n",
      "Epoch 23: 100%|██████████| 300/300 [00:23<00:00, 12.59it/s, loss=3.62, v_num=2, val_loss=3.590, val_acc=0.254]Adjusting learning rate of group 0 to 6.5451e-03.\n",
      "Adjusting learning rate of group 1 to 3.2725e-04.\n",
      "Epoch 23: 100%|██████████| 300/300 [00:24<00:00, 12.49it/s, loss=3.62, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Epoch 24:  67%|██████▋   | 200/300 [00:20<00:10, 10.00it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/100 [00:00<00:42,  2.34it/s]\u001b[A\n",
      "Epoch 24:  68%|██████▊   | 205/300 [00:20<00:09,  9.95it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Epoch 24:  70%|███████   | 210/300 [00:20<00:08, 10.11it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Epoch 24:  72%|███████▏  | 215/300 [00:20<00:08, 10.28it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Validating:  15%|█▌        | 15/100 [00:00<00:03, 22.77it/s]\u001b[A\n",
      "Epoch 24:  73%|███████▎  | 220/300 [00:21<00:07, 10.42it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Epoch 24:  75%|███████▌  | 225/300 [00:21<00:07, 10.58it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Validating:  25%|██▌       | 25/100 [00:01<00:02, 26.75it/s]\u001b[A\n",
      "Epoch 24:  77%|███████▋  | 230/300 [00:21<00:06, 10.72it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Epoch 24:  78%|███████▊  | 235/300 [00:21<00:05, 10.87it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Validating:  35%|███▌      | 35/100 [00:01<00:02, 28.90it/s]\u001b[A\n",
      "Epoch 24:  80%|████████  | 240/300 [00:21<00:05, 11.02it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Epoch 24:  82%|████████▏ | 245/300 [00:21<00:04, 11.16it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Validating:  45%|████▌     | 45/100 [00:01<00:01, 29.51it/s]\u001b[A\n",
      "Epoch 24:  83%|████████▎ | 250/300 [00:22<00:04, 11.30it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Epoch 24:  85%|████████▌ | 255/300 [00:22<00:03, 11.47it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Epoch 24:  87%|████████▋ | 260/300 [00:22<00:03, 11.55it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Epoch 24:  88%|████████▊ | 265/300 [00:22<00:02, 11.71it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Epoch 24:  90%|█████████ | 270/300 [00:22<00:02, 11.82it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Validating:  70%|███████   | 70/100 [00:02<00:01, 26.55it/s]\u001b[A\n",
      "Epoch 24:  92%|█████████▏| 275/300 [00:23<00:02, 11.95it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Epoch 24:  93%|█████████▎| 280/300 [00:23<00:01, 12.08it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Epoch 24:  95%|█████████▌| 285/300 [00:23<00:01, 12.21it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Epoch 24:  97%|█████████▋| 290/300 [00:23<00:00, 12.34it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Epoch 24:  98%|█████████▊| 295/300 [00:23<00:00, 12.49it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 28.07it/s]\u001b[A\n",
      "Epoch 24: 100%|██████████| 300/300 [00:23<00:00, 12.57it/s, loss=3.55, v_num=2, val_loss=3.610, val_acc=0.250]Adjusting learning rate of group 0 to 6.2941e-03.\n",
      "Adjusting learning rate of group 1 to 3.1470e-04.\n",
      "Epoch 24: 100%|██████████| 300/300 [00:24<00:00, 12.45it/s, loss=3.55, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Epoch 25:  67%|██████▋   | 200/300 [00:19<00:09, 10.04it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25:  68%|██████▊   | 205/300 [00:20<00:09, 10.00it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Validating:   5%|▌         | 5/100 [00:00<00:09,  9.86it/s]\u001b[A\n",
      "Epoch 25:  70%|███████   | 210/300 [00:20<00:08, 10.15it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Epoch 25:  72%|███████▏  | 215/300 [00:20<00:08, 10.32it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Validating:  16%|█▌        | 16/100 [00:00<00:03, 23.70it/s]\u001b[A\n",
      "Epoch 25:  73%|███████▎  | 220/300 [00:21<00:07, 10.47it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Epoch 25:  75%|███████▌  | 225/300 [00:21<00:07, 10.63it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Epoch 25:  77%|███████▋  | 230/300 [00:21<00:06, 10.78it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Validating:  30%|███       | 30/100 [00:01<00:02, 30.37it/s]\u001b[A\n",
      "Epoch 25:  78%|███████▊  | 235/300 [00:21<00:05, 10.91it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Epoch 25:  80%|████████  | 240/300 [00:21<00:05, 11.06it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Epoch 25:  82%|████████▏ | 245/300 [00:21<00:04, 11.20it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Epoch 25:  83%|████████▎ | 250/300 [00:22<00:04, 11.34it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Validating:  50%|█████     | 50/100 [00:02<00:01, 28.48it/s]\u001b[A\n",
      "Epoch 25:  85%|████████▌ | 255/300 [00:22<00:03, 11.49it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Epoch 25:  87%|████████▋ | 260/300 [00:22<00:03, 11.61it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Epoch 25:  88%|████████▊ | 265/300 [00:22<00:02, 11.76it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Epoch 25:  90%|█████████ | 270/300 [00:22<00:02, 11.90it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Validating:  70%|███████   | 70/100 [00:02<00:00, 30.39it/s]\u001b[A\n",
      "Epoch 25:  92%|█████████▏| 275/300 [00:22<00:02, 12.02it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Epoch 25:  93%|█████████▎| 280/300 [00:23<00:01, 12.12it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Epoch 25:  95%|█████████▌| 285/300 [00:23<00:01, 12.27it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 27.08it/s]\u001b[A\n",
      "Epoch 25:  97%|█████████▋| 290/300 [00:23<00:00, 12.37it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Epoch 25:  98%|█████████▊| 295/300 [00:23<00:00, 12.49it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 27.85it/s]\u001b[A\n",
      "Epoch 25: 100%|██████████| 300/300 [00:23<00:00, 12.60it/s, loss=3.56, v_num=2, val_loss=3.600, val_acc=0.258]Adjusting learning rate of group 0 to 6.0396e-03.\n",
      "Adjusting learning rate of group 1 to 3.0198e-04.\n",
      "Epoch 25: 100%|██████████| 300/300 [00:24<00:00, 12.49it/s, loss=3.56, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  67%|██████▋   | 200/300 [00:19<00:09, 10.11it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26:  68%|██████▊   | 205/300 [00:20<00:09, 10.07it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  70%|███████   | 210/300 [00:20<00:08, 10.23it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  72%|███████▏  | 215/300 [00:20<00:08, 10.42it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Validating:  15%|█▌        | 15/100 [00:00<00:04, 20.10it/s]\u001b[A\n",
      "Epoch 26:  73%|███████▎  | 220/300 [00:20<00:07, 10.52it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  75%|███████▌  | 225/300 [00:21<00:07, 10.69it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  77%|███████▋  | 230/300 [00:21<00:06, 10.83it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  78%|███████▊  | 235/300 [00:21<00:05, 10.98it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Validating:  35%|███▌      | 35/100 [00:01<00:02, 28.79it/s]\u001b[A\n",
      "Epoch 26:  80%|████████  | 240/300 [00:21<00:05, 11.14it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  82%|████████▏ | 245/300 [00:21<00:04, 11.28it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  83%|████████▎ | 250/300 [00:21<00:04, 11.42it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  85%|████████▌ | 255/300 [00:22<00:03, 11.58it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  87%|████████▋ | 260/300 [00:22<00:03, 11.66it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  88%|████████▊ | 265/300 [00:22<00:02, 11.82it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Validating:  65%|██████▌   | 65/100 [00:02<00:01, 30.25it/s]\u001b[A\n",
      "Epoch 26:  90%|█████████ | 270/300 [00:22<00:02, 11.91it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  92%|█████████▏| 275/300 [00:22<00:02, 12.08it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  93%|█████████▎| 280/300 [00:22<00:01, 12.18it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  95%|█████████▌| 285/300 [00:23<00:01, 12.30it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26:  97%|█████████▋| 290/300 [00:23<00:00, 12.42it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Validating:  90%|█████████ | 90/100 [00:03<00:00, 28.17it/s]\u001b[A\n",
      "Epoch 26:  98%|█████████▊| 295/300 [00:23<00:00, 12.53it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]\n",
      "Epoch 26: 100%|██████████| 300/300 [00:23<00:00, 12.66it/s, loss=3.48, v_num=2, val_loss=3.620, val_acc=0.252]Adjusting learning rate of group 0 to 5.7822e-03.\n",
      "Adjusting learning rate of group 1 to 2.8911e-04.\n",
      "Epoch 26: 100%|██████████| 300/300 [00:23<00:00, 12.54it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Epoch 27:  67%|██████▋   | 200/300 [00:20<00:10,  9.99it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27:  68%|██████▊   | 205/300 [00:20<00:09,  9.95it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Epoch 27:  70%|███████   | 210/300 [00:20<00:08, 10.13it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Validating:  10%|█         | 10/100 [00:00<00:05, 15.42it/s]\u001b[A\n",
      "Epoch 27:  72%|███████▏  | 215/300 [00:20<00:08, 10.24it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Epoch 27:  73%|███████▎  | 220/300 [00:21<00:07, 10.40it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Epoch 27:  75%|███████▌  | 225/300 [00:21<00:07, 10.55it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Validating:  25%|██▌       | 25/100 [00:01<00:02, 25.34it/s]\u001b[A\n",
      "Epoch 27:  77%|███████▋  | 230/300 [00:21<00:06, 10.70it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Epoch 27:  78%|███████▊  | 235/300 [00:21<00:05, 10.85it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Epoch 27:  80%|████████  | 240/300 [00:21<00:05, 11.00it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Validating:  40%|████      | 40/100 [00:01<00:01, 30.09it/s]\u001b[A\n",
      "Epoch 27:  82%|████████▏ | 245/300 [00:22<00:04, 11.14it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Epoch 27:  83%|████████▎ | 250/300 [00:22<00:04, 11.30it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Epoch 27:  85%|████████▌ | 255/300 [00:22<00:03, 11.41it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Validating:  56%|█████▌    | 56/100 [00:02<00:01, 26.61it/s]\u001b[A\n",
      "Epoch 27:  87%|████████▋ | 260/300 [00:22<00:03, 11.53it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Epoch 27:  88%|████████▊ | 265/300 [00:22<00:02, 11.68it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Epoch 27:  90%|█████████ | 270/300 [00:22<00:02, 11.80it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Validating:  70%|███████   | 70/100 [00:02<00:01, 28.80it/s]\u001b[A\n",
      "Epoch 27:  92%|█████████▏| 275/300 [00:23<00:02, 11.94it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Epoch 27:  93%|█████████▎| 280/300 [00:23<00:01, 12.09it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Epoch 27:  95%|█████████▌| 285/300 [00:23<00:01, 12.21it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Epoch 27:  97%|█████████▋| 290/300 [00:23<00:00, 12.33it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Validating:  90%|█████████ | 90/100 [00:03<00:00, 29.98it/s]\u001b[A\n",
      "Epoch 27:  98%|█████████▊| 295/300 [00:23<00:00, 12.41it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]\n",
      "Epoch 27: 100%|██████████| 300/300 [00:23<00:00, 12.56it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.259]Adjusting learning rate of group 0 to 5.5226e-03.\n",
      "Adjusting learning rate of group 1 to 2.7613e-04.\n",
      "Epoch 27: 100%|██████████| 300/300 [00:24<00:00, 12.46it/s, loss=3.52, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28:  67%|██████▋   | 200/300 [00:19<00:09, 10.08it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/100 [00:00<00:44,  2.23it/s]\u001b[A\n",
      "Epoch 28:  68%|██████▊   | 205/300 [00:20<00:09, 10.03it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28:  70%|███████   | 210/300 [00:20<00:08, 10.19it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Validating:  10%|█         | 10/100 [00:00<00:05, 16.49it/s]\u001b[A\n",
      "Epoch 28:  72%|███████▏  | 215/300 [00:20<00:08, 10.34it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28:  73%|███████▎  | 220/300 [00:20<00:07, 10.50it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28:  75%|███████▌  | 225/300 [00:21<00:07, 10.68it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Validating:  25%|██▌       | 25/100 [00:01<00:02, 30.84it/s]\u001b[A\n",
      "Epoch 28:  77%|███████▋  | 230/300 [00:21<00:06, 10.81it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28:  78%|███████▊  | 235/300 [00:21<00:05, 10.96it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28:  80%|████████  | 240/300 [00:21<00:05, 11.14it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28:  82%|████████▏ | 245/300 [00:21<00:04, 11.25it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28:  83%|████████▎ | 250/300 [00:21<00:04, 11.39it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Validating:  50%|█████     | 50/100 [00:02<00:01, 29.01it/s]\u001b[A\n",
      "Epoch 28:  85%|████████▌ | 255/300 [00:22<00:03, 11.54it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28:  87%|████████▋ | 260/300 [00:22<00:03, 11.66it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28:  88%|████████▊ | 265/300 [00:22<00:02, 11.80it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Validating:  65%|██████▌   | 65/100 [00:02<00:01, 30.05it/s]\u001b[A\n",
      "Epoch 28:  90%|█████████ | 270/300 [00:22<00:02, 11.92it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28:  92%|█████████▏| 275/300 [00:22<00:02, 12.05it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28:  93%|█████████▎| 280/300 [00:22<00:01, 12.18it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28:  95%|█████████▌| 285/300 [00:23<00:01, 12.31it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 29.38it/s]\u001b[A\n",
      "Epoch 28:  97%|█████████▋| 290/300 [00:23<00:00, 12.45it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28:  98%|█████████▊| 295/300 [00:23<00:00, 12.54it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Epoch 28: 100%|██████████| 300/300 [00:23<00:00, 12.68it/s, loss=3.48, v_num=2, val_loss=3.600, val_acc=0.256]\n",
      "Validating: 100%|██████████| 100/100 [00:03<00:00, 27.38it/s]\u001b[AAdjusting learning rate of group 0 to 5.2617e-03.\n",
      "Adjusting learning rate of group 1 to 2.6308e-04.\n",
      "Epoch 28: 100%|██████████| 300/300 [00:23<00:00, 12.55it/s, loss=3.48, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Epoch 29:  67%|██████▋   | 200/300 [00:19<00:09, 10.05it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29:  68%|██████▊   | 205/300 [00:20<00:09, 10.00it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Validating:   6%|▌         | 6/100 [00:00<00:07, 12.35it/s]\u001b[A\n",
      "Epoch 29:  70%|███████   | 210/300 [00:20<00:08, 10.15it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Epoch 29:  72%|███████▏  | 215/300 [00:20<00:08, 10.31it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Validating:  15%|█▌        | 15/100 [00:00<00:04, 20.69it/s]\u001b[A\n",
      "Epoch 29:  73%|███████▎  | 220/300 [00:21<00:07, 10.47it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Epoch 29:  75%|███████▌  | 225/300 [00:21<00:07, 10.62it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Epoch 29:  77%|███████▋  | 230/300 [00:21<00:06, 10.79it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Validating:  30%|███       | 30/100 [00:01<00:02, 27.78it/s]\u001b[A\n",
      "Epoch 29:  78%|███████▊  | 235/300 [00:21<00:05, 10.91it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Epoch 29:  80%|████████  | 240/300 [00:21<00:05, 11.06it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Epoch 29:  82%|████████▏ | 245/300 [00:21<00:04, 11.21it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Epoch 29:  83%|████████▎ | 250/300 [00:22<00:04, 11.36it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Validating:  50%|█████     | 50/100 [00:02<00:01, 30.34it/s]\u001b[A\n",
      "Epoch 29:  85%|████████▌ | 255/300 [00:22<00:03, 11.47it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Epoch 29:  87%|████████▋ | 260/300 [00:22<00:03, 11.62it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Epoch 29:  88%|████████▊ | 265/300 [00:22<00:02, 11.76it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Epoch 29:  90%|█████████ | 270/300 [00:22<00:02, 11.88it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Validating:  70%|███████   | 70/100 [00:02<00:01, 28.81it/s]\u001b[A\n",
      "Epoch 29:  92%|█████████▏| 275/300 [00:22<00:02, 12.01it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Epoch 29:  93%|█████████▎| 280/300 [00:23<00:01, 12.14it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Epoch 29:  95%|█████████▌| 285/300 [00:23<00:01, 12.27it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 29.11it/s]\u001b[A\n",
      "Epoch 29:  97%|█████████▋| 290/300 [00:23<00:00, 12.39it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Epoch 29:  98%|█████████▊| 295/300 [00:23<00:00, 12.51it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 29.92it/s]\u001b[A\n",
      "Epoch 29: 100%|██████████| 300/300 [00:23<00:00, 12.64it/s, loss=3.47, v_num=2, val_loss=3.590, val_acc=0.260]Adjusting learning rate of group 0 to 5.0000e-03.\n",
      "Adjusting learning rate of group 1 to 2.5000e-04.\n",
      "Epoch 29: 100%|██████████| 300/300 [00:23<00:00, 12.54it/s, loss=3.47, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30:  67%|██████▋   | 200/300 [00:19<00:09, 10.04it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/100 [00:00<00:43,  2.29it/s]\u001b[A\n",
      "Epoch 30:  68%|██████▊   | 205/300 [00:20<00:09,  9.98it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Validating:   6%|▌         | 6/100 [00:00<00:08, 11.09it/s]\u001b[A\n",
      "Epoch 30:  70%|███████   | 210/300 [00:20<00:08, 10.13it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30:  72%|███████▏  | 215/300 [00:20<00:08, 10.30it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Validating:  16%|█▌        | 16/100 [00:01<00:03, 23.75it/s]\u001b[A\n",
      "Epoch 30:  73%|███████▎  | 220/300 [00:21<00:07, 10.45it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30:  75%|███████▌  | 225/300 [00:21<00:07, 10.60it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30:  77%|███████▋  | 230/300 [00:21<00:06, 10.75it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30:  78%|███████▊  | 235/300 [00:21<00:05, 10.90it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30:  80%|████████  | 240/300 [00:21<00:05, 11.05it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30:  82%|████████▏ | 245/300 [00:21<00:04, 11.22it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Validating:  45%|████▌     | 45/100 [00:01<00:01, 33.43it/s]\u001b[A\n",
      "Epoch 30:  83%|████████▎ | 250/300 [00:22<00:04, 11.32it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30:  85%|████████▌ | 255/300 [00:22<00:03, 11.49it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30:  87%|████████▋ | 260/300 [00:22<00:03, 11.59it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30:  88%|████████▊ | 265/300 [00:22<00:02, 11.72it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30:  90%|█████████ | 270/300 [00:22<00:02, 11.87it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Validating:  70%|███████   | 70/100 [00:02<00:01, 29.91it/s]\u001b[A\n",
      "Epoch 30:  92%|█████████▏| 275/300 [00:22<00:02, 11.99it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30:  93%|█████████▎| 280/300 [00:23<00:01, 12.11it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30:  95%|█████████▌| 285/300 [00:23<00:01, 12.26it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Validating:  86%|████████▌ | 86/100 [00:03<00:00, 28.76it/s]\u001b[A\n",
      "Epoch 30:  97%|█████████▋| 290/300 [00:23<00:00, 12.35it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30:  98%|█████████▊| 295/300 [00:23<00:00, 12.47it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Epoch 30: 100%|██████████| 300/300 [00:23<00:00, 12.58it/s, loss=3.49, v_num=2, val_loss=3.580, val_acc=0.260]\n",
      "Validating: 100%|██████████| 100/100 [00:03<00:00, 27.93it/s]\u001b[AAdjusting learning rate of group 0 to 4.7383e-03.\n",
      "Adjusting learning rate of group 1 to 2.3692e-04.\n",
      "Epoch 30: 100%|██████████| 300/300 [00:24<00:00, 12.45it/s, loss=3.49, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  67%|██████▋   | 200/300 [00:19<00:09, 10.10it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/100 [00:00<00:50,  1.98it/s]\u001b[A\n",
      "Epoch 31:  68%|██████▊   | 205/300 [00:20<00:09, 10.03it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  70%|███████   | 210/300 [00:20<00:08, 10.22it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  72%|███████▏  | 215/300 [00:20<00:08, 10.36it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  73%|███████▎  | 220/300 [00:20<00:07, 10.51it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Validating:  20%|██        | 20/100 [00:01<00:03, 23.63it/s]\u001b[A\n",
      "Epoch 31:  75%|███████▌  | 225/300 [00:21<00:07, 10.66it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  77%|███████▋  | 230/300 [00:21<00:06, 10.81it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  78%|███████▊  | 235/300 [00:21<00:05, 10.97it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Validating:  35%|███▌      | 35/100 [00:01<00:02, 28.92it/s]\u001b[A\n",
      "Epoch 31:  80%|████████  | 240/300 [00:21<00:05, 11.12it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  82%|████████▏ | 245/300 [00:21<00:04, 11.29it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  83%|████████▎ | 250/300 [00:21<00:04, 11.40it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  85%|████████▌ | 255/300 [00:22<00:03, 11.54it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Validating:  55%|█████▌    | 55/100 [00:02<00:01, 30.21it/s]\u001b[A\n",
      "Epoch 31:  87%|████████▋ | 260/300 [00:22<00:03, 11.66it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  88%|████████▊ | 265/300 [00:22<00:02, 11.80it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  90%|█████████ | 270/300 [00:22<00:02, 11.95it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  92%|█████████▏| 275/300 [00:22<00:02, 12.05it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  93%|█████████▎| 280/300 [00:22<00:01, 12.18it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Validating:  80%|████████  | 80/100 [00:03<00:00, 30.21it/s]\u001b[A\n",
      "Epoch 31:  95%|█████████▌| 285/300 [00:23<00:01, 12.30it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  97%|█████████▋| 290/300 [00:23<00:00, 12.44it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Epoch 31:  98%|█████████▊| 295/300 [00:23<00:00, 12.56it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 29.91it/s]\u001b[A\n",
      "Epoch 31: 100%|██████████| 300/300 [00:23<00:00, 12.68it/s, loss=3.46, v_num=2, val_loss=3.590, val_acc=0.256]Adjusting learning rate of group 0 to 4.4774e-03.\n",
      "Adjusting learning rate of group 1 to 2.2387e-04.\n",
      "Epoch 31: 100%|██████████| 300/300 [00:23<00:00, 12.54it/s, loss=3.46, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 32:  92%|█████████▏| 275/300 [00:22<00:02, 12.10it/s, loss=3.39, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 32:  93%|█████████▎| 280/300 [00:22<00:01, 12.22it/s, loss=3.39, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 32:  95%|█████████▌| 285/300 [00:23<00:01, 12.33it/s, loss=3.39, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 28.45it/s]\u001b[A\n",
      "Epoch 32:  97%|█████████▋| 290/300 [00:23<00:00, 12.46it/s, loss=3.39, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 32:  98%|█████████▊| 295/300 [00:23<00:00, 12.59it/s, loss=3.39, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 32: 100%|██████████| 300/300 [00:23<00:00, 12.69it/s, loss=3.39, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Validating: 100%|██████████| 100/100 [00:03<00:00, 27.67it/s]\u001b[AAdjusting learning rate of group 0 to 4.2178e-03.\n",
      "Adjusting learning rate of group 1 to 2.1089e-04.\n",
      "Epoch 32: 100%|██████████| 300/300 [00:23<00:00, 12.56it/s, loss=3.39, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 33:  67%|██████▋   | 200/300 [00:19<00:09, 10.18it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/100 [00:00<00:43,  2.26it/s]\u001b[A\n",
      "Epoch 33:  68%|██████▊   | 205/300 [00:20<00:09, 10.12it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Validating:   6%|▌         | 6/100 [00:00<00:08, 11.28it/s]\u001b[A\n",
      "Epoch 33:  70%|███████   | 210/300 [00:20<00:08, 10.28it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 33:  72%|███████▏  | 215/300 [00:20<00:08, 10.44it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 33:  73%|███████▎  | 220/300 [00:20<00:07, 10.62it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Validating:  20%|██        | 20/100 [00:01<00:03, 26.11it/s]\u001b[A\n",
      "Epoch 33:  75%|███████▌  | 225/300 [00:20<00:06, 10.74it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 33:  77%|███████▋  | 230/300 [00:21<00:06, 10.90it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 33:  78%|███████▊  | 235/300 [00:21<00:05, 11.05it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Validating:  35%|███▌      | 35/100 [00:01<00:02, 28.37it/s]\u001b[A\n",
      "Epoch 33:  80%|████████  | 240/300 [00:21<00:05, 11.18it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 33:  82%|████████▏ | 245/300 [00:21<00:04, 11.34it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 33:  83%|████████▎ | 250/300 [00:21<00:04, 11.47it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Validating:  50%|█████     | 50/100 [00:02<00:01, 27.50it/s]\u001b[A\n",
      "Epoch 33:  85%|████████▌ | 255/300 [00:21<00:03, 11.60it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 33:  87%|████████▋ | 260/300 [00:22<00:03, 11.74it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Validating:  60%|██████    | 60/100 [00:02<00:01, 29.83it/s]\u001b[A\n",
      "Epoch 33:  88%|████████▊ | 265/300 [00:22<00:02, 11.88it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 33:  90%|█████████ | 270/300 [00:22<00:02, 12.00it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 33:  92%|█████████▏| 275/300 [00:22<00:02, 12.13it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Validating:  75%|███████▌  | 75/100 [00:03<00:00, 29.31it/s]\u001b[A\n",
      "Epoch 33:  93%|█████████▎| 280/300 [00:22<00:01, 12.26it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 33:  95%|█████████▌| 285/300 [00:22<00:01, 12.39it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 29.53it/s]\u001b[A\n",
      "Epoch 33:  97%|█████████▋| 290/300 [00:23<00:00, 12.51it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 33:  98%|█████████▊| 295/300 [00:23<00:00, 12.62it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]\n",
      "Epoch 33: 100%|██████████| 300/300 [00:23<00:00, 12.77it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.261]Adjusting learning rate of group 0 to 3.9604e-03.\n",
      "Adjusting learning rate of group 1 to 1.9802e-04.\n",
      "Epoch 33: 100%|██████████| 300/300 [00:23<00:00, 12.64it/s, loss=3.44, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Epoch 34:  67%|██████▋   | 200/300 [00:19<00:09, 10.00it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34:  68%|██████▊   | 205/300 [00:20<00:09,  9.95it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Validating:   6%|▌         | 6/100 [00:00<00:07, 11.98it/s]\u001b[A\n",
      "Epoch 34:  70%|███████   | 210/300 [00:20<00:08, 10.12it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Epoch 34:  72%|███████▏  | 215/300 [00:20<00:08, 10.26it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Epoch 34:  73%|███████▎  | 220/300 [00:21<00:07, 10.42it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Validating:  20%|██        | 20/100 [00:01<00:03, 23.93it/s]\u001b[A\n",
      "Epoch 34:  75%|███████▌  | 225/300 [00:21<00:07, 10.57it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Epoch 34:  77%|███████▋  | 230/300 [00:21<00:06, 10.72it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Epoch 34:  78%|███████▊  | 235/300 [00:21<00:05, 10.88it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Validating:  35%|███▌      | 35/100 [00:01<00:02, 28.13it/s]\u001b[A\n",
      "Epoch 34:  80%|████████  | 240/300 [00:21<00:05, 11.02it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Epoch 34:  82%|████████▏ | 245/300 [00:21<00:04, 11.19it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Epoch 34:  83%|████████▎ | 250/300 [00:22<00:04, 11.31it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Epoch 34:  85%|████████▌ | 255/300 [00:22<00:03, 11.42it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Validating:  55%|█████▌    | 55/100 [00:02<00:01, 27.22it/s]\u001b[A\n",
      "Epoch 34:  87%|████████▋ | 260/300 [00:22<00:03, 11.55it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Epoch 34:  88%|████████▊ | 265/300 [00:22<00:02, 11.69it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Epoch 34:  90%|█████████ | 270/300 [00:22<00:02, 11.82it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Validating:  70%|███████   | 70/100 [00:02<00:01, 28.39it/s]\u001b[A\n",
      "Epoch 34:  92%|█████████▏| 275/300 [00:23<00:02, 11.95it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Epoch 34:  93%|█████████▎| 280/300 [00:23<00:01, 12.10it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Epoch 34:  95%|█████████▌| 285/300 [00:23<00:01, 12.21it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Epoch 34:  97%|█████████▋| 290/300 [00:23<00:00, 12.34it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Validating:  90%|█████████ | 90/100 [00:03<00:00, 29.38it/s]\u001b[A\n",
      "Epoch 34:  98%|█████████▊| 295/300 [00:23<00:00, 12.45it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]\n",
      "Epoch 34: 100%|██████████| 300/300 [00:23<00:00, 12.57it/s, loss=3.37, v_num=2, val_loss=3.570, val_acc=0.263]Adjusting learning rate of group 0 to 3.7059e-03.\n",
      "Adjusting learning rate of group 1 to 1.8530e-04.\n",
      "Epoch 34: 100%|██████████| 300/300 [00:24<00:00, 12.45it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Epoch 35:  67%|██████▋   | 200/300 [00:19<00:09, 10.07it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/100 [00:00<00:39,  2.50it/s]\u001b[A\n",
      "Epoch 35:  68%|██████▊   | 205/300 [00:20<00:09, 10.04it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Epoch 35:  70%|███████   | 210/300 [00:20<00:08, 10.19it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Validating:  10%|█         | 10/100 [00:00<00:04, 20.31it/s]\u001b[A\n",
      "Epoch 35:  72%|███████▏  | 215/300 [00:20<00:08, 10.33it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Epoch 35:  73%|███████▎  | 220/300 [00:20<00:07, 10.49it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Validating:  20%|██        | 20/100 [00:01<00:03, 24.55it/s]\u001b[A\n",
      "Epoch 35:  75%|███████▌  | 225/300 [00:21<00:07, 10.65it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Epoch 35:  77%|███████▋  | 230/300 [00:21<00:06, 10.81it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Epoch 35:  78%|███████▊  | 235/300 [00:21<00:05, 10.96it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Validating:  35%|███▌      | 35/100 [00:01<00:02, 31.56it/s]\u001b[A\n",
      "Epoch 35:  80%|████████  | 240/300 [00:21<00:05, 11.09it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Epoch 35:  82%|████████▏ | 245/300 [00:21<00:04, 11.23it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Epoch 35:  83%|████████▎ | 250/300 [00:21<00:04, 11.37it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Validating:  50%|█████     | 50/100 [00:02<00:01, 28.62it/s]\u001b[A\n",
      "Epoch 35:  85%|████████▌ | 255/300 [00:22<00:03, 11.50it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Epoch 35:  87%|████████▋ | 260/300 [00:22<00:03, 11.65it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Epoch 35:  88%|████████▊ | 265/300 [00:22<00:02, 11.78it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Validating:  65%|██████▌   | 65/100 [00:02<00:01, 29.00it/s]\u001b[A\n",
      "Epoch 35:  90%|█████████ | 270/300 [00:22<00:02, 11.91it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Epoch 35:  92%|█████████▏| 275/300 [00:22<00:02, 12.04it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Epoch 35:  93%|█████████▎| 280/300 [00:23<00:01, 12.17it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Validating:  80%|████████  | 80/100 [00:03<00:00, 27.72it/s]\u001b[A\n",
      "Epoch 35:  95%|█████████▌| 285/300 [00:23<00:01, 12.30it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Epoch 35:  97%|█████████▋| 290/300 [00:23<00:00, 12.41it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Epoch 35:  98%|█████████▊| 295/300 [00:23<00:00, 12.54it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 31.19it/s]\u001b[A\n",
      "Epoch 35: 100%|██████████| 300/300 [00:23<00:00, 12.63it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.265]Adjusting learning rate of group 0 to 3.4549e-03.\n",
      "Adjusting learning rate of group 1 to 1.7275e-04.\n",
      "Epoch 35: 100%|██████████| 300/300 [00:23<00:00, 12.52it/s, loss=3.37, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 36:  67%|██████▋   | 200/300 [00:19<00:09, 10.03it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 36:  68%|██████▊   | 205/300 [00:20<00:09,  9.99it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating:   6%|▌         | 6/100 [00:00<00:07, 12.23it/s]\u001b[A\n",
      "Epoch 36:  70%|███████   | 210/300 [00:20<00:08, 10.13it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 36:  72%|███████▏  | 215/300 [00:20<00:08, 10.29it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating:  15%|█▌        | 15/100 [00:00<00:04, 20.67it/s]\u001b[A\n",
      "Epoch 36:  73%|███████▎  | 220/300 [00:21<00:07, 10.45it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 36:  75%|███████▌  | 225/300 [00:21<00:07, 10.61it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 36:  77%|███████▋  | 230/300 [00:21<00:06, 10.76it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating:  30%|███       | 30/100 [00:01<00:02, 29.45it/s]\u001b[A\n",
      "Epoch 36:  78%|███████▊  | 235/300 [00:21<00:05, 10.90it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 36:  80%|████████  | 240/300 [00:21<00:05, 11.05it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 36:  82%|████████▏ | 245/300 [00:21<00:04, 11.20it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 36:  83%|████████▎ | 250/300 [00:21<00:04, 11.37it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating:  51%|█████     | 51/100 [00:02<00:01, 29.36it/s]\u001b[A\n",
      "Epoch 36:  85%|████████▌ | 255/300 [00:22<00:03, 11.47it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 36:  87%|████████▋ | 260/300 [00:22<00:03, 11.61it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 36:  88%|████████▊ | 265/300 [00:22<00:02, 11.75it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating:  65%|██████▌   | 65/100 [00:02<00:01, 29.95it/s]\u001b[A\n",
      "Epoch 36:  90%|█████████ | 270/300 [00:22<00:02, 11.88it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 36:  92%|█████████▏| 275/300 [00:22<00:02, 12.00it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 36:  93%|█████████▎| 280/300 [00:23<00:01, 12.14it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 36:  95%|█████████▌| 285/300 [00:23<00:01, 12.26it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 28.63it/s]\u001b[A\n",
      "Epoch 36:  97%|█████████▋| 290/300 [00:23<00:00, 12.38it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 36:  98%|█████████▊| 295/300 [00:23<00:00, 12.51it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 36: 100%|██████████| 300/300 [00:23<00:00, 12.64it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating: 100%|██████████| 100/100 [00:03<00:00, 30.15it/s]\u001b[AAdjusting learning rate of group 0 to 3.2082e-03.\n",
      "Adjusting learning rate of group 1 to 1.6041e-04.\n",
      "Epoch 36: 100%|██████████| 300/300 [00:23<00:00, 12.51it/s, loss=3.36, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Epoch 37:  67%|██████▋   | 200/300 [00:19<00:09, 10.09it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 37:  68%|██████▊   | 205/300 [00:20<00:09, 10.06it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Validating:   5%|▌         | 5/100 [00:00<00:08, 10.64it/s]\u001b[A\n",
      "Epoch 37:  70%|███████   | 210/300 [00:20<00:08, 10.20it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Epoch 37:  72%|███████▏  | 215/300 [00:20<00:08, 10.36it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Validating:  15%|█▌        | 15/100 [00:00<00:03, 22.08it/s]\u001b[A\n",
      "Epoch 37:  73%|███████▎  | 220/300 [00:20<00:07, 10.51it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Epoch 37:  75%|███████▌  | 225/300 [00:21<00:07, 10.66it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Epoch 37:  77%|███████▋  | 230/300 [00:21<00:06, 10.82it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Validating:  30%|███       | 30/100 [00:01<00:02, 27.21it/s]\u001b[A\n",
      "Epoch 37:  78%|███████▊  | 235/300 [00:21<00:05, 10.97it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Epoch 37:  80%|████████  | 240/300 [00:21<00:05, 11.11it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Validating:  40%|████      | 40/100 [00:01<00:02, 28.47it/s]\u001b[A\n",
      "Epoch 37:  82%|████████▏ | 245/300 [00:21<00:04, 11.25it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Epoch 37:  83%|████████▎ | 250/300 [00:21<00:04, 11.40it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Epoch 37:  85%|████████▌ | 255/300 [00:22<00:03, 11.54it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Validating:  56%|█████▌    | 56/100 [00:02<00:01, 27.73it/s]\u001b[A\n",
      "Epoch 37:  87%|████████▋ | 260/300 [00:22<00:03, 11.65it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Epoch 37:  88%|████████▊ | 265/300 [00:22<00:02, 11.81it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Epoch 37:  90%|█████████ | 270/300 [00:22<00:02, 11.95it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Epoch 37:  92%|█████████▏| 275/300 [00:22<00:02, 12.05it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Epoch 37:  93%|█████████▎| 280/300 [00:22<00:01, 12.20it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Epoch 37:  95%|█████████▌| 285/300 [00:23<00:01, 12.30it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 26.79it/s]\u001b[A\n",
      "Epoch 37:  97%|█████████▋| 290/300 [00:23<00:00, 12.41it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Epoch 37:  98%|█████████▊| 295/300 [00:23<00:00, 12.55it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 27.81it/s]\u001b[A\n",
      "Epoch 37: 100%|██████████| 300/300 [00:23<00:00, 12.68it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.266]Adjusting learning rate of group 0 to 2.9663e-03.\n",
      "Adjusting learning rate of group 1 to 1.4832e-04.\n",
      "Epoch 37: 100%|██████████| 300/300 [00:23<00:00, 12.57it/s, loss=3.35, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Epoch 38:  67%|██████▋   | 200/300 [00:19<00:09, 10.13it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 38:  68%|██████▊   | 205/300 [00:20<00:09, 10.07it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Validating:   6%|▌         | 6/100 [00:00<00:07, 12.24it/s]\u001b[A\n",
      "Epoch 38:  70%|███████   | 210/300 [00:20<00:08, 10.22it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Epoch 38:  72%|███████▏  | 215/300 [00:20<00:08, 10.38it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Epoch 38:  73%|███████▎  | 220/300 [00:20<00:07, 10.53it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Validating:  20%|██        | 20/100 [00:01<00:03, 23.50it/s]\u001b[A\n",
      "Epoch 38:  75%|███████▌  | 225/300 [00:21<00:07, 10.69it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Epoch 38:  77%|███████▋  | 230/300 [00:21<00:06, 10.84it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Validating:  30%|███       | 30/100 [00:01<00:02, 27.19it/s]\u001b[A\n",
      "Epoch 38:  78%|███████▊  | 235/300 [00:21<00:05, 10.99it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Epoch 38:  80%|████████  | 240/300 [00:21<00:05, 11.14it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Epoch 38:  82%|████████▏ | 245/300 [00:21<00:04, 11.29it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Validating:  45%|████▌     | 45/100 [00:01<00:01, 29.03it/s]\u001b[A\n",
      "Epoch 38:  83%|████████▎ | 250/300 [00:21<00:04, 11.42it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Epoch 38:  85%|████████▌ | 255/300 [00:22<00:03, 11.57it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Epoch 38:  87%|████████▋ | 260/300 [00:22<00:03, 11.69it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Validating:  61%|██████    | 61/100 [00:02<00:01, 28.68it/s]\u001b[A\n",
      "Epoch 38:  88%|████████▊ | 265/300 [00:22<00:02, 11.83it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Epoch 38:  90%|█████████ | 270/300 [00:22<00:02, 11.96it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Validating:  70%|███████   | 70/100 [00:02<00:01, 28.12it/s]\u001b[A\n",
      "Epoch 38:  92%|█████████▏| 275/300 [00:22<00:02, 12.09it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Epoch 38:  93%|█████████▎| 280/300 [00:22<00:01, 12.24it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Epoch 38:  95%|█████████▌| 285/300 [00:23<00:01, 12.34it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 27.17it/s]\u001b[A\n",
      "Epoch 38:  97%|█████████▋| 290/300 [00:23<00:00, 12.46it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Epoch 38:  98%|█████████▊| 295/300 [00:23<00:00, 12.59it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]\n",
      "Epoch 38: 100%|██████████| 300/300 [00:23<00:00, 12.70it/s, loss=3.39, v_num=2, val_loss=3.550, val_acc=0.270]Adjusting learning rate of group 0 to 2.7300e-03.\n",
      "Adjusting learning rate of group 1 to 1.3650e-04.\n",
      "Epoch 38: 100%|██████████| 300/300 [00:23<00:00, 12.59it/s, loss=3.39, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Epoch 39:  67%|██████▋   | 200/300 [00:19<00:09, 10.07it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/100 [00:00<00:43,  2.25it/s]\u001b[A\n",
      "Epoch 39:  68%|██████▊   | 205/300 [00:20<00:09, 10.03it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Epoch 39:  70%|███████   | 210/300 [00:20<00:08, 10.18it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Epoch 39:  72%|███████▏  | 215/300 [00:20<00:08, 10.35it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Validating:  15%|█▌        | 15/100 [00:00<00:03, 23.18it/s]\u001b[A\n",
      "Epoch 39:  73%|███████▎  | 220/300 [00:20<00:07, 10.51it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Validating:  21%|██        | 21/100 [00:01<00:03, 25.88it/s]\u001b[A\n",
      "Epoch 39:  75%|███████▌  | 225/300 [00:21<00:07, 10.65it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Epoch 39:  77%|███████▋  | 230/300 [00:21<00:06, 10.82it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Epoch 39:  78%|███████▊  | 235/300 [00:21<00:05, 10.95it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Validating:  35%|███▌      | 35/100 [00:01<00:02, 29.19it/s]\u001b[A\n",
      "Epoch 39:  80%|████████  | 240/300 [00:21<00:05, 11.09it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Epoch 39:  82%|████████▏ | 245/300 [00:21<00:04, 11.25it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Epoch 39:  83%|████████▎ | 250/300 [00:21<00:04, 11.38it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Validating:  50%|█████     | 50/100 [00:02<00:01, 28.04it/s]\u001b[A\n",
      "Epoch 39:  85%|████████▌ | 255/300 [00:22<00:03, 11.52it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Epoch 39:  87%|████████▋ | 260/300 [00:22<00:03, 11.65it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Epoch 39:  88%|████████▊ | 265/300 [00:22<00:02, 11.81it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Validating:  66%|██████▌   | 66/100 [00:02<00:01, 29.03it/s]\u001b[A\n",
      "Epoch 39:  90%|█████████ | 270/300 [00:22<00:02, 11.91it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Epoch 39:  92%|█████████▏| 275/300 [00:22<00:02, 12.04it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Epoch 39:  93%|█████████▎| 280/300 [00:22<00:01, 12.18it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Validating:  80%|████████  | 80/100 [00:03<00:00, 30.65it/s]\u001b[A\n",
      "Epoch 39:  95%|█████████▌| 285/300 [00:23<00:01, 12.28it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Epoch 39:  97%|█████████▋| 290/300 [00:23<00:00, 12.43it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Epoch 39:  98%|█████████▊| 295/300 [00:23<00:00, 12.54it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Epoch 39: 100%|██████████| 300/300 [00:23<00:00, 12.65it/s, loss=3.33, v_num=2, val_loss=3.570, val_acc=0.266]\n",
      "Validating: 100%|██████████| 100/100 [00:03<00:00, 27.93it/s]\u001b[AAdjusting learning rate of group 0 to 2.5000e-03.\n",
      "Adjusting learning rate of group 1 to 1.2500e-04.\n",
      "Epoch 39: 100%|██████████| 300/300 [00:23<00:00, 12.53it/s, loss=3.33, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 40:  67%|██████▋   | 200/300 [00:19<00:09, 10.01it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 40:  68%|██████▊   | 205/300 [00:20<00:09,  9.96it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 40:  70%|███████   | 210/300 [00:20<00:08, 10.15it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating:  10%|█         | 10/100 [00:00<00:05, 17.85it/s]\u001b[A\n",
      "Epoch 40:  72%|███████▏  | 215/300 [00:20<00:08, 10.27it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 40:  73%|███████▎  | 220/300 [00:21<00:07, 10.44it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 40:  75%|███████▌  | 225/300 [00:21<00:07, 10.57it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating:  26%|██▌       | 26/100 [00:01<00:03, 24.44it/s]\u001b[A\n",
      "Epoch 40:  77%|███████▋  | 230/300 [00:21<00:06, 10.72it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 40:  78%|███████▊  | 235/300 [00:21<00:05, 10.87it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 40:  80%|████████  | 240/300 [00:21<00:05, 11.01it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating:  40%|████      | 40/100 [00:01<00:02, 27.73it/s]\u001b[A\n",
      "Epoch 40:  82%|████████▏ | 245/300 [00:21<00:04, 11.16it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 40:  83%|████████▎ | 250/300 [00:22<00:04, 11.29it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating:  51%|█████     | 51/100 [00:02<00:01, 28.62it/s]\u001b[A\n",
      "Epoch 40:  85%|████████▌ | 255/300 [00:22<00:03, 11.43it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 40:  87%|████████▋ | 260/300 [00:22<00:03, 11.56it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating:  60%|██████    | 60/100 [00:02<00:01, 28.63it/s]\u001b[A\n",
      "Epoch 40:  88%|████████▊ | 265/300 [00:22<00:02, 11.70it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 40:  90%|█████████ | 270/300 [00:22<00:02, 11.83it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating:  70%|███████   | 70/100 [00:02<00:01, 28.00it/s]\u001b[A\n",
      "Epoch 40:  92%|█████████▏| 275/300 [00:23<00:02, 11.95it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 40:  93%|█████████▎| 280/300 [00:23<00:01, 12.10it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 40:  95%|█████████▌| 285/300 [00:23<00:01, 12.23it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating:  85%|████████▌ | 85/100 [00:03<00:00, 32.21it/s]\u001b[A\n",
      "Epoch 40:  97%|█████████▋| 290/300 [00:23<00:00, 12.33it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Epoch 40:  98%|█████████▊| 295/300 [00:23<00:00, 12.46it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 27.81it/s]\u001b[A\n",
      "Epoch 40: 100%|██████████| 300/300 [00:23<00:00, 12.58it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.267]Adjusting learning rate of group 0 to 2.2768e-03.\n",
      "Adjusting learning rate of group 1 to 1.1384e-04.\n",
      "Epoch 40: 100%|██████████| 300/300 [00:24<00:00, 12.48it/s, loss=3.35, v_num=2, val_loss=3.560, val_acc=0.269]\n",
      "Epoch 41:  37%|███▋      | 111/300 [00:11<00:19,  9.68it/s, loss=3.32, v_num=2, val_loss=3.560, val_acc=0.269]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56:  67%|██████▋   | 200/300 [00:20<00:10,  9.88it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 56:  68%|██████▊   | 205/300 [00:20<00:09,  9.85it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Validating:   5%|▌         | 5/100 [00:00<00:09, 10.32it/s]\u001b[A\n",
      "Epoch 56:  70%|███████   | 210/300 [00:21<00:09,  9.99it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Epoch 56:  72%|███████▏  | 215/300 [00:21<00:08, 10.16it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Epoch 56:  73%|███████▎  | 220/300 [00:21<00:07, 10.33it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Epoch 56:  75%|███████▌  | 225/300 [00:21<00:07, 10.46it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Validating:  25%|██▌       | 25/100 [00:01<00:02, 25.72it/s]\u001b[A\n",
      "Epoch 56:  77%|███████▋  | 230/300 [00:21<00:06, 10.61it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Epoch 56:  78%|███████▊  | 235/300 [00:21<00:06, 10.76it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Epoch 56:  80%|████████  | 240/300 [00:21<00:05, 10.91it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Epoch 56:  82%|████████▏ | 245/300 [00:22<00:04, 11.05it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Validating:  45%|████▌     | 45/100 [00:01<00:01, 30.31it/s]\u001b[A\n",
      "Epoch 56:  83%|████████▎ | 250/300 [00:22<00:04, 11.18it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Epoch 56:  85%|████████▌ | 255/300 [00:22<00:03, 11.31it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Epoch 56:  87%|████████▋ | 260/300 [00:22<00:03, 11.45it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Epoch 56:  88%|████████▊ | 265/300 [00:22<00:03, 11.59it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Validating:  65%|██████▌   | 65/100 [00:02<00:01, 29.29it/s]\u001b[A\n",
      "Epoch 56:  90%|█████████ | 270/300 [00:23<00:02, 11.71it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Epoch 56:  92%|█████████▏| 275/300 [00:23<00:02, 11.86it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Epoch 56:  93%|█████████▎| 280/300 [00:23<00:01, 11.97it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Epoch 56:  95%|█████████▌| 285/300 [00:23<00:01, 12.13it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Epoch 56:  97%|█████████▋| 290/300 [00:23<00:00, 12.21it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Epoch 56:  98%|█████████▊| 295/300 [00:23<00:00, 12.34it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]\n",
      "Validating:  95%|█████████▌| 95/100 [00:03<00:00, 31.11it/s]\u001b[A\n",
      "Epoch 56: 100%|██████████| 300/300 [00:24<00:00, 12.44it/s, loss=3.19, v_num=2, val_loss=3.550, val_acc=0.271]Adjusting learning rate of group 0 to 6.1558e-05.\n",
      "Adjusting learning rate of group 1 to 3.0779e-06.\n",
      "Epoch 56: 100%|██████████| 300/300 [00:24<00:00, 12.33it/s, loss=3.19, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57:  67%|██████▋   | 200/300 [00:20<00:10,  9.99it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 57:  68%|██████▊   | 205/300 [00:20<00:09,  9.95it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57:  70%|███████   | 210/300 [00:20<00:08, 10.11it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Validating:  10%|█         | 10/100 [00:00<00:05, 17.19it/s]\u001b[A\n",
      "Epoch 57:  72%|███████▏  | 215/300 [00:20<00:08, 10.25it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57:  73%|███████▎  | 220/300 [00:21<00:07, 10.44it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57:  75%|███████▌  | 225/300 [00:21<00:07, 10.55it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57:  77%|███████▋  | 230/300 [00:21<00:06, 10.73it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57:  78%|███████▊  | 235/300 [00:21<00:05, 10.84it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57:  80%|████████  | 240/300 [00:21<00:05, 11.01it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Validating:  40%|████      | 40/100 [00:01<00:01, 30.08it/s]\u001b[A\n",
      "Epoch 57:  82%|████████▏ | 245/300 [00:22<00:04, 11.13it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57:  83%|████████▎ | 250/300 [00:22<00:04, 11.27it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57:  85%|████████▌ | 255/300 [00:22<00:03, 11.41it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57:  87%|████████▋ | 260/300 [00:22<00:03, 11.55it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Validating:  60%|██████    | 60/100 [00:02<00:01, 29.67it/s]\u001b[A\n",
      "Epoch 57:  88%|████████▊ | 265/300 [00:22<00:03, 11.66it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57:  90%|█████████ | 270/300 [00:22<00:02, 11.80it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Validating:  70%|███████   | 70/100 [00:02<00:01, 28.14it/s]\u001b[A\n",
      "Epoch 57:  92%|█████████▏| 275/300 [00:23<00:02, 11.93it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57:  93%|█████████▎| 280/300 [00:23<00:01, 12.05it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57:  95%|█████████▌| 285/300 [00:23<00:01, 12.18it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Validating:  86%|████████▌ | 86/100 [00:03<00:00, 29.23it/s]\u001b[A\n",
      "Epoch 57:  97%|█████████▋| 290/300 [00:23<00:00, 12.31it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57:  98%|█████████▊| 295/300 [00:23<00:00, 12.42it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Epoch 57: 100%|██████████| 300/300 [00:23<00:00, 12.55it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.271]\n",
      "Validating: 100%|██████████| 100/100 [00:03<00:00, 30.32it/s]\u001b[AAdjusting learning rate of group 0 to 2.7391e-05.\n",
      "Adjusting learning rate of group 1 to 1.3695e-06.\n",
      "Epoch 57: 100%|██████████| 300/300 [00:24<00:00, 12.42it/s, loss=3.18, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Epoch 58:  67%|██████▋   | 200/300 [00:19<00:09, 10.06it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 58:  68%|██████▊   | 205/300 [00:20<00:09, 10.01it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Validating:   5%|▌         | 5/100 [00:00<00:09,  9.91it/s]\u001b[A\n",
      "Epoch 58:  70%|███████   | 210/300 [00:20<00:08, 10.17it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Epoch 58:  72%|███████▏  | 215/300 [00:20<00:08, 10.33it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Validating:  15%|█▌        | 15/100 [00:00<00:03, 21.61it/s]\u001b[A\n",
      "Epoch 58:  73%|███████▎  | 220/300 [00:20<00:07, 10.48it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Epoch 58:  75%|███████▌  | 225/300 [00:21<00:07, 10.63it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Epoch 58:  77%|███████▋  | 230/300 [00:21<00:06, 10.79it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Validating:  30%|███       | 30/100 [00:01<00:02, 28.97it/s]\u001b[A\n",
      "Epoch 58:  78%|███████▊  | 235/300 [00:21<00:05, 10.91it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Epoch 58:  80%|████████  | 240/300 [00:21<00:05, 11.06it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Epoch 58:  82%|████████▏ | 245/300 [00:21<00:04, 11.21it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Epoch 58:  83%|████████▎ | 250/300 [00:21<00:04, 11.37it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Validating:  50%|█████     | 50/100 [00:02<00:01, 30.99it/s]\u001b[A\n",
      "Epoch 58:  85%|████████▌ | 255/300 [00:22<00:03, 11.51it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Epoch 58:  87%|████████▋ | 260/300 [00:22<00:03, 11.64it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Epoch 58:  88%|████████▊ | 265/300 [00:22<00:02, 11.75it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Validating:  65%|██████▌   | 65/100 [00:02<00:01, 27.75it/s]\u001b[A\n",
      "Epoch 58:  90%|█████████ | 270/300 [00:22<00:02, 11.88it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Epoch 58:  92%|█████████▏| 275/300 [00:22<00:02, 12.01it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Validating:  75%|███████▌  | 75/100 [00:03<00:00, 27.25it/s]\u001b[A\n",
      "Epoch 58:  93%|█████████▎| 280/300 [00:23<00:01, 12.13it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Epoch 58:  95%|█████████▌| 285/300 [00:23<00:01, 12.26it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Validating:  86%|████████▌ | 86/100 [00:03<00:00, 29.13it/s]\u001b[A\n",
      "Epoch 58:  97%|█████████▋| 290/300 [00:23<00:00, 12.37it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Epoch 58:  98%|█████████▊| 295/300 [00:23<00:00, 12.50it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]\n",
      "Epoch 58: 100%|██████████| 300/300 [00:23<00:00, 12.61it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.274]Adjusting learning rate of group 0 to 6.8523e-06.\n",
      "Adjusting learning rate of group 1 to 3.4262e-07.\n",
      "Epoch 58: 100%|██████████| 300/300 [00:24<00:00, 12.50it/s, loss=3.25, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Epoch 59:  67%|██████▋   | 200/300 [00:19<00:09, 10.11it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/100 [00:00<00:42,  2.32it/s]\u001b[A\n",
      "Epoch 59:  68%|██████▊   | 205/300 [00:20<00:09, 10.06it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Epoch 59:  70%|███████   | 210/300 [00:20<00:08, 10.24it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Validating:  11%|█         | 11/100 [00:00<00:04, 21.12it/s]\u001b[A\n",
      "Epoch 59:  72%|███████▏  | 215/300 [00:20<00:08, 10.37it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Epoch 59:  73%|███████▎  | 220/300 [00:20<00:07, 10.53it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Epoch 59:  75%|███████▌  | 225/300 [00:21<00:07, 10.69it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Validating:  25%|██▌       | 25/100 [00:01<00:02, 26.69it/s]\u001b[A\n",
      "Epoch 59:  77%|███████▋  | 230/300 [00:21<00:06, 10.83it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Epoch 59:  78%|███████▊  | 235/300 [00:21<00:05, 10.98it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Epoch 59:  80%|████████  | 240/300 [00:21<00:05, 11.14it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Validating:  40%|████      | 40/100 [00:01<00:02, 29.20it/s]\u001b[A\n",
      "Epoch 59:  82%|████████▏ | 245/300 [00:21<00:04, 11.28it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Epoch 59:  83%|████████▎ | 250/300 [00:21<00:04, 11.42it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Epoch 59:  85%|████████▌ | 255/300 [00:22<00:03, 11.56it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Validating:  55%|█████▌    | 55/100 [00:02<00:01, 30.11it/s]\u001b[A\n",
      "Epoch 59:  87%|████████▋ | 260/300 [00:22<00:03, 11.71it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Epoch 59:  88%|████████▊ | 265/300 [00:22<00:02, 11.82it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Epoch 59:  90%|█████████ | 270/300 [00:22<00:02, 11.96it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Validating:  70%|███████   | 70/100 [00:02<00:01, 26.91it/s]\u001b[A\n",
      "Epoch 59:  92%|█████████▏| 275/300 [00:22<00:02, 12.08it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Epoch 59:  93%|█████████▎| 280/300 [00:22<00:01, 12.22it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Validating:  81%|████████  | 81/100 [00:03<00:00, 29.12it/s]\u001b[A\n",
      "Epoch 59:  95%|█████████▌| 285/300 [00:23<00:01, 12.33it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Epoch 59:  97%|█████████▋| 290/300 [00:23<00:00, 12.46it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Epoch 59:  98%|█████████▊| 295/300 [00:23<00:00, 12.58it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Epoch 59: 100%|██████████| 300/300 [00:23<00:00, 12.69it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.273]\n",
      "Validating: 100%|██████████| 100/100 [00:03<00:00, 28.72it/s]\u001b[AAdjusting learning rate of group 0 to 0.0000e+00.\n",
      "Adjusting learning rate of group 1 to 0.0000e+00.\n",
      "Epoch 59: 100%|██████████| 300/300 [00:23<00:00, 12.57it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.272]\n",
      "                                                             \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 300/300 [00:24<00:00, 12.45it/s, loss=3.24, v_num=2, val_loss=3.540, val_acc=0.272]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  1465.9         \t|  100 %          \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  24.324         \t|60             \t|  1459.5         \t|  99.563         \t|\n",
      "run_training_batch                 \t|  0.072835       \t|12000          \t|  874.02         \t|  59.624         \t|\n",
      "optimizer_step_and_closure_0       \t|  0.022154       \t|12000          \t|  265.85         \t|  18.136         \t|\n",
      "get_train_batch                    \t|  0.021765       \t|12000          \t|  261.18         \t|  17.817         \t|\n",
      "training_step_and_backward         \t|  0.021689       \t|12000          \t|  260.27         \t|  17.755         \t|\n",
      "evaluation_step_and_end            \t|  0.02336        \t|6002           \t|  140.21         \t|  9.5648         \t|\n",
      "validation_step                    \t|  0.023214       \t|6002           \t|  139.33         \t|  9.5051         \t|\n",
      "model_backward                     \t|  0.010654       \t|12000          \t|  127.85         \t|  8.7215         \t|\n",
      "model_forward                      \t|  0.010468       \t|12000          \t|  125.61         \t|  8.5692         \t|\n",
      "training_step                      \t|  0.010181       \t|12000          \t|  122.17         \t|  8.3344         \t|\n",
      "on_train_batch_end                 \t|  0.0042321      \t|12000          \t|  50.786         \t|  3.4645         \t|\n",
      "on_validation_end                  \t|  0.35454        \t|61             \t|  21.627         \t|  1.4754         \t|\n",
      "on_validation_batch_end            \t|  0.0033563      \t|6002           \t|  20.144         \t|  1.3742         \t|\n",
      "cache_result                       \t|  1.988e-05      \t|78676          \t|  1.564          \t|  0.1067         \t|\n",
      "on_batch_start                     \t|  1.8133e-05     \t|12000          \t|  0.21759        \t|  0.014844       \t|\n",
      "on_before_zero_grad                \t|  1.5473e-05     \t|12000          \t|  0.18568        \t|  0.012667       \t|\n",
      "on_batch_end                       \t|  1.2147e-05     \t|12000          \t|  0.14576        \t|  0.0099436      \t|\n",
      "on_after_backward                  \t|  1.2055e-05     \t|12000          \t|  0.14466        \t|  0.0098687      \t|\n",
      "training_step_end                  \t|  9.5137e-06     \t|12000          \t|  0.11416        \t|  0.0077881      \t|\n",
      "on_validation_start                \t|  0.0017938      \t|61             \t|  0.10942        \t|  0.0074646      \t|\n",
      "on_train_batch_start               \t|  8.8152e-06     \t|12000          \t|  0.10578        \t|  0.0072164      \t|\n",
      "on_train_end                       \t|  0.094336       \t|1              \t|  0.094336       \t|  0.0064355      \t|\n",
      "on_validation_batch_start          \t|  1.2293e-05     \t|6002           \t|  0.073785       \t|  0.0050335      \t|\n",
      "validation_step_end                \t|  1.0763e-05     \t|6002           \t|  0.064601       \t|  0.004407       \t|\n",
      "on_train_epoch_start               \t|  0.00095902     \t|60             \t|  0.057541       \t|  0.0039254      \t|\n",
      "on_train_start                     \t|  0.0032542      \t|1              \t|  0.0032542      \t|  0.000222       \t|\n",
      "on_epoch_start                     \t|  1.2017e-05     \t|121            \t|  0.001454       \t|  9.9191e-05     \t|\n",
      "on_epoch_end                       \t|  9.1745e-06     \t|121            \t|  0.0011101      \t|  7.573e-05      \t|\n",
      "on_validation_epoch_end            \t|  1.7765e-05     \t|61             \t|  0.0010837      \t|  7.3927e-05     \t|\n",
      "on_train_epoch_end                 \t|  1.4888e-05     \t|60             \t|  0.00089327     \t|  6.0938e-05     \t|\n",
      "on_validation_epoch_start          \t|  8.4303e-06     \t|61             \t|  0.00051425     \t|  3.5082e-05     \t|\n",
      "on_fit_start                       \t|  1.6015e-05     \t|1              \t|  1.6015e-05     \t|  1.0925e-06     \t|\n",
      "on_before_accelerator_backend_setup\t|  1.0038e-05     \t|1              \t|  1.0038e-05     \t|  6.8477e-07     \t|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_trainer.fit(classifier, train_dataloader=nyudata.train_dataloader(), val_dataloaders=nyudata.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "57cbea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 27.58%\n"
     ]
    }
   ],
   "source": [
    "net = classifier.cuda()\n",
    "\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in nyudata.val_dataloader():\n",
    "        images, labels = batch\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "894a66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(),\n",
    "           '/scratch/vvb238/' + checkpointDir + '/27-classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d66f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
