{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66041fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "from simclr.modules.identity import Identity\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from simclr import SimCLR\n",
    "from simclr.modules import NT_Xent\n",
    "from simclr.modules.transformations import TransformsSimCLR\n",
    "from simclr.modules.sync_batchnorm import convert_model\n",
    "from simclr.modules import LARS\n",
    "from simclr.modules.identity import Identity\n",
    "\n",
    "import random\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "\n",
    "from simclr.modules.transformations import TransformsSimCLR\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "\n",
    "import resnet\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_acc', save_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6c2058",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_id = 15\n",
    "team_name = \"loSSLess\"\n",
    "email_address = \"vvb238@nyu.edu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb10a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split, transform, limit=0):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            root: Location of the dataset folder, usually it is /dataset\n",
    "            split: The split you want to used, it should be one of train, val or unlabeled.\n",
    "            transform: the transform you want to applied to the images.\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_dir = os.path.join(root, split)\n",
    "        \n",
    "        label_path = os.path.join(root, f\"{split}_label_tensor.pt\")\n",
    "        if limit == 0:\n",
    "            self.num_images = len(os.listdir(self.image_dir))\n",
    "        else:\n",
    "            self.num_images = limit\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            self.labels = torch.load(label_path).float()\n",
    "        else:\n",
    "            self.labels = -1 * torch.ones(self.num_images, dtype=torch.long)\n",
    "            \n",
    "            \n",
    "        if self.split == \"unlabeled\":\n",
    "            label_path = os.path.join(\"label_15.pt\")\n",
    "            if os.path.exists(label_path):\n",
    "                labels = torch.load(label_path).float()\n",
    "\n",
    "            images = []\n",
    "            f = open(\"requests.txt\", \"r\")\n",
    "            s = str(f.read()).split(\"\\n\")\n",
    "            for img in s:\n",
    "                images.append(int(img.replace(\".png,\",\"\")))\n",
    "                \n",
    "            self.imageLabelDict = { images[i]: labels[i]  for i in range(len(images))} \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)\n",
    "        with open(os.path.join(self.image_dir, f\"{idx}.png\"), 'rb') as f:\n",
    "            img = Image.open(f).convert('RGB')\n",
    "\n",
    "        if self.split == \"unlabeled\" and idx in self.imageLabelDict:\n",
    "            return self.transform(img), self.imageLabelDict[idx], torch.tensor(idx).float()            \n",
    "        else:\n",
    "            return self.transform(img), self.labels[idx], torch.tensor(idx).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac371af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlur(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            sigma = random.random() * 1.9 + 0.1\n",
    "            return img.filter(ImageFilter.GaussianBlur(sigma))\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "\n",
    "class Solarization(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            return ImageOps.solarize(img)\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69491f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.unlabeled_train_transform = transforms.Compose([\n",
    "            transforms.Normalize(\n",
    "               mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "               std=[1/0.229, 1/0.224, 1/0.225]\n",
    "            ),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomResizedCrop(96, interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "        #             transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                        saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            GaussianBlur(p=0.5),\n",
    "            Solarization(p=0.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.unlabeled_train_transform:\n",
    "            x = self.unlabeled_train_transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y, self.tensors[2][index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483107bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYUImageNetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(96, interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                        saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            GaussianBlur(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        trainset = CustomDataset(root='/dataset', split=\"train\", transform=self.train_transform)\n",
    "#         train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        return trainset\n",
    "    \n",
    "    def extra_train_loader(self):\n",
    "        unlabeledset = CustomDataset(root='/dataset', split=\"unlabeled\", transform=self.train_transform)\n",
    "        unlabeledGivenData = torch.utils.data.Subset(unlabeledset, list(unlabeledset.imageLabelDict.keys()))\n",
    "        trainset = CustomDataset(root='/dataset', split=\"train\", transform=self.train_transform)\n",
    "        trainExtraDataset = torch.utils.data.ConcatDataset((unlabeledGivenData, trainset))\n",
    "#         train_loader = torch.utils.data.DataLoader(trainExtraDataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        return trainExtraDataset\n",
    "        \n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        eval_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        evalset = CustomDataset(root='/dataset', split=\"val\", transform=eval_transform)\n",
    "        eval_loader = torch.utils.data.DataLoader(evalset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        return eval_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06caa259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/dev/lib/python3.8/site-packages/torchvision/transforms/transforms.py:803: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataClass = NYUImageNetDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6457fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.backbone = torchvision.models.resnet34(zero_init_residual=True)\n",
    "        self.backbone = resnet.get_custom_resnet34()\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.lastLayer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            torch.nn.Linear(1024, 800),\n",
    "        )\n",
    "        for layer in self.lastLayer.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.01)\n",
    "                layer.bias.data.zero_()\n",
    "        \n",
    "        self.param_groups = [dict(params=self.lastLayer.parameters(), lr=0.01)]\n",
    "        self.param_groups.append(dict(params=self.backbone.parameters(), lr=0.0005))\n",
    "\n",
    "        \n",
    "        self.criterion=torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.lastLayer(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, label, idx= batch\n",
    "        classProbs = self.forward(data)\n",
    "        loss = self.criterion(classProbs, label.long())\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def _evaluate(self, batch, batch_idx, stage=None):\n",
    "        x, y, _ = batch\n",
    "        out = self.forward(x)\n",
    "        logits = F.log_softmax(out, dim=-1)\n",
    "        loss = F.nll_loss(logits, y.long())\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        acc = accuracy(preds, y.long())\n",
    "\n",
    "        if stage:\n",
    "            self.log(f'{stage}_loss', loss, prog_bar=True)\n",
    "            self.log(f'{stage}_acc', acc, prog_bar=True)\n",
    "\n",
    "        return loss, acc\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        self._evaluate(batch, batch_idx, 'val')[0]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.param_groups, 0, momentum=0.9, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 100, verbose=True)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5b84cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ResNetClassifier()\n",
    "classifier.load_state_dict(torch.load(os.path.join('/scratch/vvb238/barlow-custom34-1000', '437265625-extra-classifier.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e1748da",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "entireUnlabeledDataset = CustomDataset(root='/dataset', split=\"unlabeled\", transform=unlabeled_transform)\n",
    "toBeRankedIndices = torch.tensor([i for i in range(len(entireUnlabeledDataset)) if i not in entireUnlabeledDataset.imageLabelDict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c99c77c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bce9405",
   "metadata": {},
   "outputs": [],
   "source": [
    "activeLearningLoopCount = 1\n",
    "skimTopPercentage = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ba6085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12181fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "originalAndExtraDataset = dataClass.extra_train_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc28c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "evalset = CustomDataset(root='/dataset', split=\"val\", transform=eval_transform)\n",
    "evalloader = torch.utils.data.DataLoader(evalset, batch_size=512, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbbefa5e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running loop number 0\n",
      "\tStarting the evaluation process with unlabeled data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 975/975 [34:15<00:00,  2.11s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tGot the predictions of 499200  images\n",
      "\tSorted the predictions based on confidence scores\n",
      "\tGot the top  14976 confidence indices\n",
      "\tRemoved the indices of top ranked from further consideration\n",
      "\tCombined the original training set and the new dataset to a length of 53376\n",
      "\tStarting to train the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:41<00:00,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 0 is 1.5713549735163042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Current learning rate 0.0001\n",
      "\t\tTeam 15: loSSLess Accuracy: 45.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:34<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 1 is 1.4914096628161644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Current learning rate 0.0001\n",
      "\t\tTeam 15: loSSLess Accuracy: 45.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:34<00:00,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 2 is 1.4579816286226543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Current learning rate 0.0001\n",
      "\t\tTeam 15: loSSLess Accuracy: 45.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:34<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 3 is 1.403506440510281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Current learning rate 0.0001\n",
      "\t\tTeam 15: loSSLess Accuracy: 46.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:34<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 4 is 1.3832718982970973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Current learning rate 0.0001\n",
      "\t\tTeam 15: loSSLess Accuracy: 46.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:34<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 5 is 1.353137275726675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Current learning rate 0.0001\n",
      "\t\tTeam 15: loSSLess Accuracy: 46.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:34<00:00,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 6 is 1.31287523182176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Current learning rate 0.0001\n",
      "\t\tTeam 15: loSSLess Accuracy: 46.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:34<00:00,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 7 is 1.2851458815076082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Current learning rate 0.0001\n",
      "\t\tTeam 15: loSSLess Accuracy: 46.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:34<00:00,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 8 is 1.2709943169026638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Current learning rate 0.0001\n",
      "\t\tTeam 15: loSSLess Accuracy: 46.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:34<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 9 is 1.237322086767613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Current learning rate 0.0001\n",
      "\t\tTeam 15: loSSLess Accuracy: 46.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:34<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 10 is 1.221108958589659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Current learning rate 0.0001\n",
      "\t\tTeam 15: loSSLess Accuracy: 47.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:34<00:00,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 11 is 1.1975412764709346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Current learning rate 0.0001\n",
      "\t\tTeam 15: loSSLess Accuracy: 46.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:34<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 12 is 1.1597130977564292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Current learning rate 0.0001\n",
      "\t\tTeam 15: loSSLess Accuracy: 46.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:34<00:00,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 13 is 1.1347080516300614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Current learning rate 0.0001\n",
      "\t\tTeam 15: loSSLess Accuracy: 47.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [01:34<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tLoss at epoch 14 is 1.1230595374850632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    15: reducing learning rate of group 0 to 2.0000e-05.\n",
      "\t\t Current learning rate 2e-05\n",
      "\t\tTeam 15: loSSLess Accuracy: 46.86%\n"
     ]
    }
   ],
   "source": [
    "for i in range(activeLearningLoopCount):\n",
    "    print(\"\\n\\nRunning loop number\", i)\n",
    "    unlabeledFilteredData = torch.utils.data.Subset(entireUnlabeledDataset, toBeRankedIndices.tolist())\n",
    "    unlabeledFiteredDataLoader = torch.utils.data.DataLoader(unlabeledFilteredData, batch_size=512, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    allConfidenceScores, predictedLabels = torch.Tensor(), torch.Tensor()\n",
    "    actualLabels, allIndices, allImageTensors = torch.Tensor(), torch.tensor([]), torch.Tensor()\n",
    "    \n",
    "    classifier.eval()\n",
    "    print(\"\\tStarting the evaluation process with unlabeled data\")\n",
    "    with torch.no_grad():\n",
    "        # Going through the left over unlabeled set and collecting the confidence for model predictions\n",
    "        numOfBatches = len(unlabeledFilteredData) / unlabeledFiteredDataLoader.batch_size\n",
    "        for idx, batch in tqdm(enumerate(unlabeledFiteredDataLoader), total=int(numOfBatches)):\n",
    "            images, labels, indices = batch\n",
    "\n",
    "            images = images.cuda()\n",
    "#             labels = labels.cuda()\n",
    "\n",
    "            classScores = classifier(images)\n",
    "            classLogits = F.softmax(classScores, dim=1)\n",
    "            \n",
    "            labelConfidence, predictions = torch.max(classLogits.data, 1)\n",
    "            \n",
    "            sortedBatchConfidence, sortedBatchConfidencePos = torch.sort(labelConfidence, descending=True)\n",
    "            topConfidencePos = sortedBatchConfidencePos[:150]\n",
    "            \n",
    "            allConfidenceScores = torch.cat((allConfidenceScores, labelConfidence[topConfidencePos].cpu()))\n",
    "            predictedLabels = torch.cat((predictedLabels, predictions[topConfidencePos].cpu()))\n",
    "#             actualLabels = torch.cat((actualLabels, labels.cpu()))\n",
    "            allIndices = torch.cat((allIndices, indices[topConfidencePos].cpu()))\n",
    "            allImageTensors = torch.cat((allImageTensors, images[topConfidencePos].cpu()))\n",
    "#             break\n",
    "            \n",
    "\n",
    "        print(\"\\tGot the predictions of\" , len(unlabeledFilteredData), \" images\")\n",
    "            \n",
    "        # Sorting all the predictions based on the confidence scores and the argsort\n",
    "        sortedConfidence, sortedConfidencePos = torch.sort(allConfidenceScores, descending=True)\n",
    "        print(\"\\tSorted the predictions based on confidence scores\")\n",
    "\n",
    "        # Calculating how many top predictions to retrain the model on\n",
    "        limit = int(len(unlabeledFilteredData) * (skimTopPercentage/100))\n",
    "        topConfidencePos = sortedConfidencePos[:limit]\n",
    "        print(\"\\tGot the top \", limit, \"confidence indices\")\n",
    "        skimTopPercentage -= 1\n",
    "        \n",
    "\n",
    "        # Fetching the top confidence's index in original dataset\n",
    "        topConfidenceIndices = allIndices[topConfidencePos]\n",
    "        # And removing these indices from toBeRankedIndices\n",
    "        combined = torch.cat((toBeRankedIndices, topConfidenceIndices))\n",
    "        uniques, counts = combined.unique(return_counts=True)\n",
    "        toBeRankedIndices = uniques[counts == 1]\n",
    "        print(\"\\tRemoved the indices of top ranked from further consideration\")\n",
    "        \n",
    "        # Fetching the top confidence's images and labels\n",
    "        topConfidenceImages = allImageTensors[topConfidencePos]\n",
    "        topConfidenceLabels = predictedLabels[topConfidencePos]\n",
    "        additionalTopConfidenceDataset = CustomTensorDataset((topConfidenceImages, topConfidenceLabels, topConfidenceIndices))\n",
    "        originalAndExtraDataset = torch.utils.data.ConcatDataset((additionalTopConfidenceDataset, originalAndExtraDataset))\n",
    "        \n",
    "        originalAndExtraTopConfidenceDataLoader = torch.utils.data.DataLoader(originalAndExtraDataset, batch_size=128, shuffle=True,num_workers=4, pin_memory=True)\n",
    "        print(\"\\tCombined the original training set and the new dataset to a length of\", len(originalAndExtraDataset))\n",
    "    \n",
    "#     classifierFineTuned = ResNetClassifier()\n",
    "#     classifierFineTuned.load_state_dict(torch.load(os.path.join('/scratch/vvb238/barlow-custom34-1000', 'base-classifier.pth')))\n",
    "#     classifier_trainer = Trainer(gpus=1,deterministic=True, max_epochs=100, default_root_dir='/scratch/vvb238/iterativeClassifier-barlow-custom34-1000', profiler=\"simple\",\n",
    "#                      limit_val_batches= 0.3, benchmark=True, callbacks=[checkpoint_callback], fast_dev_run=False)\n",
    "#     classifier_trainer.fit(classifierFineTuned, train_dataloader=originalAndExtraTopConfidenceDataLoader, val_dataloaders=dataClass.val_dataloader())\n",
    "\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=learning, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3, factor=0.2, verbose=True)  \n",
    "    numOfBatches = len(originalAndExtraDataset) / originalAndExtraTopConfidenceDataLoader.batch_size\n",
    "    print(\"\\tStarting to train the model\")\n",
    "    for epoch in range(20):\n",
    "        classifier.train()\n",
    "        running_loss = 0.0\n",
    "        for idx, data in tqdm(enumerate(originalAndExtraTopConfidenceDataLoader), total=int(numOfBatches)):\n",
    "            inputs, labels, idx = data\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = classifier(inputs)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(\"\\t\\tLoss at epoch\", epoch, \"is\", running_loss/numOfBatches)\n",
    "#         scheduler.step(running_loss)\n",
    "        \n",
    "        classifier.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in evalloader:\n",
    "                images, labels, idx = data\n",
    "\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "                outputs = classifier(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = (100 * correct / total)\n",
    "        \n",
    "        scheduler.step(accuracy)\n",
    "        learning = scheduler._last_lr[0]\n",
    "        print(\"\\t\\t Current learning rate\", learning)\n",
    "\n",
    "        print(f\"\\t\\tTeam {team_id}: {team_name} Accuracy: {accuracy:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901329f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning = 2.0000e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28626a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = classifier.cuda()\n",
    "\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in evalloader:\n",
    "        images, labels, idx = data\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print(f\"Team {team_id}: {team_name} Accuracy: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358caf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = dict(model=classifier.state_dict(),\n",
    "                 indices=toBeRankedIndices)\n",
    "torch.save(state, '/scratch/vvb238/iterativeExperiment/4345-extra-checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3183d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
