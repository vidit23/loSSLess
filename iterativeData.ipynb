{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca815b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from simclr import SimCLR\n",
    "from simclr.modules import NT_Xent\n",
    "from simclr.modules.transformations import TransformsSimCLR\n",
    "from simclr.modules.sync_batchnorm import convert_model\n",
    "from simclr.modules import LARS\n",
    "from simclr.modules.identity import Identity\n",
    "\n",
    "import random\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "\n",
    "import resnet\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c141dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split, transform, limit=0):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            root: Location of the dataset folder, usually it is /dataset\n",
    "            split: The split you want to used, it should be one of train, val or unlabeled.\n",
    "            transform: the transform you want to applied to the images.\n",
    "        \"\"\"\n",
    "\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_dir = os.path.join(root, split)\n",
    "        label_path = os.path.join(root, f\"{split}_label_tensor.pt\")\n",
    "\n",
    "        if limit == 0:\n",
    "            self.num_images = len(os.listdir(self.image_dir))\n",
    "        else:\n",
    "            self.num_images = limit\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            self.labels = torch.load(label_path)\n",
    "        else:\n",
    "            self.labels = -1 * torch.ones(self.num_images, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(os.path.join(self.image_dir, f\"{idx}.png\"), 'rb') as f:\n",
    "            img = Image.open(f).convert('RGB')\n",
    "\n",
    "        return self.transform(img), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064de790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYUImageNetDataModule(pl.LightningDataModule):\n",
    "  \n",
    "    def train_dataloader(self):\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        trainset = CustomDataset(root='/dataset', split=\"train\", transform=train_transform)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        eval_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        evalset = CustomDataset(root='/dataset', split=\"val\", transform=eval_transform)\n",
    "        eval_loader = torch.utils.data.DataLoader(evalset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        return eval_loader\n",
    "    \n",
    "    def ssl_train_dataloader(self, batch_size):\n",
    "        unlabeled_dataset = CustomDataset(root='/dataset', split='unlabeled', transform=TransformsSimCLR(96))\n",
    "        unlabeled_dataloader = torch.utils.data.DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        return unlabeled_dataloader\n",
    "        \n",
    "    def ssl_val_dataloader(self, batch_size):\n",
    "        val_dataset = CustomDataset(root='/dataset', split='val', transform=TransformsSimCLR(96))\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f1bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = NYUImageNetDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f071370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = resnet.get_custom_resnet18()\n",
    "        # self.encoder.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'simclr_encoder.pth')))\n",
    "        self.encoder.fc = Identity()\n",
    "        self.lastLayer = torch.nn.Linear(512, 800)\n",
    "        self.criterion=torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.lastLayer(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, label = batch\n",
    "        classProbs = self.forward(data)\n",
    "        loss = self.criterion(classProbs, label)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        data, label = batch\n",
    "        classProbs = self.forward(data)\n",
    "        loss = self.criterion(classProbs, label)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return { 'val_loss' : loss, 'prediction' : classProbs, 'target' : label }\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "        return ({'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22fb34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ResNetClassifier()\n",
    "classifier.load_state_dict(torch.load(os.path.join('/scratch/vvb238/finalSubmission', 'classifier.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cced6bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 17.45%\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.cuda()\n",
    "\n",
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "conf, pred, actual = torch.Tensor().cuda(), torch.Tensor().cuda(), torch.Tensor().cuda()\n",
    "with torch.no_grad():\n",
    "    for batch in data.val_dataloader():\n",
    "        images, labels = batch\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = classifier(images)\n",
    "        logits = F.softmax(outputs, dim=1)\n",
    "        labelConfidence, predictedLabels = torch.max(logits.data, 1)\n",
    "        correctSamples = (predictedLabels == labels)\n",
    "        \n",
    "        conf = torch.cat((conf, labelConfidence))\n",
    "        pred = torch.cat((pred, predictedLabels))\n",
    "        actual = torch.cat((actual, labels))\n",
    "        \n",
    "#         print(labelConfidence, predictedLabels)\n",
    "#         print(((predictedLabels == labels) != 0).nonzero().squeeze())\n",
    "#         print(labelConfidence[((predictedLabels == labels) != 0).nonzero().squeeze()])\n",
    "#         print(torch.sort(labelConfidence, descending=True))\n",
    "#         break\n",
    "        total += labels.size(0)\n",
    "        correct += correctSamples.sum().item()\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d2f07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedConf, sortedLabel = torch.sort(conf.cpu(), descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d129ee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Labels tensor(592)\n",
      "Incorrect Labels tensor(176)\n",
      "Percentage correct tensor(0.7708)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    limit = int(actual.shape[0] * 0.03)\n",
    "    top20Labels = sortedLabel[:limit]\n",
    "    equalLabels = pred[top20Labels].cpu() == actual[top20Labels].cpu()\n",
    "    print(\"Correct Labels\", equalLabels.count_nonzero())\n",
    "    print(\"Incorrect Labels\", limit - equalLabels.count_nonzero())\n",
    "    print(\"Percentage correct\", equalLabels.count_nonzero() / limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b28ed6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25600"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97c06415",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), os.path.join('/scratch/vvb238/iterativeDataFeeding', 'classifier.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f526aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ac9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
