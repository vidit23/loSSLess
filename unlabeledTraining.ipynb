{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc0acf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from simclr import SimCLR\n",
    "from simclr.modules import NT_Xent\n",
    "from simclr.modules.transformations import TransformsSimCLR\n",
    "from simclr.modules.sync_batchnorm import convert_model\n",
    "from simclr.modules import LARS\n",
    "from simclr.modules.identity import Identity\n",
    "\n",
    "import random\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "\n",
    "from simclr.modules.transformations import TransformsSimCLR\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "\n",
    "import resnet\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e281083",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_id = 15\n",
    "team_name = \"loSSLess\"\n",
    "email_address = \"vvb238@nyu.edu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb4b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split, transform, limit=0):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            root: Location of the dataset folder, usually it is /dataset\n",
    "            split: The split you want to used, it should be one of train, val or unlabeled.\n",
    "            transform: the transform you want to applied to the images.\n",
    "        \"\"\"\n",
    "\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.image_dir = os.path.join(root, split)\n",
    "        label_path = os.path.join(root, f\"{split}_label_tensor.pt\")\n",
    "\n",
    "        if limit == 0:\n",
    "            self.num_images = len(os.listdir(self.image_dir))\n",
    "        else:\n",
    "            self.num_images = limit\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            self.labels = torch.load(label_path).float()\n",
    "        else:\n",
    "            self.labels = -1 * torch.ones(self.num_images, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)\n",
    "        with open(os.path.join(self.image_dir, f\"{idx}.png\"), 'rb') as f:\n",
    "            img = Image.open(f).convert('RGB')\n",
    "\n",
    "        return self.transform(img), self.labels[idx], torch.tensor(idx).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03e34341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlur(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            sigma = random.random() * 1.9 + 0.1\n",
    "            return img.filter(ImageFilter.GaussianBlur(sigma))\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "\n",
    "class Solarization(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            return ImageOps.solarize(img)\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71dd319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYUImageNetDataModule(pl.LightningDataModule):\n",
    "  \n",
    "    def train_dataloader(self):\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(96, interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                        saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            GaussianBlur(p=1.0),\n",
    "            Solarization(p=0.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        trainset = CustomDataset(root='/dataset', split=\"train\", transform=train_transform)\n",
    "#         train_loader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        return trainset\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        eval_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        evalset = CustomDataset(root='/dataset', split=\"val\", transform=eval_transform)\n",
    "        eval_loader = torch.utils.data.DataLoader(evalset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        return eval_loader\n",
    "    \n",
    "    def unlabeled_dataloader(self):\n",
    "        unlabeled_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.unlabeledset = CustomDataset(root='/dataset', split=\"unlabeled\", transform=unlabeled_transform)\n",
    "        unlabeled_loader = torch.utils.data.DataLoader(unlabeledset, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        return unlabeled_loader\n",
    "    \n",
    "    def ssl_train_dataloader(self, batch_size):\n",
    "        unlabeled_dataset = CustomDataset(root='/dataset', split='unlabeled', transform=TransformsSimCLR(96))\n",
    "        unlabeled_dataloader = torch.utils.data.DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        return unlabeled_dataloader\n",
    "        \n",
    "    def ssl_val_dataloader(self, batch_size):\n",
    "        val_dataset = CustomDataset(root='/dataset', split='val', transform=TransformsSimCLR(96))\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19e2a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataClass = NYUImageNetDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2df869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.encoder = resnet.get_custom_resnet18()\n",
    "#         self.encoder.fc = Identity()\n",
    "#         self.lastLayer = torch.nn.Linear(512, 800)\n",
    "\n",
    "\n",
    "        self.backbone = torchvision.models.resnet34(zero_init_residual=True)\n",
    "#         self.backbone = resnet.get_custom_resnet34()\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.lastLayer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(1024, 800),\n",
    "        )\n",
    "        for layer in self.lastLayer.modules():\n",
    "           if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.01)\n",
    "                layer.bias.data.zero_()\n",
    "        \n",
    "        self.param_groups = [dict(params=self.lastLayer.parameters(), lr=0.01)]\n",
    "        self.param_groups.append(dict(params=self.backbone.parameters(), lr=0.0001))\n",
    "\n",
    "        \n",
    "        self.criterion=torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.lastLayer(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, label, idx= batch\n",
    "        classProbs = self.forward(data)\n",
    "        loss = self.criterion(classProbs, label)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def _evaluate(self, batch, batch_idx, stage=None):\n",
    "        x, y, _ = batch\n",
    "        out = self.forward(x)\n",
    "        logits = F.log_softmax(out, dim=-1)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f'{stage}_loss', loss, prog_bar=True)\n",
    "            self.log(f'{stage}_acc', acc, prog_bar=True)\n",
    "\n",
    "        return loss, acc\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        self._evaluate(batch, batch_idx, 'val')[0]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.param_groups, 0, momentum=0.9, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS, verbose=True)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1950e329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ResNetClassifier()\n",
    "classifier.load_state_dict(torch.load(os.path.join('/scratch/vvb238/barlow-34', '27-classifier.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "089c32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "entireUnlabeledDataset = CustomDataset(root='/dataset', split=\"unlabeled\", transform=unlabeled_transform)\n",
    "toBeRankedIndices = torch.tensor([i for i in range(len(entireUnlabeledDataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c8ee408",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.cuda()\n",
    "activeLearningLoopCount = 3\n",
    "skimTopPercentage = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "064d9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916782c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running loop number 0\n",
      "\tStarting the evaluation process with unlabeled data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 87/1000 [01:20<09:04,  1.68it/s] "
     ]
    }
   ],
   "source": [
    "for i in range(activeLearningLoopCount):\n",
    "    print(\"\\n\\nRunning loop number\", i)\n",
    "    unlabeledFilteredData = torch.utils.data.Subset(entireUnlabeledDataset, toBeRankedIndices.tolist())\n",
    "    unlabeledFiteredDataLoader = torch.utils.data.DataLoader(unlabeledFilteredData, batch_size=512, shuffle=True, num_workers=6, pin_memory=False)\n",
    "    allConfidenceScores, predictedLabels = torch.Tensor(), torch.Tensor()\n",
    "    actualLabels, allIndices, allImageTensors = torch.Tensor(), torch.tensor([]), torch.Tensor()\n",
    "    \n",
    "    classifier.eval()\n",
    "    print(\"\\tStarting the evaluation process with unlabeled data\")\n",
    "    with torch.no_grad():\n",
    "        # Going through the left over unlabeled set and collecting the confidence for model predictions\n",
    "        numOfBatches = len(unlabeledFilteredData) / unlabeledFiteredDataLoader.batch_size\n",
    "        for idx, batch in tqdm(enumerate(unlabeledFiteredDataLoader), total=int(numOfBatches)):\n",
    "            images, labels, indices = batch\n",
    "\n",
    "            images = images.cuda()\n",
    "#             labels = labels.cuda()\n",
    "\n",
    "            classScores = classifier(images)\n",
    "            classLogits = F.softmax(classScores, dim=1)\n",
    "            \n",
    "            labelConfidence, predictions = torch.max(classLogits.data, 1)\n",
    "            \n",
    "            sortedBatchConfidence, sortedBatchConfidencePos = torch.sort(labelConfidence, descending=True)\n",
    "            topConfidencePos = sortedBatchConfidencePos[:150]\n",
    "            \n",
    "            allConfidenceScores = torch.cat((allConfidenceScores, labelConfidence[topConfidencePos].cpu()))\n",
    "            predictedLabels = torch.cat((predictedLabels, predictions[topConfidencePos].cpu()))\n",
    "#             actualLabels = torch.cat((actualLabels, labels.cpu()))\n",
    "            allIndices = torch.cat((allIndices, indices[topConfidencePos].cpu()))\n",
    "            allImageTensors = torch.cat((allImageTensors, images[topConfidencePos].cpu()))\n",
    "            \n",
    "#             if idx and idx % 100 == 0: print(\"\\t\\tFinished batch number\", idx)\n",
    "#             if idx and idx % 500 == 0: break\n",
    "                \n",
    "            \n",
    "#             break\n",
    "        print(\"\\tGot the predictions of\" , len(unlabeledFilteredData), \" images\")\n",
    "            \n",
    "        # Sorting all the predictions based on the confidence scores and the argsort\n",
    "        sortedConfidence, sortedConfidencePos = torch.sort(allConfidenceScores, descending=True)\n",
    "        print(\"\\tSorted the predictions based on confidence scores\")\n",
    "\n",
    "        # Calculating how many top predictions to retrain the model on\n",
    "        limit = int(len(unlabeledFilteredData) * (skimTopPercentage/100))\n",
    "        topConfidencePos = sortedConfidencePos[:limit+2]\n",
    "        print(\"\\tGot the top \", limit, \"confidence indices\")\n",
    "        \n",
    "\n",
    "        # Fetching the top confidence's index in original dataset\n",
    "        topConfidenceIndices = allIndices[topConfidencePos]\n",
    "        # And removing these indices from toBeRankedIndices\n",
    "        combined = torch.cat((toBeRankedIndices, topConfidenceIndices))\n",
    "        uniques, counts = combined.unique(return_counts=True)\n",
    "        toBeRankedIndices = uniques[counts == 1]\n",
    "        print(\"\\tRemoved the indices of top ranked from further consideration\")\n",
    "        \n",
    "        # Fetching the top confidence's images and labels\n",
    "        topConfidenceImages = allImageTensors[topConfidencePos]\n",
    "        topConfidenceLabels = predictedLabels[topConfidencePos]\n",
    "        additionalTopConfidenceDataset = torch.utils.data.TensorDataset(topConfidenceImages, topConfidenceLabels, topConfidenceIndices)\n",
    "        originalAndExtraDataset = torch.utils.data.ConcatDataset((additionalTopConfidenceDataset, dataClass.train_dataloader()))\n",
    "        originalAndExtraTopConfidenceDataLoader = torch.utils.data.DataLoader(originalAndExtraDataset, batch_size=32, shuffle=True,num_workers=6, pin_memory=True)\n",
    "        print(\"\\tCombined the original training set and the new dataset\")\n",
    "        \n",
    "    classifier.train()\n",
    "    numOfBatches = len(originalAndExtraDataset) / originalAndExtraTopConfidenceDataLoader.batch_size\n",
    "    print(\"\\tStarting to train the model\")\n",
    "    for epoch in range(15):\n",
    "        running_loss = 0.0\n",
    "        for idx, data in tqdm(enumerate(originalAndExtraTopConfidenceDataLoader), total=int(numOfBatches)):\n",
    "            inputs, labels, idx = data\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = classifier(inputs)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(\"\\t\\tLoss at epoch\", epoch, \"is\", running_loss/numOfBatches)\n",
    "        scheduler.step(running_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "evalset = CustomDataset(root='/dataset', split=\"val\", transform=eval_transform)\n",
    "evalloader = torch.utils.data.DataLoader(evalset, batch_size=256, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = classifier\n",
    "\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in evalloader:\n",
    "        images, labels, idx = data\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print(f\"Team {team_id}: {team_name} Accuracy: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74613346",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = dict(model=classifier.state_dict(),\n",
    "                 indices=toBeRankedIndices)\n",
    "torch.save(state, '/scratch/vvb238/iterativeExperiment/checkpoint.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
