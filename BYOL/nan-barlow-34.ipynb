{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3bf97e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nr2229/.local/lib/python3.8/site-packages/lightly/api/version_checking.py:54: Warning: There is a newer version of the package available. For compatability reasons, please upgrade your current version:pip install lightly==1.1.7\n",
      "  warnings.warn(Warning(warning))\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import lightly\n",
    "import lightly.models as models\n",
    "import lightly.loss as loss\n",
    "import lightly.data as data\n",
    "from lightly.models.barlowtwins import BarlowTwins\n",
    "from lightly.models.simclr import SimCLR\n",
    "\n",
    "from simclr.modules.transformations import TransformsSimCLR\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "\n",
    "import resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774c5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointDir = 'barlow-custom34-1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54920674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split, transform, limit=0):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            root: Location of the dataset folder, usually it is /dataset\n",
    "            split: The split you want to used, it should be one of train, val or unlabeled.\n",
    "            transform: the transform you want to applied to the images.\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_dir = os.path.join(root, split)\n",
    "        \n",
    "        label_path = os.path.join(root, f\"{split}_label_tensor.pt\")\n",
    "        if limit == 0:\n",
    "            self.num_images = len(os.listdir(self.image_dir))\n",
    "        else:\n",
    "            self.num_images = limit\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            self.labels = torch.load(label_path)\n",
    "        else:\n",
    "            self.labels = -1 * torch.ones(self.num_images, dtype=torch.long)\n",
    "            \n",
    "            \n",
    "        if self.split == \"unlabeled\":\n",
    "            label_path = os.path.join(\"label_15.pt\")\n",
    "            if os.path.exists(label_path):\n",
    "                labels = torch.load(label_path)\n",
    "\n",
    "            images = []\n",
    "            f = open(\"requests.txt\", \"r\")\n",
    "            s = str(f.read()).split(\"\\n\")\n",
    "            for img in s:\n",
    "                images.append(int(img.replace(\".png,\",\"\")))\n",
    "                \n",
    "            self.imageLabelDict = { images[i]: labels[i]  for i in range(len(images))} \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(os.path.join(self.image_dir, f\"{idx}.png\"), 'rb') as f:\n",
    "            img = Image.open(f).convert('RGB')\n",
    "\n",
    "        if self.split == \"unlabeled\" and idx in self.imageLabelDict:\n",
    "            return self.transform(img), self.imageLabelDict[idx]            \n",
    "        else:\n",
    "            return self.transform(img), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df996cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlur(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            sigma = random.random() * 1.9 + 0.1\n",
    "            return img.filter(ImageFilter.GaussianBlur(sigma))\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "\n",
    "class Solarization(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            return ImageOps.solarize(img)\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "463e279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform:\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(96, interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                        saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            GaussianBlur(p=1.0),\n",
    "            Solarization(p=0.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.transform_prime = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(96, interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                        saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            GaussianBlur(p=0.1),\n",
    "            Solarization(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y1 = self.transform(x)\n",
    "        y2 = self.transform_prime(x)\n",
    "        return y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e82e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LARS(optim.Optimizer):\n",
    "    def __init__(self, params, lr, weight_decay=0, momentum=0.9, eta=0.001,\n",
    "                 weight_decay_filter=None, lars_adaptation_filter=None):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum,\n",
    "                        eta=eta, weight_decay_filter=weight_decay_filter,\n",
    "                        lars_adaptation_filter=lars_adaptation_filter)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for g in self.param_groups:\n",
    "            for p in g['params']:\n",
    "                dp = p.grad\n",
    "\n",
    "                if dp is None:\n",
    "                    continue\n",
    "\n",
    "                if g['weight_decay_filter'] is None or not g['weight_decay_filter'](p):\n",
    "                    dp = dp.add(p, alpha=g['weight_decay'])\n",
    "\n",
    "                if g['lars_adaptation_filter'] is None or not g['lars_adaptation_filter'](p):\n",
    "                    param_norm = torch.norm(p)\n",
    "                    update_norm = torch.norm(dp)\n",
    "                    one = torch.ones_like(param_norm)\n",
    "                    q = torch.where(param_norm > 0.,\n",
    "                                    torch.where(update_norm > 0,\n",
    "                                                (g['eta'] * param_norm / update_norm), one), one)\n",
    "                    dp = dp.mul(q)\n",
    "\n",
    "                param_state = self.state[p]\n",
    "                if 'mu' not in param_state:\n",
    "                    param_state['mu'] = torch.zeros_like(p)\n",
    "                mu = param_state['mu']\n",
    "                mu.mul_(g['momentum']).add_(dp)\n",
    "\n",
    "                p.add_(mu, alpha=-g['lr'])\n",
    "\n",
    "\n",
    "def exclude_bias_and_norm(p):\n",
    "    return p.ndim == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df7eaacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset from your image folder\n",
    "dataset = CustomDataset(root='/dataset', split='unlabeled', transform=Transform())\n",
    "\n",
    "# build a PyTorch dataloader\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1024, shuffle=True, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eace874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarlowTwins(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.backbone = torchvision.models.resnet34(zero_init_residual=True)\n",
    "        self.backbone = resnet.get_custom_resnet34()\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        # projector\n",
    "        sizes = [512] + list(map(int, '1024-1024-1024'.split('-')))\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=False))\n",
    "            layers.append(nn.BatchNorm1d(sizes[i + 1]))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Linear(sizes[-2], sizes[-1], bias=False))\n",
    "        self.projector = nn.Sequential(*layers)\n",
    "\n",
    "        # normalization layer for the representations z1 and z2\n",
    "        self.bn = nn.BatchNorm1d(sizes[-1], affine=False)\n",
    "\n",
    "    def forward(self, y1, y2):\n",
    "        z1 = self.projector(self.backbone(y1))\n",
    "        z2 = self.projector(self.backbone(y2))\n",
    "\n",
    "        # empirical cross-correlation matrix\n",
    "        c = self.bn(z1).T @ self.bn(z2)\n",
    "\n",
    "        # sum the cross-correlation matrix between all gpus\n",
    "        c.div_(1024)\n",
    "#         torch.distributed.all_reduce(c)\n",
    "\n",
    "        # use --scale-loss to multiply the loss by a constant factor\n",
    "        # see the Issues section of the readme\n",
    "        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum().mul(1/32)\n",
    "        off_diag = off_diagonal(c).pow_(2).sum().mul(1/32)\n",
    "        loss = on_diag + 3.9e-3 * off_diag\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b150c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    # return a flattened view of the off-diagonal elements of a square matrix\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad1c2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, loader, step):\n",
    "    max_steps = 300 * len(loader)\n",
    "    warmup_steps = 10 * len(loader)\n",
    "    base_lr = 0.2 * 1024 / 256\n",
    "    if step < warmup_steps:\n",
    "        lr = base_lr * step / warmup_steps\n",
    "    else:\n",
    "        step -= warmup_steps\n",
    "        max_steps -= warmup_steps\n",
    "        q = 0.5 * (1 + math.cos(math.pi * step / max_steps))\n",
    "        end_lr = base_lr * 0.001\n",
    "        lr = base_lr * q + end_lr * (1 - q)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "067aef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = BarlowTwins().cuda()\n",
    "# model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "optimizer = LARS(model.parameters(), lr=0, weight_decay=1e-6,\n",
    "                 weight_decay_filter=exclude_bias_and_norm,\n",
    "                 lars_adaptation_filter=exclude_bias_and_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c686a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # automatically resume from checkpoint if it exists\n",
    "# if os.path.isfile('/scratch/vvb238/' + checkpointDir + '/checkpoint.pth'):\n",
    "#     ckpt = torch.load('/scratch/vvb238/' + checkpointDir + '/checkpoint.pth',\n",
    "#                       map_location='cpu')\n",
    "#     start_epoch = ckpt['epoch']\n",
    "#     model.load_state_dict(ckpt['model'])\n",
    "#     optimizer.load_state_dict(ckpt['optimizer'])\n",
    "# else:\n",
    "#     start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dd91b10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "# for epoch in range(start_epoch, 300):\n",
    "# #     sampler.set_epoch(epoch)\n",
    "#     for step, ((y1, y2), _) in enumerate(loader, start=epoch * len(loader)):\n",
    "#         y1 = y1.cuda()\n",
    "#         y2 = y2.cuda()\n",
    "#         lr = adjust_learning_rate(optimizer, loader, step)\n",
    "#         optimizer.zero_grad()\n",
    "#         with torch.cuda.amp.autocast():\n",
    "#             loss = model.forward(y1, y2)\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "#         if step % 10 == 0:\n",
    "# #             torch.distributed.reduce(loss.div_(args.world_size), 0)\n",
    "# #             if args.rank == 0:\n",
    "#             stats = dict(epoch=epoch, step=step, learning_rate=lr,\n",
    "#                          loss=loss.item(),\n",
    "#                          time=int(time.time() - start_time))\n",
    "#             print(json.dumps(stats))\n",
    "# #                 print(json.dumps(stats), file=stats_file)\n",
    "#         # save checkpoint\n",
    "#     state = dict(epoch=epoch + 1, model=model.state_dict(),\n",
    "#                  optimizer=optimizer.state_dict())\n",
    "#     torch.save(state, '/scratch/vvb238/' + checkpointDir + '/checkpoint.pth')\n",
    "    \n",
    "# torch.save(model.backbone.state_dict(),\n",
    "#            '/scratch/vvb238/' + checkpointDir + '/resnet50.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b2db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a33a9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS PART TAKES THE MODEL IN THE MIDDLE AND USES IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f74fd33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlur(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            sigma = random.random() * 1.9 + 0.1\n",
    "            return img.filter(ImageFilter.GaussianBlur(sigma))\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "\n",
    "class Solarization(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            return ImageOps.solarize(img)\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a196237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYUImageNetDataModule(pl.LightningDataModule):\n",
    "  \n",
    "    def train_dataloader(self):\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(96, interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "#             transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                        saturation=0.2, hue=0.1)],\n",
    "                p=0.1\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            GaussianBlur(p=0.2),\n",
    "            Solarization(p=0.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        trainset = CustomDataset(root='/dataset', split=\"train\", transform=train_transform)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        return train_loader\n",
    "    \n",
    "    def added_train_loader(self):\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(96, interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "#             transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                        saturation=0.2, hue=0.1)],\n",
    "                p=0.1\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            GaussianBlur(p=0.2),\n",
    "            Solarization(p=0.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        unlabeledset = CustomDataset(root='/dataset', split=\"unlabeled\", transform=train_transform)\n",
    "        unlabeledGivenData = torch.utils.data.Subset(unlabeledset, list(unlabeledset.imageLabelDict.keys()))\n",
    "        trainset = CustomDataset(root='/dataset', split=\"train\", transform=train_transform)\n",
    "        trainExtraDataset = torch.utils.data.ConcatDataset((unlabeledGivenData, trainset))\n",
    "        train_loader = torch.utils.data.DataLoader(trainExtraDataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        return train_loader\n",
    "        \n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        eval_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        evalset = CustomDataset(root='/dataset', split=\"val\", transform=eval_transform)\n",
    "        eval_loader = torch.utils.data.DataLoader(evalset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        return eval_loader\n",
    "    \n",
    "nyudata = NYUImageNetDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f422893",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('/scratch/nr2229/best-checkpoint.pth'):\n",
    "    ckpt = torch.load('/scratch/nr2229/best-checkpoint.pth',\n",
    "                      map_location='cpu')\n",
    "    model.load_state_dict(ckpt['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2d8bf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "620"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e39e4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simclr.modules.identity import Identity\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "class ResNetClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.backbone = torchvision.models.resnet34(zero_init_residual=True)\n",
    "        self.backbone = resnet.get_custom_resnet34()\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.backbone.load_state_dict(model.backbone.state_dict())\n",
    "        \n",
    "        self.lastLayer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            torch.nn.Linear(1024, 800),\n",
    "        )\n",
    "#         self.lastLayer = torch.nn.Linear(512, 800)\n",
    "        for layer in self.lastLayer.modules():\n",
    "           if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.01)\n",
    "                layer.bias.data.zero_()\n",
    "        \n",
    "        self.param_groups = [dict(params=self.lastLayer.parameters(), lr=0.01)]\n",
    "        self.param_groups.append(dict(params=model.parameters(), lr=0.0005))\n",
    "        \n",
    "        self.criterion=torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "#         x = self.relu(self.projector(x))\n",
    "        x = self.lastLayer(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, label = batch\n",
    "        classProbs = self.forward(data)\n",
    "        loss = self.criterion(classProbs, label)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def _evaluate(self, batch, batch_idx, stage=None):\n",
    "        x, y = batch\n",
    "        out = self.forward(x)\n",
    "        logits = F.log_softmax(out, dim=-1)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f'{stage}_loss', loss, prog_bar=True)\n",
    "            self.log(f'{stage}_acc', acc, prog_bar=True)\n",
    "\n",
    "        return loss, acc\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        self._evaluate(batch, batch_idx, 'val')[0]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.param_groups, 0, momentum=0.9, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS, verbose=True)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ba392e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "classifier = ResNetClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b987a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', save_last=True)\n",
    "classifier_trainer = Trainer(gpus=1,deterministic=True, max_epochs=EPOCHS, default_root_dir='/scratch/nr2229/classifier-' + checkpointDir, profiler=\"simple\",\n",
    "                     limit_val_batches= 0.3, benchmark=True, callbacks=[checkpoint_callback], fast_dev_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517665c1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Adjusting learning rate of group 1 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | backbone  | ResNet           | 21.3 M\n",
      "1 | lastLayer | Sequential       | 1.3 M \n",
      "2 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "22.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.6 M    Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  84%|████████▎ | 301/360 [01:32<00:18,  3.24it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  84%|████████▍ | 302/360 [01:33<00:17,  3.24it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  84%|████████▍ | 304/360 [01:33<00:17,  3.25it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  85%|████████▌ | 306/360 [01:33<00:16,  3.27it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  86%|████████▌ | 308/360 [01:33<00:15,  3.29it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  86%|████████▌ | 310/360 [01:33<00:15,  3.31it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  87%|████████▋ | 312/360 [01:33<00:14,  3.32it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  87%|████████▋ | 314/360 [01:34<00:13,  3.33it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  88%|████████▊ | 316/360 [01:34<00:13,  3.34it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  88%|████████▊ | 318/360 [01:34<00:12,  3.35it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  89%|████████▉ | 320/360 [01:35<00:11,  3.37it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  89%|████████▉ | 322/360 [01:35<00:11,  3.38it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  90%|█████████ | 324/360 [01:35<00:10,  3.40it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  91%|█████████ | 326/360 [01:35<00:09,  3.41it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  91%|█████████ | 328/360 [01:35<00:09,  3.43it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  92%|█████████▏| 330/360 [01:35<00:08,  3.45it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  92%|█████████▏| 332/360 [01:35<00:08,  3.46it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  93%|█████████▎| 334/360 [01:35<00:07,  3.48it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  93%|█████████▎| 336/360 [01:36<00:06,  3.50it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  94%|█████████▍| 338/360 [01:36<00:06,  3.51it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  94%|█████████▍| 340/360 [01:36<00:05,  3.53it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  95%|█████████▌| 342/360 [01:36<00:05,  3.54it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  96%|█████████▌| 344/360 [01:36<00:04,  3.56it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  96%|█████████▌| 346/360 [01:36<00:03,  3.58it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  97%|█████████▋| 348/360 [01:36<00:03,  3.59it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  97%|█████████▋| 350/360 [01:36<00:02,  3.61it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  98%|█████████▊| 352/360 [01:37<00:02,  3.63it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  98%|█████████▊| 354/360 [01:37<00:01,  3.64it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  99%|█████████▉| 356/360 [01:37<00:01,  3.66it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0:  99%|█████████▉| 358/360 [01:37<00:00,  3.67it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]\n",
      "Epoch 0: 100%|██████████| 360/360 [01:37<00:00,  3.69it/s, loss=6.47, v_num=1, val_loss=6.68, val_acc=0]Adjusting learning rate of group 0 to 9.9975e-03.\n",
      "Adjusting learning rate of group 1 to 4.9988e-04.\n",
      "Epoch 0: 100%|██████████| 360/360 [01:38<00:00,  3.65it/s, loss=6.47, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  83%|████████▎ | 300/360 [01:00<00:12,  4.92it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 302/360 [01:01<00:11,  4.92it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  84%|████████▍ | 304/360 [01:01<00:11,  4.94it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  85%|████████▌ | 306/360 [01:01<00:10,  4.96it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  86%|████████▌ | 308/360 [01:01<00:10,  4.99it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  86%|████████▌ | 310/360 [01:01<00:09,  5.01it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  87%|████████▋ | 312/360 [01:02<00:09,  5.03it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  87%|████████▋ | 314/360 [01:02<00:09,  5.05it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  88%|████████▊ | 316/360 [01:02<00:08,  5.07it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  88%|████████▊ | 318/360 [01:02<00:08,  5.10it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  89%|████████▉ | 320/360 [01:02<00:07,  5.12it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  89%|████████▉ | 322/360 [01:02<00:07,  5.14it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  90%|█████████ | 324/360 [01:02<00:06,  5.16it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  91%|█████████ | 326/360 [01:02<00:06,  5.18it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  91%|█████████ | 328/360 [01:03<00:06,  5.21it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  92%|█████████▏| 330/360 [01:03<00:05,  5.23it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  92%|█████████▏| 332/360 [01:03<00:05,  5.25it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  93%|█████████▎| 334/360 [01:03<00:04,  5.27it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  93%|█████████▎| 336/360 [01:03<00:04,  5.29it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  94%|█████████▍| 338/360 [01:03<00:04,  5.31it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  94%|█████████▍| 340/360 [01:03<00:03,  5.33it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  95%|█████████▌| 342/360 [01:03<00:03,  5.35it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  96%|█████████▌| 344/360 [01:04<00:02,  5.37it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  96%|█████████▌| 346/360 [01:04<00:02,  5.40it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  97%|█████████▋| 348/360 [01:04<00:02,  5.42it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  97%|█████████▋| 350/360 [01:04<00:01,  5.44it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  98%|█████████▊| 352/360 [01:04<00:01,  5.46it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  98%|█████████▊| 354/360 [01:04<00:01,  5.48it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  99%|█████████▉| 356/360 [01:04<00:00,  5.50it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1:  99%|█████████▉| 358/360 [01:04<00:00,  5.52it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]\n",
      "Epoch 1: 100%|██████████| 360/360 [01:04<00:00,  5.54it/s, loss=5.55, v_num=1, val_loss=6.43, val_acc=0.0337]Adjusting learning rate of group 0 to 9.9901e-03.\n",
      "Adjusting learning rate of group 1 to 4.9951e-04.\n",
      "Epoch 1: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=5.55, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  83%|████████▎ | 300/360 [01:01<00:12,  4.92it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  84%|████████▍ | 302/360 [01:01<00:11,  4.91it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  85%|████████▌ | 306/360 [01:01<00:10,  4.96it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  86%|████████▌ | 308/360 [01:01<00:10,  4.98it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  86%|████████▌ | 310/360 [01:01<00:09,  5.00it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  87%|████████▋ | 312/360 [01:02<00:09,  5.02it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  87%|████████▋ | 314/360 [01:02<00:09,  5.05it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  88%|████████▊ | 316/360 [01:02<00:08,  5.07it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  88%|████████▊ | 318/360 [01:02<00:08,  5.09it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  89%|████████▉ | 320/360 [01:02<00:07,  5.11it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  89%|████████▉ | 322/360 [01:02<00:07,  5.13it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  90%|█████████ | 324/360 [01:02<00:06,  5.16it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  91%|█████████ | 326/360 [01:02<00:06,  5.18it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  91%|█████████ | 328/360 [01:03<00:06,  5.20it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  92%|█████████▏| 330/360 [01:03<00:05,  5.22it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  92%|█████████▏| 332/360 [01:03<00:05,  5.24it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  93%|█████████▎| 334/360 [01:03<00:04,  5.26it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  93%|█████████▎| 336/360 [01:03<00:04,  5.28it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  94%|█████████▍| 338/360 [01:03<00:04,  5.30it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  94%|█████████▍| 340/360 [01:03<00:03,  5.33it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  95%|█████████▌| 342/360 [01:03<00:03,  5.35it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  96%|█████████▌| 344/360 [01:04<00:02,  5.37it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  96%|█████████▌| 346/360 [01:04<00:02,  5.39it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  97%|█████████▋| 348/360 [01:04<00:02,  5.41it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  97%|█████████▋| 350/360 [01:04<00:01,  5.43it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  98%|█████████▊| 352/360 [01:04<00:01,  5.45it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  98%|█████████▊| 354/360 [01:04<00:01,  5.47it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  99%|█████████▉| 356/360 [01:04<00:00,  5.49it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2:  99%|█████████▉| 358/360 [01:04<00:00,  5.51it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]\n",
      "Epoch 2: 100%|██████████| 360/360 [01:05<00:00,  5.53it/s, loss=4.78, v_num=1, val_loss=5.46, val_acc=0.0757]Adjusting learning rate of group 0 to 9.9778e-03.\n",
      "Adjusting learning rate of group 1 to 4.9889e-04.\n",
      "Epoch 2: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=4.78, v_num=1, val_loss=4.64, val_acc=0.136] \n",
      "Epoch 3:  83%|████████▎ | 300/360 [01:01<00:12,  4.91it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  84%|████████▍ | 302/360 [01:01<00:11,  4.91it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  86%|████████▌ | 310/360 [01:02<00:10,  5.00it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  87%|████████▋ | 312/360 [01:02<00:09,  5.02it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  88%|████████▊ | 318/360 [01:02<00:08,  5.09it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  89%|████████▉ | 320/360 [01:02<00:07,  5.11it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  89%|████████▉ | 322/360 [01:02<00:07,  5.13it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  92%|█████████▏| 330/360 [01:03<00:05,  5.22it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  92%|█████████▏| 332/360 [01:03<00:05,  5.24it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  93%|█████████▎| 334/360 [01:03<00:04,  5.26it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  93%|█████████▎| 336/360 [01:03<00:04,  5.28it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  94%|█████████▍| 338/360 [01:03<00:04,  5.30it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  98%|█████████▊| 352/360 [01:04<00:01,  5.45it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  98%|█████████▊| 354/360 [01:04<00:01,  5.47it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  99%|█████████▉| 356/360 [01:04<00:00,  5.49it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3:  99%|█████████▉| 358/360 [01:05<00:00,  5.51it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]\n",
      "Epoch 3: 100%|██████████| 360/360 [01:05<00:00,  5.53it/s, loss=4.24, v_num=1, val_loss=4.64, val_acc=0.136]Adjusting learning rate of group 0 to 9.9606e-03.\n",
      "Adjusting learning rate of group 1 to 4.9803e-04.\n",
      "Epoch 3: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=4.24, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  83%|████████▎ | 300/360 [01:01<00:12,  4.91it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  87%|████████▋ | 312/360 [01:02<00:09,  5.02it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  89%|████████▉ | 322/360 [01:02<00:07,  5.13it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  93%|█████████▎| 336/360 [01:03<00:04,  5.28it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  94%|█████████▍| 338/360 [01:03<00:04,  5.30it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]\n",
      "Epoch 4: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=4.01, v_num=1, val_loss=4.15, val_acc=0.178]Adjusting learning rate of group 0 to 9.9384e-03.\n",
      "Adjusting learning rate of group 1 to 4.9692e-04.\n",
      "Epoch 4: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=4.01, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  83%|████████▎ | 300/360 [01:01<00:12,  4.91it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  87%|████████▋ | 312/360 [01:02<00:09,  5.02it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  89%|████████▉ | 322/360 [01:02<00:07,  5.13it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  94%|█████████▍| 338/360 [01:03<00:04,  5.30it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]\n",
      "Epoch 5: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=3.77, v_num=1, val_loss=3.84, val_acc=0.225]Adjusting learning rate of group 0 to 9.9114e-03.\n",
      "Adjusting learning rate of group 1 to 4.9557e-04.\n",
      "Epoch 5: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=3.77, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]\n",
      "Epoch 6: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=3.57, v_num=1, val_loss=3.63, val_acc=0.247]Adjusting learning rate of group 0 to 9.8796e-03.\n",
      "Adjusting learning rate of group 1 to 4.9398e-04.\n",
      "Epoch 6: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=3.57, v_num=1, val_loss=3.46, val_acc=0.27] \n",
      "Epoch 7:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  87%|████████▋ | 312/360 [01:02<00:09,  5.02it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  89%|████████▉ | 322/360 [01:02<00:07,  5.13it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  93%|█████████▎| 336/360 [01:03<00:04,  5.28it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  94%|█████████▍| 338/360 [01:03<00:04,  5.30it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]\n",
      "Epoch 7: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=3.44, v_num=1, val_loss=3.46, val_acc=0.27]Adjusting learning rate of group 0 to 9.8429e-03.\n",
      "Adjusting learning rate of group 1 to 4.9215e-04.\n",
      "Epoch 7: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=3.44, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]\n",
      "Epoch 8: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=3.46, v_num=1, val_loss=3.35, val_acc=0.289]Adjusting learning rate of group 0 to 9.8015e-03.\n",
      "Adjusting learning rate of group 1 to 4.9007e-04.\n",
      "Epoch 8: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=3.46, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]\n",
      "Epoch 9: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=3.35, v_num=1, val_loss=3.28, val_acc=0.296]Adjusting learning rate of group 0 to 9.7553e-03.\n",
      "Adjusting learning rate of group 1 to 4.8776e-04.\n",
      "Epoch 9: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=3.35, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]\n",
      "Epoch 10: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=3.2, v_num=1, val_loss=3.21, val_acc=0.316]Adjusting learning rate of group 0 to 9.7044e-03.\n",
      "Adjusting learning rate of group 1 to 4.8522e-04.\n",
      "Epoch 10: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]\n",
      "Epoch 11: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=3.2, v_num=1, val_loss=3.14, val_acc=0.318]Adjusting learning rate of group 0 to 9.6489e-03.\n",
      "Adjusting learning rate of group 1 to 4.8244e-04.\n",
      "Epoch 11: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=3.2, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]\n",
      "Epoch 12: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=3.17, v_num=1, val_loss=3.06, val_acc=0.334]Adjusting learning rate of group 0 to 9.5888e-03.\n",
      "Adjusting learning rate of group 1 to 4.7944e-04.\n",
      "Epoch 12: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=3.17, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  86%|████████▌ | 308/360 [01:02<00:10,  4.96it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  88%|████████▊ | 318/360 [01:02<00:08,  5.07it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  92%|█████████▏| 330/360 [01:03<00:05,  5.20it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  98%|█████████▊| 354/360 [01:04<00:01,  5.45it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  99%|█████████▉| 356/360 [01:05<00:00,  5.48it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]\n",
      "Epoch 13: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=3.11, v_num=1, val_loss=3.03, val_acc=0.339]Adjusting learning rate of group 0 to 9.5241e-03.\n",
      "Adjusting learning rate of group 1 to 4.7621e-04.\n",
      "Epoch 13: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]   \n",
      "Epoch 14:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  84%|████████▍ | 302/360 [01:01<00:11,  4.89it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  86%|████████▌ | 308/360 [01:02<00:10,  4.96it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  86%|████████▌ | 310/360 [01:02<00:10,  4.98it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  87%|████████▋ | 312/360 [01:02<00:09,  5.00it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  88%|████████▊ | 318/360 [01:02<00:08,  5.07it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  89%|████████▉ | 320/360 [01:02<00:07,  5.09it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  89%|████████▉ | 322/360 [01:02<00:07,  5.11it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  92%|█████████▏| 330/360 [01:03<00:05,  5.20it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  92%|█████████▏| 332/360 [01:03<00:05,  5.22it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  93%|█████████▎| 334/360 [01:03<00:04,  5.24it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  93%|█████████▎| 336/360 [01:03<00:04,  5.26it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  98%|█████████▊| 354/360 [01:04<00:01,  5.45it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  99%|█████████▉| 356/360 [01:05<00:00,  5.47it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14:  99%|█████████▉| 358/360 [01:05<00:00,  5.49it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]\n",
      "Epoch 14: 100%|██████████| 360/360 [01:05<00:00,  5.51it/s, loss=3.11, v_num=1, val_loss=3, val_acc=0.343]Adjusting learning rate of group 0 to 9.4550e-03.\n",
      "Adjusting learning rate of group 1 to 4.7275e-04.\n",
      "Epoch 14: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=3.11, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  86%|████████▌ | 308/360 [01:02<00:10,  4.96it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  88%|████████▊ | 318/360 [01:02<00:08,  5.07it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  92%|█████████▏| 330/360 [01:03<00:05,  5.20it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  92%|█████████▏| 332/360 [01:03<00:05,  5.22it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  98%|█████████▊| 354/360 [01:04<00:01,  5.45it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  99%|█████████▉| 356/360 [01:05<00:00,  5.47it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15:  99%|█████████▉| 358/360 [01:05<00:00,  5.49it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]\n",
      "Epoch 15: 100%|██████████| 360/360 [01:05<00:00,  5.51it/s, loss=3.03, v_num=1, val_loss=2.98, val_acc=0.351]Adjusting learning rate of group 0 to 9.3815e-03.\n",
      "Adjusting learning rate of group 1 to 4.6908e-04.\n",
      "Epoch 15: 100%|██████████| 360/360 [01:06<00:00,  5.45it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]\n",
      "Epoch 16: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.351]Adjusting learning rate of group 0 to 9.3037e-03.\n",
      "Adjusting learning rate of group 1 to 4.6519e-04.\n",
      "Epoch 16: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=3.03, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]\n",
      "Epoch 17: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.9, v_num=1, val_loss=2.93, val_acc=0.358]Adjusting learning rate of group 0 to 9.2216e-03.\n",
      "Adjusting learning rate of group 1 to 4.6108e-04.\n",
      "Epoch 17: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.9, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  86%|████████▌ | 308/360 [01:02<00:10,  4.96it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  88%|████████▊ | 318/360 [01:02<00:08,  5.07it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  92%|█████████▏| 330/360 [01:03<00:05,  5.20it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  92%|█████████▏| 332/360 [01:03<00:05,  5.22it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  98%|█████████▊| 354/360 [01:04<00:01,  5.45it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  99%|█████████▉| 356/360 [01:05<00:00,  5.47it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18:  99%|█████████▉| 358/360 [01:05<00:00,  5.49it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]\n",
      "Epoch 18: 100%|██████████| 360/360 [01:05<00:00,  5.51it/s, loss=2.87, v_num=1, val_loss=2.89, val_acc=0.361]Adjusting learning rate of group 0 to 9.1354e-03.\n",
      "Adjusting learning rate of group 1 to 4.5677e-04.\n",
      "Epoch 18: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=2.87, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  92%|█████████▏| 330/360 [01:03<00:05,  5.20it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  98%|█████████▊| 354/360 [01:04<00:01,  5.45it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  99%|█████████▉| 356/360 [01:05<00:00,  5.47it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19:  99%|█████████▉| 358/360 [01:05<00:00,  5.49it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]\n",
      "Epoch 19: 100%|██████████| 360/360 [01:05<00:00,  5.51it/s, loss=2.91, v_num=1, val_loss=2.86, val_acc=0.369]Adjusting learning rate of group 0 to 9.0451e-03.\n",
      "Adjusting learning rate of group 1 to 4.5225e-04.\n",
      "Epoch 19: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=2.91, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]\n",
      "Epoch 20: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.89, v_num=1, val_loss=2.84, val_acc=0.372]Adjusting learning rate of group 0 to 8.9508e-03.\n",
      "Adjusting learning rate of group 1 to 4.4754e-04.\n",
      "Epoch 20: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.89, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  83%|████████▎ | 300/360 [01:01<00:12,  4.91it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  94%|█████████▍| 338/360 [01:03<00:04,  5.30it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]\n",
      "Epoch 21: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.79, v_num=1, val_loss=2.85, val_acc=0.372]Adjusting learning rate of group 0 to 8.8526e-03.\n",
      "Adjusting learning rate of group 1 to 4.4263e-04.\n",
      "Epoch 21: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.79, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]\n",
      "Epoch 22: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.76, v_num=1, val_loss=2.81, val_acc=0.378]Adjusting learning rate of group 0 to 8.7506e-03.\n",
      "Adjusting learning rate of group 1 to 4.3753e-04.\n",
      "Epoch 22: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.76, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  99%|█████████▉| 356/360 [01:05<00:00,  5.48it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]\n",
      "Epoch 23: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.73, v_num=1, val_loss=2.82, val_acc=0.373]Adjusting learning rate of group 0 to 8.6448e-03.\n",
      "Adjusting learning rate of group 1 to 4.3224e-04.\n",
      "Epoch 23: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=2.73, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]\n",
      "Epoch 24: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.65, v_num=1, val_loss=2.81, val_acc=0.372]Adjusting learning rate of group 0 to 8.5355e-03.\n",
      "Adjusting learning rate of group 1 to 4.2678e-04.\n",
      "Epoch 24: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.65, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  94%|█████████▍| 338/360 [01:03<00:04,  5.30it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]\n",
      "Epoch 25: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.67, v_num=1, val_loss=2.77, val_acc=0.385]Adjusting learning rate of group 0 to 8.4227e-03.\n",
      "Adjusting learning rate of group 1 to 4.2114e-04.\n",
      "Epoch 25: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]\n",
      "Epoch 26: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.386]Adjusting learning rate of group 0 to 8.3066e-03.\n",
      "Adjusting learning rate of group 1 to 4.1533e-04.\n",
      "Epoch 26: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]\n",
      "Epoch 27: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.67, v_num=1, val_loss=2.76, val_acc=0.383]Adjusting learning rate of group 0 to 8.1871e-03.\n",
      "Adjusting learning rate of group 1 to 4.0936e-04.\n",
      "Epoch 27: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.67, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  99%|█████████▉| 356/360 [01:05<00:00,  5.48it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]\n",
      "Epoch 28: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.64, v_num=1, val_loss=2.75, val_acc=0.385]Adjusting learning rate of group 0 to 8.0645e-03.\n",
      "Adjusting learning rate of group 1 to 4.0323e-04.\n",
      "Epoch 28: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.64, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]\n",
      "Epoch 29: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.54, v_num=1, val_loss=2.76, val_acc=0.382]Adjusting learning rate of group 0 to 7.9389e-03.\n",
      "Adjusting learning rate of group 1 to 3.9695e-04.\n",
      "Epoch 29: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=2.54, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  86%|████████▌ | 308/360 [01:02<00:10,  4.96it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  88%|████████▊ | 318/360 [01:02<00:08,  5.07it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  92%|█████████▏| 330/360 [01:03<00:05,  5.20it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  92%|█████████▏| 332/360 [01:03<00:05,  5.22it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  98%|█████████▊| 354/360 [01:04<00:01,  5.45it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  99%|█████████▉| 356/360 [01:05<00:00,  5.47it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30:  99%|█████████▉| 358/360 [01:05<00:00,  5.49it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]\n",
      "Epoch 30: 100%|██████████| 360/360 [01:05<00:00,  5.51it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.389]Adjusting learning rate of group 0 to 7.8104e-03.\n",
      "Adjusting learning rate of group 1 to 3.9052e-04.\n",
      "Epoch 30: 100%|██████████| 360/360 [01:06<00:00,  5.44it/s, loss=2.57, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]\n",
      "Epoch 31: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.48, v_num=1, val_loss=2.74, val_acc=0.387]Adjusting learning rate of group 0 to 7.6791e-03.\n",
      "Adjusting learning rate of group 1 to 3.8396e-04.\n",
      "Epoch 31: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=2.48, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]\n",
      "Epoch 32: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.54, v_num=1, val_loss=2.71, val_acc=0.392]Adjusting learning rate of group 0 to 7.5452e-03.\n",
      "Adjusting learning rate of group 1 to 3.7726e-04.\n",
      "Epoch 32: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.54, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  83%|████████▎ | 300/360 [01:01<00:12,  4.91it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]\n",
      "Epoch 33: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.43, v_num=1, val_loss=2.72, val_acc=0.392]Adjusting learning rate of group 0 to 7.4088e-03.\n",
      "Adjusting learning rate of group 1 to 3.7044e-04.\n",
      "Epoch 33: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=2.43, v_num=1, val_loss=2.7, val_acc=0.392] \n",
      "Epoch 34:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]\n",
      "Epoch 34: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.49, v_num=1, val_loss=2.7, val_acc=0.392]Adjusting learning rate of group 0 to 7.2700e-03.\n",
      "Adjusting learning rate of group 1 to 3.6350e-04.\n",
      "Epoch 34: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.49, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 35:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]\n",
      "Epoch 35: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.46, v_num=1, val_loss=2.71, val_acc=0.39]Adjusting learning rate of group 0 to 7.1289e-03.\n",
      "Adjusting learning rate of group 1 to 3.5644e-04.\n",
      "Epoch 35: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=2.46, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 36:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]\n",
      "Epoch 36: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.43, v_num=1, val_loss=2.68, val_acc=0.403]Adjusting learning rate of group 0 to 6.9857e-03.\n",
      "Adjusting learning rate of group 1 to 3.4929e-04.\n",
      "Epoch 36: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.43, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  83%|████████▎ | 300/360 [01:01<00:12,  4.91it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 37:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  94%|█████████▍| 338/360 [01:03<00:04,  5.30it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]\n",
      "Epoch 37: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.404]Adjusting learning rate of group 0 to 6.8406e-03.\n",
      "Adjusting learning rate of group 1 to 3.4203e-04.\n",
      "Epoch 37: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.48, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 38:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  92%|█████████▏| 330/360 [01:03<00:05,  5.20it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  98%|█████████▊| 354/360 [01:04<00:01,  5.45it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  99%|█████████▉| 356/360 [01:05<00:00,  5.47it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38:  99%|█████████▉| 358/360 [01:05<00:00,  5.49it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]\n",
      "Epoch 38: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.41, v_num=1, val_loss=2.69, val_acc=0.395]Adjusting learning rate of group 0 to 6.6937e-03.\n",
      "Adjusting learning rate of group 1 to 3.3468e-04.\n",
      "Epoch 38: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]\n",
      "Epoch 39: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.41, v_num=1, val_loss=2.66, val_acc=0.403]Adjusting learning rate of group 0 to 6.5451e-03.\n",
      "Adjusting learning rate of group 1 to 3.2725e-04.\n",
      "Epoch 39: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 40:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  86%|████████▌ | 308/360 [01:02<00:10,  4.96it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  88%|████████▊ | 318/360 [01:02<00:08,  5.07it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  92%|█████████▏| 330/360 [01:03<00:05,  5.20it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  92%|█████████▏| 332/360 [01:03<00:05,  5.22it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  98%|█████████▊| 354/360 [01:04<00:01,  5.45it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  99%|█████████▉| 356/360 [01:05<00:00,  5.47it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40:  99%|█████████▉| 358/360 [01:05<00:00,  5.49it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 40: 100%|██████████| 360/360 [01:05<00:00,  5.51it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]Adjusting learning rate of group 0 to 6.3950e-03.\n",
      "Adjusting learning rate of group 1 to 3.1975e-04.\n",
      "Epoch 40: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.41, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 41:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  86%|████████▌ | 308/360 [01:02<00:10,  4.96it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  88%|████████▊ | 318/360 [01:02<00:08,  5.07it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  92%|█████████▏| 330/360 [01:03<00:05,  5.20it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  99%|█████████▉| 356/360 [01:05<00:00,  5.48it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]\n",
      "Epoch 41: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.4, v_num=1, val_loss=2.67, val_acc=0.404]Adjusting learning rate of group 0 to 6.2434e-03.\n",
      "Adjusting learning rate of group 1 to 3.1217e-04.\n",
      "Epoch 41: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=2.4, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 42:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 42: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.29, v_num=1, val_loss=2.64, val_acc=0.412]Adjusting learning rate of group 0 to 6.0907e-03.\n",
      "Adjusting learning rate of group 1 to 3.0454e-04.\n",
      "Epoch 42: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.29, v_num=1, val_loss=2.67, val_acc=0.4]  \n",
      "Epoch 43:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 43:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]\n",
      "Epoch 43: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.4]Adjusting learning rate of group 0 to 5.9369e-03.\n",
      "Adjusting learning rate of group 1 to 2.9685e-04.\n",
      "Epoch 43: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.33, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 44:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]\n",
      "Epoch 44: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.27, v_num=1, val_loss=2.67, val_acc=0.405]Adjusting learning rate of group 0 to 5.7822e-03.\n",
      "Adjusting learning rate of group 1 to 2.8911e-04.\n",
      "Epoch 44: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 45:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]\n",
      "Epoch 45: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.404]Adjusting learning rate of group 0 to 5.6267e-03.\n",
      "Adjusting learning rate of group 1 to 2.8133e-04.\n",
      "Epoch 45: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.32, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 46: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.407]Adjusting learning rate of group 0 to 5.4705e-03.\n",
      "Adjusting learning rate of group 1 to 2.7353e-04.\n",
      "Epoch 46: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.3, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  83%|████████▎ | 300/360 [01:01<00:12,  4.91it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 47:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  94%|█████████▍| 338/360 [01:03<00:04,  5.30it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 47: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.33, v_num=1, val_loss=2.65, val_acc=0.406]Adjusting learning rate of group 0 to 5.3140e-03.\n",
      "Adjusting learning rate of group 1 to 2.6570e-04.\n",
      "Epoch 47: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.33, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  83%|████████▎ | 300/360 [01:01<00:12,  4.91it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 48:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  87%|████████▋ | 312/360 [01:02<00:09,  5.02it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  89%|████████▉ | 322/360 [01:02<00:07,  5.13it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  94%|█████████▍| 338/360 [01:03<00:04,  5.30it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]\n",
      "Epoch 48: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.3, v_num=1, val_loss=2.66, val_acc=0.408]Adjusting learning rate of group 0 to 5.1571e-03.\n",
      "Adjusting learning rate of group 1 to 2.5785e-04.\n",
      "Epoch 48: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.3, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]\n",
      "Epoch 49: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.412]Adjusting learning rate of group 0 to 5.0000e-03.\n",
      "Adjusting learning rate of group 1 to 2.5000e-04.\n",
      "Epoch 49: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.22, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  83%|████████▎ | 300/360 [01:01<00:12,  4.91it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 50:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  87%|████████▋ | 312/360 [01:02<00:09,  5.02it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  89%|████████▉ | 322/360 [01:02<00:07,  5.13it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  94%|█████████▍| 338/360 [01:03<00:04,  5.30it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]\n",
      "Epoch 50: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.27, v_num=1, val_loss=2.65, val_acc=0.407]Adjusting learning rate of group 0 to 4.8429e-03.\n",
      "Adjusting learning rate of group 1 to 2.4215e-04.\n",
      "Epoch 50: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.27, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  83%|████████▎ | 300/360 [01:01<00:12,  4.91it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 51:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]\n",
      "Epoch 51: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.413]Adjusting learning rate of group 0 to 4.6860e-03.\n",
      "Adjusting learning rate of group 1 to 2.3430e-04.\n",
      "Epoch 51: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.28, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 52:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 52: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.22, v_num=1, val_loss=2.63, val_acc=0.412]Adjusting learning rate of group 0 to 4.5295e-03.\n",
      "Adjusting learning rate of group 1 to 2.2647e-04.\n",
      "Epoch 52: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.22, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 53:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]\n",
      "Epoch 53: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.406]Adjusting learning rate of group 0 to 4.3733e-03.\n",
      "Adjusting learning rate of group 1 to 2.1867e-04.\n",
      "Epoch 53: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=2.21, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 54:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]\n",
      "Epoch 54: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.23, v_num=1, val_loss=2.62, val_acc=0.418]Adjusting learning rate of group 0 to 4.2178e-03.\n",
      "Adjusting learning rate of group 1 to 2.1089e-04.\n",
      "Epoch 54: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.23, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 55:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  98%|█████████▊| 354/360 [01:04<00:01,  5.45it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  99%|█████████▉| 356/360 [01:05<00:00,  5.47it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55:  99%|█████████▉| 358/360 [01:05<00:00,  5.49it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]\n",
      "Epoch 55: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.22, v_num=1, val_loss=2.64, val_acc=0.413]Adjusting learning rate of group 0 to 4.0631e-03.\n",
      "Adjusting learning rate of group 1 to 2.0315e-04.\n",
      "Epoch 55: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.22, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 56:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]\n",
      "Epoch 56: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.21, v_num=1, val_loss=2.65, val_acc=0.411]Adjusting learning rate of group 0 to 3.9093e-03.\n",
      "Adjusting learning rate of group 1 to 1.9546e-04.\n",
      "Epoch 56: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.21, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 57:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  99%|█████████▉| 356/360 [01:05<00:00,  5.48it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]\n",
      "Epoch 57: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.12, v_num=1, val_loss=2.63, val_acc=0.411]Adjusting learning rate of group 0 to 3.7566e-03.\n",
      "Adjusting learning rate of group 1 to 1.8783e-04.\n",
      "Epoch 57: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.41] \n",
      "Epoch 58:  83%|████████▎ | 300/360 [01:01<00:12,  4.91it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 58:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  87%|████████▋ | 312/360 [01:02<00:09,  5.02it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  89%|████████▉ | 322/360 [01:02<00:07,  5.13it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  93%|█████████▎| 334/360 [01:03<00:04,  5.26it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  93%|█████████▎| 336/360 [01:03<00:04,  5.28it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  94%|█████████▍| 338/360 [01:03<00:04,  5.30it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]\n",
      "Epoch 58: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.41]Adjusting learning rate of group 0 to 3.6050e-03.\n",
      "Adjusting learning rate of group 1 to 1.8025e-04.\n",
      "Epoch 58: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.14, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 59:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  92%|█████████▏| 330/360 [01:03<00:05,  5.20it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  99%|█████████▉| 356/360 [01:05<00:00,  5.48it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 59: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.415]Adjusting learning rate of group 0 to 3.4549e-03.\n",
      "Adjusting learning rate of group 1 to 1.7275e-04.\n",
      "Epoch 59: 100%|██████████| 360/360 [01:05<00:00,  5.46it/s, loss=2.09, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 60:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 60: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.17, v_num=1, val_loss=2.61, val_acc=0.415]Adjusting learning rate of group 0 to 3.3063e-03.\n",
      "Adjusting learning rate of group 1 to 1.6532e-04.\n",
      "Epoch 60: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.17, v_num=1, val_loss=2.63, val_acc=0.41] \n",
      "Epoch 61:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 61:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  99%|█████████▉| 356/360 [01:05<00:00,  5.48it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]\n",
      "Epoch 61: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.41]Adjusting learning rate of group 0 to 3.1594e-03.\n",
      "Adjusting learning rate of group 1 to 1.5797e-04.\n",
      "Epoch 61: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.13, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  83%|████████▎ | 300/360 [01:01<00:12,  4.91it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 62:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]\n",
      "Epoch 62: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.15, v_num=1, val_loss=2.63, val_acc=0.412]Adjusting learning rate of group 0 to 3.0143e-03.\n",
      "Adjusting learning rate of group 1 to 1.5071e-04.\n",
      "Epoch 62: 100%|██████████| 360/360 [01:05<00:00,  5.49it/s, loss=2.15, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 63:  84%|████████▍ | 302/360 [01:01<00:11,  4.89it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  84%|████████▍ | 304/360 [01:01<00:11,  4.91it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  86%|████████▌ | 308/360 [01:02<00:10,  4.96it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  86%|████████▌ | 310/360 [01:02<00:10,  4.98it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  87%|████████▋ | 312/360 [01:02<00:09,  5.00it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  88%|████████▊ | 318/360 [01:02<00:08,  5.07it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  89%|████████▉ | 320/360 [01:02<00:07,  5.09it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  89%|████████▉ | 322/360 [01:02<00:07,  5.11it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  90%|█████████ | 324/360 [01:03<00:07,  5.13it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  92%|█████████▏| 330/360 [01:03<00:05,  5.20it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  92%|█████████▏| 332/360 [01:03<00:05,  5.22it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  93%|█████████▎| 334/360 [01:03<00:04,  5.24it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  93%|█████████▎| 336/360 [01:03<00:04,  5.26it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  94%|█████████▍| 338/360 [01:03<00:04,  5.28it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  94%|█████████▍| 340/360 [01:04<00:03,  5.30it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  95%|█████████▌| 342/360 [01:04<00:03,  5.32it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  96%|█████████▌| 344/360 [01:04<00:02,  5.34it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  96%|█████████▌| 346/360 [01:04<00:02,  5.36it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  97%|█████████▋| 348/360 [01:04<00:02,  5.38it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  97%|█████████▋| 350/360 [01:04<00:01,  5.40it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  98%|█████████▊| 354/360 [01:05<00:01,  5.45it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  99%|█████████▉| 356/360 [01:05<00:00,  5.47it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63:  99%|█████████▉| 358/360 [01:05<00:00,  5.49it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]\n",
      "Epoch 63: 100%|██████████| 360/360 [01:05<00:00,  5.51it/s, loss=2.16, v_num=1, val_loss=2.62, val_acc=0.413]Adjusting learning rate of group 0 to 2.8711e-03.\n",
      "Adjusting learning rate of group 1 to 1.4356e-04.\n",
      "Epoch 63: 100%|██████████| 360/360 [01:06<00:00,  5.45it/s, loss=2.16, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 64:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]\n",
      "Epoch 64: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.15, v_num=1, val_loss=2.61, val_acc=0.418]Adjusting learning rate of group 0 to 2.7300e-03.\n",
      "Adjusting learning rate of group 1 to 1.3650e-04.\n",
      "Epoch 64: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.15, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 65:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  90%|█████████ | 324/360 [01:02<00:06,  5.14it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]\n",
      "Epoch 65: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.417]Adjusting learning rate of group 0 to 2.5912e-03.\n",
      "Adjusting learning rate of group 1 to 1.2956e-04.\n",
      "Epoch 65: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.17, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  83%|████████▎ | 300/360 [01:01<00:12,  4.91it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 66:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  84%|████████▍ | 304/360 [01:01<00:11,  4.93it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  85%|████████▌ | 306/360 [01:01<00:10,  4.95it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  86%|████████▌ | 308/360 [01:01<00:10,  4.97it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  87%|████████▋ | 314/360 [01:02<00:09,  5.04it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  90%|█████████ | 324/360 [01:02<00:06,  5.15it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  91%|█████████ | 326/360 [01:03<00:06,  5.17it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  94%|█████████▍| 340/360 [01:03<00:03,  5.32it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  95%|█████████▌| 342/360 [01:04<00:03,  5.34it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  96%|█████████▌| 344/360 [01:04<00:02,  5.36it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  96%|█████████▌| 346/360 [01:04<00:02,  5.38it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]\n",
      "Epoch 66: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.415]Adjusting learning rate of group 0 to 2.4548e-03.\n",
      "Adjusting learning rate of group 1 to 1.2274e-04.\n",
      "Epoch 66: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.419] \n",
      "Epoch 67:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 67:  84%|████████▍ | 302/360 [01:01<00:11,  4.89it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  84%|████████▍ | 304/360 [01:01<00:11,  4.91it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  85%|████████▌ | 306/360 [01:02<00:10,  4.94it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  86%|████████▌ | 308/360 [01:02<00:10,  4.96it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  86%|████████▌ | 310/360 [01:02<00:10,  4.98it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  87%|████████▋ | 312/360 [01:02<00:09,  5.00it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  87%|████████▋ | 314/360 [01:02<00:09,  5.02it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  88%|████████▊ | 318/360 [01:02<00:08,  5.07it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  89%|████████▉ | 320/360 [01:02<00:07,  5.09it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  89%|████████▉ | 322/360 [01:03<00:07,  5.11it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  90%|█████████ | 324/360 [01:03<00:07,  5.13it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  91%|█████████ | 326/360 [01:03<00:06,  5.15it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  92%|█████████▏| 330/360 [01:03<00:05,  5.20it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  92%|█████████▏| 332/360 [01:03<00:05,  5.22it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  93%|█████████▎| 334/360 [01:03<00:04,  5.24it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  93%|█████████▎| 336/360 [01:03<00:04,  5.26it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  94%|█████████▍| 338/360 [01:03<00:04,  5.28it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  94%|█████████▍| 340/360 [01:04<00:03,  5.30it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  95%|█████████▌| 342/360 [01:04<00:03,  5.32it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  96%|█████████▌| 344/360 [01:04<00:02,  5.34it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  96%|█████████▌| 346/360 [01:04<00:02,  5.36it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  98%|█████████▊| 354/360 [01:04<00:01,  5.45it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  99%|█████████▉| 356/360 [01:05<00:00,  5.47it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67:  99%|█████████▉| 358/360 [01:05<00:00,  5.49it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]\n",
      "Epoch 67: 100%|██████████| 360/360 [01:05<00:00,  5.51it/s, loss=2.09, v_num=1, val_loss=2.6, val_acc=0.419]Adjusting learning rate of group 0 to 2.3209e-03.\n",
      "Adjusting learning rate of group 1 to 1.1604e-04.\n",
      "Epoch 67: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.09, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 68:  84%|████████▍ | 302/360 [01:01<00:11,  4.89it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  84%|████████▍ | 304/360 [01:01<00:11,  4.91it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  86%|████████▌ | 308/360 [01:02<00:10,  4.96it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  86%|████████▌ | 310/360 [01:02<00:10,  4.98it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  87%|████████▋ | 312/360 [01:02<00:09,  5.00it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  87%|████████▋ | 314/360 [01:02<00:09,  5.02it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  88%|████████▊ | 318/360 [01:02<00:08,  5.07it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  89%|████████▉ | 320/360 [01:02<00:07,  5.09it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  89%|████████▉ | 322/360 [01:02<00:07,  5.11it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  90%|█████████ | 324/360 [01:03<00:07,  5.13it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  91%|█████████ | 326/360 [01:03<00:06,  5.15it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  92%|█████████▏| 330/360 [01:03<00:05,  5.20it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  92%|█████████▏| 332/360 [01:03<00:05,  5.22it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  93%|█████████▎| 334/360 [01:03<00:04,  5.24it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  93%|█████████▎| 336/360 [01:03<00:04,  5.26it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  94%|█████████▍| 338/360 [01:03<00:04,  5.28it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  94%|█████████▍| 340/360 [01:04<00:03,  5.30it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  95%|█████████▌| 342/360 [01:04<00:03,  5.32it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  96%|█████████▌| 344/360 [01:04<00:02,  5.34it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  98%|█████████▊| 354/360 [01:04<00:01,  5.45it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  99%|█████████▉| 356/360 [01:05<00:00,  5.47it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68:  99%|█████████▉| 358/360 [01:05<00:00,  5.49it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]\n",
      "Epoch 68: 100%|██████████| 360/360 [01:05<00:00,  5.51it/s, loss=2.12, v_num=1, val_loss=2.62, val_acc=0.414]Adjusting learning rate of group 0 to 2.1896e-03.\n",
      "Adjusting learning rate of group 1 to 1.0948e-04.\n",
      "Epoch 68: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418] \n",
      "Epoch 69:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 69:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  86%|████████▌ | 308/360 [01:02<00:10,  4.96it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  86%|████████▌ | 310/360 [01:02<00:10,  4.98it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  88%|████████▊ | 316/360 [01:02<00:08,  5.05it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  88%|████████▊ | 318/360 [01:02<00:08,  5.07it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  89%|████████▉ | 320/360 [01:02<00:07,  5.09it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  91%|█████████ | 328/360 [01:03<00:06,  5.18it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  92%|█████████▏| 330/360 [01:03<00:05,  5.20it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  92%|█████████▏| 332/360 [01:03<00:05,  5.22it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  93%|█████████▎| 334/360 [01:03<00:04,  5.24it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  94%|█████████▍| 340/360 [01:04<00:03,  5.31it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  97%|█████████▋| 348/360 [01:04<00:02,  5.39it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  97%|█████████▋| 350/360 [01:04<00:01,  5.41it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  98%|█████████▊| 352/360 [01:04<00:01,  5.43it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  98%|█████████▊| 354/360 [01:04<00:01,  5.45it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  99%|█████████▉| 356/360 [01:05<00:00,  5.47it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69:  99%|█████████▉| 358/360 [01:05<00:00,  5.49it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]\n",
      "Epoch 69: 100%|██████████| 360/360 [01:05<00:00,  5.51it/s, loss=2.12, v_num=1, val_loss=2.6, val_acc=0.418]Adjusting learning rate of group 0 to 2.0611e-03.\n",
      "Adjusting learning rate of group 1 to 1.0305e-04.\n",
      "Epoch 69: 100%|██████████| 360/360 [01:05<00:00,  5.47it/s, loss=2.12, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  83%|████████▎ | 300/360 [01:01<00:12,  4.90it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 70:  84%|████████▍ | 302/360 [01:01<00:11,  4.90it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  84%|████████▍ | 304/360 [01:01<00:11,  4.92it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  85%|████████▌ | 306/360 [01:01<00:10,  4.94it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  86%|████████▌ | 308/360 [01:02<00:10,  4.97it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  86%|████████▌ | 310/360 [01:02<00:10,  4.99it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  87%|████████▋ | 312/360 [01:02<00:09,  5.01it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  87%|████████▋ | 314/360 [01:02<00:09,  5.03it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  88%|████████▊ | 316/360 [01:02<00:08,  5.06it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  88%|████████▊ | 318/360 [01:02<00:08,  5.08it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  89%|████████▉ | 320/360 [01:02<00:07,  5.10it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  89%|████████▉ | 322/360 [01:02<00:07,  5.12it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  90%|█████████ | 324/360 [01:03<00:07,  5.14it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  91%|█████████ | 326/360 [01:03<00:06,  5.16it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  91%|█████████ | 328/360 [01:03<00:06,  5.19it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  92%|█████████▏| 330/360 [01:03<00:05,  5.21it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  92%|█████████▏| 332/360 [01:03<00:05,  5.23it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  93%|█████████▎| 334/360 [01:03<00:04,  5.25it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  93%|█████████▎| 336/360 [01:03<00:04,  5.27it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  94%|█████████▍| 338/360 [01:03<00:04,  5.29it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  94%|█████████▍| 340/360 [01:03<00:03,  5.31it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  95%|█████████▌| 342/360 [01:04<00:03,  5.33it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  96%|█████████▌| 344/360 [01:04<00:02,  5.35it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  96%|█████████▌| 346/360 [01:04<00:02,  5.37it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  97%|█████████▋| 348/360 [01:04<00:02,  5.40it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  97%|█████████▋| 350/360 [01:04<00:01,  5.42it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  98%|█████████▊| 352/360 [01:04<00:01,  5.44it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  98%|█████████▊| 354/360 [01:04<00:01,  5.46it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  99%|█████████▉| 356/360 [01:04<00:00,  5.48it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70:  99%|█████████▉| 358/360 [01:05<00:00,  5.50it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]\n",
      "Epoch 70: 100%|██████████| 360/360 [01:05<00:00,  5.52it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.415]Adjusting learning rate of group 0 to 1.9355e-03.\n",
      "Adjusting learning rate of group 1 to 9.6773e-05.\n",
      "Epoch 70: 100%|██████████| 360/360 [01:05<00:00,  5.48it/s, loss=2.02, v_num=1, val_loss=2.61, val_acc=0.419]\n",
      "Epoch 71:   9%|▊         | 31/360 [00:06<01:11,  4.62it/s, loss=2.05, v_num=1, val_loss=2.61, val_acc=0.419] "
     ]
    }
   ],
   "source": [
    "classifier_trainer.fit(classifier, train_dataloader=nyudata.added_train_loader(), val_dataloaders=nyudata.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = classifier.cuda()\n",
    "\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in nyudata.val_dataloader():\n",
    "        images, labels = batch\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7219f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (100 * correct / total)\n",
    "print('/scratch/vvb238/' + checkpointDir + '/' + str(accuracy).replace('.', '') + '-classifier.pth')\n",
    "torch.save(classifier.state_dict(),\n",
    "           '/scratch/vvb238/' + checkpointDir + '/' + str(accuracy).replace('.', '') + '-classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3fdd76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0a076a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
