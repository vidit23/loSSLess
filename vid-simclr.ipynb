{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8619bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "from PIL import Image\n",
    "from simclr import SimCLR\n",
    "from simclr.modules import NT_Xent, get_resnet\n",
    "# from simclr.modules.transformations import TransformsSimCLR\n",
    "from simclr.modules.sync_batchnorm import convert_model\n",
    "from simclr.modules import LARS\n",
    "\n",
    "import resnet\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75aa3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"/scratch/vvb238/simclr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b05403",
   "metadata": {},
   "source": [
    "### This class reads the images and their labels from the root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c2234c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split, transform, limit=0):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            root: Location of the dataset folder, usually it is /dataset\n",
    "            split: The split you want to used, it should be one of train, val or unlabeled.\n",
    "            transform: the transform you want to applied to the images.\n",
    "        \"\"\"\n",
    "\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_dir = os.path.join(root, split)\n",
    "        label_path = os.path.join(root, f\"{split}_label_tensor.pt\")\n",
    "\n",
    "        if limit == 0:\n",
    "            self.num_images = len(os.listdir(self.image_dir))\n",
    "        else:\n",
    "            self.num_images = limit\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            self.labels = torch.load(label_path)\n",
    "        else:\n",
    "            self.labels = -1 * torch.ones(self.num_images, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(os.path.join(self.image_dir, f\"{idx}.png\"), 'rb') as f:\n",
    "            img = Image.open(f).convert('RGB')\n",
    "\n",
    "        return self.transform(img), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c40de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformsSimCLR:\n",
    "    \"\"\"\n",
    "    A stochastic data augmentation module that transforms any given data example randomly\n",
    "    resulting in two correlated views of the same example,\n",
    "    denoted x ̃i and x ̃j, which we consider as a positive pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        s = 1\n",
    "        color_jitter = torchvision.transforms.ColorJitter(\n",
    "            0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s\n",
    "        )\n",
    "        self.train_transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.RandomResizedCrop(size=size),\n",
    "                torchvision.transforms.RandomHorizontalFlip(),  # with 0.5 probability\n",
    "                torchvision.transforms.RandomApply([color_jitter], p=0.8),\n",
    "                torchvision.transforms.RandomGrayscale(p=0.2),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),   \n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.test_transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Resize(size=size),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.train_transform(x), self.train_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b6acf",
   "metadata": {},
   "source": [
    "#### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff8dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYUImageNetDataModule(pl.LightningDataModule):\n",
    "  \n",
    "    def train_dataloader(self):\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        trainset = CustomDataset(root='/dataset', split=\"train\", transform=train_transform)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        eval_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        evalset = CustomDataset(root='/dataset', split=\"val\", transform=eval_transform)\n",
    "        eval_loader = torch.utils.data.DataLoader(evalset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        return eval_loader\n",
    "    \n",
    "    def ssl_train_dataloader(self, batch_size):\n",
    "        unlabeled_dataset = CustomDataset(root='/dataset', split='unlabeled', transform=TransformsSimCLR(96))\n",
    "        unlabeled_dataloader = torch.utils.data.DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        return unlabeled_dataloader\n",
    "        \n",
    "    def ssl_val_dataloader(self, batch_size):\n",
    "        val_dataset = CustomDataset(root='/dataset', split='val', transform=TransformsSimCLR(96))\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698cb4b6",
   "metadata": {},
   "source": [
    "#### Self-supervised Learning (uses the model defined in resnet.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b59ccb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLearning(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # initialize ResNet\n",
    "        self.encoder = resnet.get_custom_resnet18()\n",
    "#         get_resnet(\"resnet18\", pretrained=False)\n",
    "        self.n_features = self.encoder.fc.in_features  # get dimensions of fc layer\n",
    "        self.model = SimCLR(self.encoder, 1024, self.n_features)\n",
    "        self.criterion = NT_Xent(\n",
    "            BATCH_SIZE, 0.5, world_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        h_i, h_j, z_i, z_j = self.model(x_i, x_j)\n",
    "        return z_i, z_j\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop. It is independent of forward\n",
    "        (x_i, x_j), _ = batch\n",
    "        z_i, z_j = self.forward(x_i, x_j)\n",
    "        loss = self.criterion(z_i, z_j)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop. It is independent of forward\n",
    "        (x_i, x_j), _ = batch\n",
    "        z_i, z_j = self.forward(x_i, x_j)\n",
    "        loss = self.criterion(z_i, z_j)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return { 'val_loss' : loss }\n",
    "\n",
    "    def configure_criterion(self):\n",
    "        criterion = NT_Xent(BATCH_SIZE, 0.5)\n",
    "        return criterion\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        scheduler = None\n",
    "#       \"Adam\":\n",
    "#         optimizer = torch.optim.Adam(self.model.parameters(), lr=3e-4)\n",
    "    \n",
    "#       \"LARS\"\n",
    "        # optimized using LARS with linear learning rate scaling\n",
    "        # (i.e. LearningRate = 0.3 × BatchSize/256) and weight decay of 10−6.\n",
    "        learning_rate = 0.3 * BATCH_SIZE / 256\n",
    "        optimizer = LARS(\n",
    "            self.model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=0.000001,\n",
    "            exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",
    "        )\n",
    "\n",
    "        # \"decay the learning rate with the cosine decay schedule without restarts\"\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, EPOCHS, eta_min=0, last_epoch=-1, verbose=True\n",
    "        )\n",
    "\n",
    "        if scheduler:\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "        else:\n",
    "            return {\"optimizer\": optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa179bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b8ae473",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = NYUImageNetDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a849dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr = ContrastiveLearning()\n",
    "# simclr = ContrastiveLearning.load_from_checkpoint(os.path.join(checkpoint_dir, 'simclr.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b256d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', save_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "614a3bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(gpus=1,deterministic=True, max_epochs=EPOCHS, default_root_dir=checkpoint_dir, profiler=\"simple\",\n",
    "                limit_val_batches= 5, precision=16, benchmark=True, callbacks=[checkpoint_callback], fast_dev_run=False,\n",
    "                resume_from_checkpoint=os.path.join(checkpoint_dir, 'lightning_logs/version_0/checkpoints/last.ckpt'))\n",
    "trainer.sync_batchnorm=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18c69b01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | encoder   | ResNet  | 11.2 M\n",
      "1 | model     | SimCLR  | 12.0 M\n",
      "2 | criterion | NT_Xent | 0     \n",
      "--------------------------------------\n",
      "12.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.0 M    Total params\n",
      "47.821    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 3.0000e-01.\n",
      "Epoch 0:   0%|          | 1/2005 [00:32<18:06:28, 32.53s/it, loss=6.22, v_num=0, val_loss=6.230]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/dev/lib/python3.8/site-packages/simclr/modules/lars.py:137: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554793803/work/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  next_v.mul_(momentum).add_(scaled_lr, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 2001/2005 [13:21<00:01,  2.50it/s, loss=4.96, v_num=0, val_loss=6.230]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 2002/2005 [13:22<00:01,  2.50it/s, loss=4.96, v_num=0, val_loss=6.230]\n",
      "Epoch 0: 100%|█████████▉| 2004/2005 [13:22<00:00,  2.50it/s, loss=4.96, v_num=0, val_loss=6.230]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.86it/s]\u001b[AAdjusting learning rate of group 0 to 2.9999e-01.\n",
      "Epoch 0: 100%|██████████| 2005/2005 [13:23<00:00,  2.49it/s, loss=4.96, v_num=0, val_loss=4.990]\n",
      "Epoch 1: 100%|█████████▉| 2000/2005 [10:15<00:01,  3.25it/s, loss=4.86, v_num=0, val_loss=4.990]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 2002/2005 [10:16<00:00,  3.25it/s, loss=4.86, v_num=0, val_loss=4.990]\n",
      "Epoch 1: 100%|█████████▉| 2004/2005 [10:16<00:00,  3.25it/s, loss=4.86, v_num=0, val_loss=4.990]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.88it/s]\u001b[AAdjusting learning rate of group 0 to 2.9997e-01.\n",
      "Epoch 1: 100%|██████████| 2005/2005 [10:18<00:00,  3.24it/s, loss=4.86, v_num=0, val_loss=4.900]\n",
      "Epoch 2: 100%|█████████▉| 2000/2005 [10:15<00:01,  3.25it/s, loss=4.82, v_num=0, val_loss=4.900]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 2002/2005 [10:17<00:00,  3.24it/s, loss=4.82, v_num=0, val_loss=4.900]\n",
      "Epoch 2: 100%|█████████▉| 2004/2005 [10:17<00:00,  3.25it/s, loss=4.82, v_num=0, val_loss=4.900]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.84it/s]\u001b[AAdjusting learning rate of group 0 to 2.9993e-01.\n",
      "Epoch 2: 100%|██████████| 2005/2005 [10:18<00:00,  3.24it/s, loss=4.82, v_num=0, val_loss=4.880]\n",
      "Epoch 3: 100%|█████████▉| 2000/2005 [10:15<00:01,  3.25it/s, loss=4.79, v_num=0, val_loss=4.880]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 2002/2005 [10:16<00:00,  3.25it/s, loss=4.79, v_num=0, val_loss=4.880]\n",
      "Epoch 3: 100%|█████████▉| 2004/2005 [10:17<00:00,  3.25it/s, loss=4.79, v_num=0, val_loss=4.880]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.80it/s]\u001b[AAdjusting learning rate of group 0 to 2.9987e-01.\n",
      "Epoch 3: 100%|██████████| 2005/2005 [10:18<00:00,  3.24it/s, loss=4.79, v_num=0, val_loss=4.850]\n",
      "Epoch 4: 100%|█████████▉| 2000/2005 [10:15<00:01,  3.25it/s, loss=4.78, v_num=0, val_loss=4.850]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 2002/2005 [10:16<00:00,  3.24it/s, loss=4.78, v_num=0, val_loss=4.850]\n",
      "Epoch 4: 100%|█████████▉| 2004/2005 [10:17<00:00,  3.25it/s, loss=4.78, v_num=0, val_loss=4.850]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.78it/s]\u001b[AAdjusting learning rate of group 0 to 2.9979e-01.\n",
      "Epoch 4: 100%|██████████| 2005/2005 [10:18<00:00,  3.24it/s, loss=4.78, v_num=0, val_loss=4.870]\n",
      "Epoch 5: 100%|█████████▉| 2000/2005 [10:16<00:01,  3.25it/s, loss=4.76, v_num=0, val_loss=4.870]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 2002/2005 [10:17<00:00,  3.24it/s, loss=4.76, v_num=0, val_loss=4.870]\n",
      "Epoch 5: 100%|█████████▉| 2004/2005 [10:17<00:00,  3.24it/s, loss=4.76, v_num=0, val_loss=4.870]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.78it/s]\u001b[AAdjusting learning rate of group 0 to 2.9970e-01.\n",
      "Epoch 5: 100%|██████████| 2005/2005 [10:19<00:00,  3.24it/s, loss=4.76, v_num=0, val_loss=4.800]\n",
      "Epoch 6: 100%|█████████▉| 2000/2005 [10:16<00:01,  3.25it/s, loss=4.76, v_num=0, val_loss=4.800]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 2002/2005 [10:17<00:00,  3.24it/s, loss=4.76, v_num=0, val_loss=4.800]\n",
      "Epoch 6: 100%|█████████▉| 2004/2005 [10:17<00:00,  3.25it/s, loss=4.76, v_num=0, val_loss=4.800]\n",
      "Validating: 100%|██████████| 5/5 [00:01<00:00,  2.91it/s]\u001b[AAdjusting learning rate of group 0 to 2.9960e-01.\n",
      "Epoch 6: 100%|██████████| 2005/2005 [10:18<00:00,  3.24it/s, loss=4.76, v_num=0, val_loss=4.770]\n",
      "Epoch 7: 100%|█████████▉| 2000/2005 [10:16<00:01,  3.25it/s, loss=4.74, v_num=0, val_loss=4.770]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 2002/2005 [10:17<00:00,  3.24it/s, loss=4.74, v_num=0, val_loss=4.770]\n",
      "Epoch 7: 100%|█████████▉| 2004/2005 [10:17<00:00,  3.24it/s, loss=4.74, v_num=0, val_loss=4.770]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.79it/s]\u001b[AAdjusting learning rate of group 0 to 2.9947e-01.\n",
      "Epoch 7: 100%|██████████| 2005/2005 [10:19<00:00,  3.24it/s, loss=4.74, v_num=0, val_loss=4.770]\n",
      "Epoch 8: 100%|█████████▉| 2000/2005 [10:16<00:01,  3.25it/s, loss=4.72, v_num=0, val_loss=4.770]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 2002/2005 [10:17<00:00,  3.24it/s, loss=4.72, v_num=0, val_loss=4.770]\n",
      "Epoch 8: 100%|█████████▉| 2004/2005 [10:17<00:00,  3.24it/s, loss=4.72, v_num=0, val_loss=4.770]\n",
      "Validating: 100%|██████████| 5/5 [00:01<00:00,  2.91it/s]\u001b[AAdjusting learning rate of group 0 to 2.9933e-01.\n",
      "Epoch 8: 100%|██████████| 2005/2005 [10:19<00:00,  3.24it/s, loss=4.72, v_num=0, val_loss=4.770]\n",
      "Epoch 9: 100%|█████████▉| 2000/2005 [10:16<00:01,  3.24it/s, loss=4.72, v_num=0, val_loss=4.770]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.72, v_num=0, val_loss=4.770]\n",
      "Epoch 9: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.72, v_num=0, val_loss=4.770]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.79it/s]\u001b[AAdjusting learning rate of group 0 to 2.9918e-01.\n",
      "Epoch 9: 100%|██████████| 2005/2005 [10:19<00:00,  3.23it/s, loss=4.72, v_num=0, val_loss=4.750]\n",
      "Epoch 10: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.72, v_num=0, val_loss=4.750]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.72, v_num=0, val_loss=4.750]\n",
      "Epoch 10: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.72, v_num=0, val_loss=4.750]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.77it/s]\u001b[AAdjusting learning rate of group 0 to 2.9901e-01.\n",
      "Epoch 10: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.72, v_num=0, val_loss=4.760]\n",
      "Epoch 11: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.71, v_num=0, val_loss=4.760]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█████████▉| 2002/2005 [10:19<00:00,  3.23it/s, loss=4.71, v_num=0, val_loss=4.760]\n",
      "Epoch 11: 100%|█████████▉| 2004/2005 [10:19<00:00,  3.24it/s, loss=4.71, v_num=0, val_loss=4.760]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.78it/s]\u001b[AAdjusting learning rate of group 0 to 2.9882e-01.\n",
      "Epoch 11: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.71, v_num=0, val_loss=4.740]\n",
      "Epoch 12: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.71, v_num=0, val_loss=4.740]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.71, v_num=0, val_loss=4.740]\n",
      "Epoch 12: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.71, v_num=0, val_loss=4.740]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.79it/s]\u001b[AAdjusting learning rate of group 0 to 2.9861e-01.\n",
      "Epoch 12: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.71, v_num=0, val_loss=4.770]\n",
      "Epoch 13: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.7, v_num=0, val_loss=4.770] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.7, v_num=0, val_loss=4.770]\n",
      "Epoch 13: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.7, v_num=0, val_loss=4.770]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.71it/s]\u001b[AAdjusting learning rate of group 0 to 2.9839e-01.\n",
      "Epoch 13: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.7, v_num=0, val_loss=4.720]\n",
      "Epoch 14: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.7, v_num=0, val_loss=4.720] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.7, v_num=0, val_loss=4.720]\n",
      "Epoch 14: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.7, v_num=0, val_loss=4.720]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.82it/s]\u001b[AAdjusting learning rate of group 0 to 2.9815e-01.\n",
      "Epoch 14: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.7, v_num=0, val_loss=4.730]\n",
      "Epoch 15: 100%|█████████▉| 2000/2005 [10:16<00:01,  3.24it/s, loss=4.69, v_num=0, val_loss=4.730]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.69, v_num=0, val_loss=4.730]\n",
      "Epoch 15: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.69, v_num=0, val_loss=4.730]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.84it/s]\u001b[AAdjusting learning rate of group 0 to 2.9790e-01.\n",
      "Epoch 15: 100%|██████████| 2005/2005 [10:19<00:00,  3.23it/s, loss=4.69, v_num=0, val_loss=4.710]\n",
      "Epoch 16: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.68, v_num=0, val_loss=4.710]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.68, v_num=0, val_loss=4.710]\n",
      "Epoch 16: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.68, v_num=0, val_loss=4.710]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.74it/s]\u001b[AAdjusting learning rate of group 0 to 2.9763e-01.\n",
      "Epoch 16: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.68, v_num=0, val_loss=4.720]\n",
      "Epoch 17: 100%|█████████▉| 2000/2005 [10:16<00:01,  3.24it/s, loss=4.69, v_num=0, val_loss=4.720]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█████████▉| 2002/2005 [10:17<00:00,  3.24it/s, loss=4.69, v_num=0, val_loss=4.720]\n",
      "Epoch 17: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.69, v_num=0, val_loss=4.720]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.84it/s]\u001b[AAdjusting learning rate of group 0 to 2.9734e-01.\n",
      "Epoch 17: 100%|██████████| 2005/2005 [10:19<00:00,  3.24it/s, loss=4.69, v_num=0, val_loss=4.710]\n",
      "Epoch 18: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.68, v_num=0, val_loss=4.710]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.68, v_num=0, val_loss=4.710]\n",
      "Epoch 18: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.68, v_num=0, val_loss=4.710]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.88it/s]\u001b[AAdjusting learning rate of group 0 to 2.9704e-01.\n",
      "Epoch 18: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.68, v_num=0, val_loss=4.710]\n",
      "Epoch 19: 100%|█████████▉| 2000/2005 [10:16<00:01,  3.24it/s, loss=4.67, v_num=0, val_loss=4.710]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█████████▉| 2002/2005 [10:17<00:00,  3.24it/s, loss=4.67, v_num=0, val_loss=4.710]\n",
      "Epoch 19: 100%|█████████▉| 2004/2005 [10:17<00:00,  3.24it/s, loss=4.67, v_num=0, val_loss=4.710]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.79it/s]\u001b[AAdjusting learning rate of group 0 to 2.9672e-01.\n",
      "Epoch 19: 100%|██████████| 2005/2005 [10:19<00:00,  3.24it/s, loss=4.67, v_num=0, val_loss=4.710]\n",
      "Epoch 20: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.67, v_num=0, val_loss=4.710]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.67, v_num=0, val_loss=4.710]\n",
      "Epoch 20: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.67, v_num=0, val_loss=4.710]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.81it/s]\u001b[AAdjusting learning rate of group 0 to 2.9639e-01.\n",
      "Epoch 20: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.67, v_num=0, val_loss=4.710]\n",
      "Epoch 21: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.67, v_num=0, val_loss=4.710]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.67, v_num=0, val_loss=4.710]\n",
      "Epoch 21: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.67, v_num=0, val_loss=4.710]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.79it/s]\u001b[AAdjusting learning rate of group 0 to 2.9604e-01.\n",
      "Epoch 21: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.67, v_num=0, val_loss=4.700]\n",
      "Epoch 22: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.67, v_num=0, val_loss=4.700]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.67, v_num=0, val_loss=4.700]\n",
      "Epoch 22: 100%|█████████▉| 2004/2005 [10:19<00:00,  3.24it/s, loss=4.67, v_num=0, val_loss=4.700]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.82it/s]\u001b[AAdjusting learning rate of group 0 to 2.9567e-01.\n",
      "Epoch 22: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.67, v_num=0, val_loss=4.700]\n",
      "Epoch 23: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.67, v_num=0, val_loss=4.700]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.67, v_num=0, val_loss=4.700]\n",
      "Epoch 23: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.67, v_num=0, val_loss=4.700]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.63it/s]\u001b[AAdjusting learning rate of group 0 to 2.9529e-01.\n",
      "Epoch 23: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.67, v_num=0, val_loss=4.700]\n",
      "Epoch 24: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.67, v_num=0, val_loss=4.700]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.67, v_num=0, val_loss=4.700]\n",
      "Epoch 24: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.67, v_num=0, val_loss=4.700]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.91it/s]\u001b[AAdjusting learning rate of group 0 to 2.9489e-01.\n",
      "Epoch 24: 100%|██████████| 2005/2005 [10:19<00:00,  3.23it/s, loss=4.67, v_num=0, val_loss=4.700]\n",
      "Epoch 25: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.66, v_num=0, val_loss=4.700]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.66, v_num=0, val_loss=4.700]\n",
      "Epoch 25: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.66, v_num=0, val_loss=4.700]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.87it/s]\u001b[AAdjusting learning rate of group 0 to 2.9447e-01.\n",
      "Epoch 25: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.66, v_num=0, val_loss=4.710]\n",
      "Epoch 26: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.66, v_num=0, val_loss=4.710]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.66, v_num=0, val_loss=4.710]\n",
      "Epoch 26: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.66, v_num=0, val_loss=4.710]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.92it/s]\u001b[AAdjusting learning rate of group 0 to 2.9404e-01.\n",
      "Epoch 26: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.66, v_num=0, val_loss=4.720]\n",
      "Epoch 27: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.66, v_num=0, val_loss=4.720]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.66, v_num=0, val_loss=4.720]\n",
      "Epoch 27: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.66, v_num=0, val_loss=4.720]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.84it/s]\u001b[AAdjusting learning rate of group 0 to 2.9360e-01.\n",
      "Epoch 27: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.66, v_num=0, val_loss=4.690]\n",
      "Epoch 28: 100%|█████████▉| 2000/2005 [10:16<00:01,  3.24it/s, loss=4.66, v_num=0, val_loss=4.690]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.66, v_num=0, val_loss=4.690]\n",
      "Epoch 28: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.66, v_num=0, val_loss=4.690]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.82it/s]\u001b[AAdjusting learning rate of group 0 to 2.9314e-01.\n",
      "Epoch 28: 100%|██████████| 2005/2005 [10:19<00:00,  3.23it/s, loss=4.66, v_num=0, val_loss=4.700]\n",
      "Epoch 29: 100%|█████████▉| 2000/2005 [10:16<00:01,  3.24it/s, loss=4.66, v_num=0, val_loss=4.700]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.66, v_num=0, val_loss=4.700]\n",
      "Epoch 29: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.66, v_num=0, val_loss=4.700]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.80it/s]\u001b[AAdjusting learning rate of group 0 to 2.9266e-01.\n",
      "Epoch 29: 100%|██████████| 2005/2005 [10:19<00:00,  3.23it/s, loss=4.66, v_num=0, val_loss=4.680]\n",
      "Epoch 30: 100%|█████████▉| 2000/2005 [10:16<00:01,  3.24it/s, loss=4.65, v_num=0, val_loss=4.680]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.65, v_num=0, val_loss=4.680]\n",
      "Epoch 30: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.65, v_num=0, val_loss=4.680]\n",
      "Validating:  80%|████████  | 4/5 [00:01<00:00,  3.67it/s]\u001b[A\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.74it/s]\u001b[AAdjusting learning rate of group 0 to 2.9217e-01.\n",
      "Epoch 30: 100%|██████████| 2005/2005 [10:19<00:00,  3.23it/s, loss=4.65, v_num=0, val_loss=4.700]\n",
      "Epoch 31: 100%|█████████▉| 2000/2005 [10:17<00:01,  3.24it/s, loss=4.65, v_num=0, val_loss=4.700]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█████████▉| 2002/2005 [10:18<00:00,  3.24it/s, loss=4.65, v_num=0, val_loss=4.700]\n",
      "Epoch 31: 100%|█████████▉| 2004/2005 [10:18<00:00,  3.24it/s, loss=4.65, v_num=0, val_loss=4.700]\n",
      "Validating: 100%|██████████| 5/5 [00:02<00:00,  2.93it/s]\u001b[AAdjusting learning rate of group 0 to 2.9166e-01.\n",
      "Epoch 31: 100%|██████████| 2005/2005 [10:20<00:00,  3.23it/s, loss=4.65, v_num=0, val_loss=4.670]\n",
      "Epoch 32:   2%|▏         | 32/2005 [00:11<11:22,  2.89it/s, loss=4.66, v_num=0, val_loss=4.670]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/dev/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Saving latest checkpoint...\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/ext3/miniconda3/envs/dev/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/ext3/miniconda3/envs/dev/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/ext3/miniconda3/envs/dev/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/ext3/miniconda3/envs/dev/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/ext3/miniconda3/envs/dev/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/ext3/miniconda3/envs/dev/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/ext3/miniconda3/envs/dev/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/ext3/miniconda3/envs/dev/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32:   2%|▏         | 32/2005 [00:11<11:58,  2.75it/s, loss=4.66, v_num=0, val_loss=4.670]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  2.0069e+04     \t|  100 %          \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  607.24         \t|33             \t|  2.0039e+04     \t|  99.852         \t|\n",
      "run_training_batch                 \t|  0.30228        \t|64033          \t|  1.9356e+04     \t|  96.449         \t|\n",
      "optimizer_step_and_closure_0       \t|  0.30186        \t|64033          \t|  1.9329e+04     \t|  96.316         \t|\n",
      "training_step_and_backward         \t|  0.1091         \t|64033          \t|  6985.9         \t|  34.81          \t|\n",
      "model_forward                      \t|  0.089855       \t|64033          \t|  5753.7         \t|  28.67          \t|\n",
      "training_step                      \t|  0.089617       \t|64033          \t|  5738.5         \t|  28.594         \t|\n",
      "model_backward                     \t|  0.018436       \t|64033          \t|  1180.5         \t|  5.8824         \t|\n",
      "get_train_batch                    \t|  0.0043273      \t|64033          \t|  277.09         \t|  1.3807         \t|\n",
      "on_train_batch_end                 \t|  0.003581       \t|64032          \t|  229.3          \t|  1.1426         \t|\n",
      "evaluation_step_and_end            \t|  0.1018         \t|162            \t|  16.492         \t|  0.082176       \t|\n",
      "validation_step                    \t|  0.10159        \t|162            \t|  16.458         \t|  0.08201        \t|\n",
      "on_validation_end                  \t|  0.29814        \t|33             \t|  9.8385         \t|  0.049024       \t|\n",
      "cache_result                       \t|  1.7804e-05     \t|321013         \t|  5.7154         \t|  0.028479       \t|\n",
      "on_batch_start                     \t|  2.2543e-05     \t|64033          \t|  1.4435         \t|  0.0071927      \t|\n",
      "on_after_backward                  \t|  1.6606e-05     \t|64033          \t|  1.0633         \t|  0.0052985      \t|\n",
      "on_batch_end                       \t|  1.4289e-05     \t|64032          \t|  0.91495        \t|  0.0045591      \t|\n",
      "on_before_zero_grad                \t|  1.4238e-05     \t|64033          \t|  0.91173        \t|  0.0045431      \t|\n",
      "on_train_batch_start               \t|  1.0814e-05     \t|64033          \t|  0.69248        \t|  0.0034506      \t|\n",
      "training_step_end                  \t|  9.9962e-06     \t|64033          \t|  0.64009        \t|  0.0031895      \t|\n",
      "on_validation_batch_end            \t|  0.0023538      \t|162            \t|  0.38132        \t|  0.0019001      \t|\n",
      "on_validation_start                \t|  0.004031       \t|33             \t|  0.13302        \t|  0.00066285     \t|\n",
      "on_train_end                       \t|  0.10705        \t|1              \t|  0.10705        \t|  0.00053342     \t|\n",
      "on_train_epoch_start               \t|  0.001813       \t|33             \t|  0.05983        \t|  0.00029813     \t|\n",
      "on_train_start                     \t|  0.012644       \t|1              \t|  0.012644       \t|  6.3003e-05     \t|\n",
      "on_validation_batch_start          \t|  2.872e-05      \t|162            \t|  0.0046526      \t|  2.3183e-05     \t|\n",
      "validation_step_end                \t|  1.0823e-05     \t|162            \t|  0.0017533      \t|  8.7366e-06     \t|\n",
      "on_validation_epoch_end            \t|  3.774e-05      \t|33             \t|  0.0012454      \t|  6.2059e-06     \t|\n",
      "on_epoch_end                       \t|  1.6612e-05     \t|65             \t|  0.0010798      \t|  5.3805e-06     \t|\n",
      "on_epoch_start                     \t|  1.261e-05      \t|66             \t|  0.00083223     \t|  4.1469e-06     \t|\n",
      "on_train_epoch_end                 \t|  1.9923e-05     \t|32             \t|  0.00063754     \t|  3.1768e-06     \t|\n",
      "on_validation_epoch_start          \t|  1.0228e-05     \t|33             \t|  0.00033753     \t|  1.6819e-06     \t|\n",
      "on_fit_start                       \t|  5.2793e-05     \t|1              \t|  5.2793e-05     \t|  2.6306e-07     \t|\n",
      "on_before_accelerator_backend_setup\t|  2.1338e-05     \t|1              \t|  2.1338e-05     \t|  1.0633e-07     \t|\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(simclr, train_dataloader=data.ssl_train_dataloader(BATCH_SIZE), val_dataloaders=data.ssl_val_dataloader(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3243332",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(simclr.model.encoder.state_dict(), os.path.join(checkpoint_dir, 'simclr_encoder.pth'))\n",
    "torch.save(simclr.model.projector.state_dict(), os.path.join(checkpoint_dir, 'simclr_projector.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606ff1aa",
   "metadata": {},
   "source": [
    "#### Supervised Learning by fine-tuning the SSL model on labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a7d175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simclr.modules.identity import Identity\n",
    "\n",
    "class ResNetClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = resnet.get_custom_resnet18()\n",
    "        self.encoder.fc = Identity()\n",
    "        self.encoder.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'simclr_encoder.pth')))\n",
    "#         self.projector = nn.Sequential(\n",
    "#             nn.Linear(512, 512, bias=False),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 1024, bias=False),\n",
    "#         )\n",
    "#         self.projector.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'simclr_projector.pth')))\n",
    "#         self.lastLayer = torch.nn.Linear(1024, 800)\n",
    "\n",
    "#         self.encoder = simclr.encoder\n",
    "        self.encoder.fc = torch.nn.Linear(512, 800)\n",
    "#         self.lastLayer = torch.nn.Linear(512, 800)\n",
    "        \n",
    "        self.criterion=torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "#         x = self.lastLayer(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, label = batch\n",
    "        classProbs = self.forward(data)\n",
    "        loss = self.criterion(classProbs, label)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def _evaluate(self, batch, batch_idx, stage=None):\n",
    "        x, y = batch\n",
    "        out = self.forward(x)\n",
    "        logits = F.log_softmax(out, dim=-1)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f'{stage}_loss', loss, prog_bar=True)\n",
    "            self.log(f'{stage}_acc', acc, prog_bar=True)\n",
    "\n",
    "        return loss, acc\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        self._evaluate(batch, batch_idx, 'val')[0]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=4, verbose=True)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd655caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ResNetClassifier()\n",
    "# classifier.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'classifier.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "148d8143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "classifier_trainer = Trainer(gpus=1,deterministic=True, max_epochs=EPOCHS, default_root_dir='/scratch/vvb238/classifier', profiler=\"simple\",\n",
    "                     limit_val_batches= 0.5, precision=16, benchmark=True, callbacks=[checkpoint_callback], fast_dev_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d45960e6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder   | ResNet           | 11.6 M\n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "11.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.6 M    Total params\n",
      "46.317    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  67%|██████▋   | 801/1200 [00:22<00:11, 35.49it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  67%|██████▋   | 805/1200 [00:22<00:11, 35.18it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  68%|██████▊   | 819/1200 [00:22<00:10, 35.63it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  70%|██████▉   | 834/1200 [00:23<00:10, 36.13it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  71%|███████   | 849/1200 [00:23<00:09, 36.60it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  72%|███████▏  | 864/1200 [00:23<00:09, 37.08it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  73%|███████▎  | 880/1200 [00:23<00:08, 37.59it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  75%|███████▍  | 896/1200 [00:23<00:07, 38.10it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  76%|███████▌  | 912/1200 [00:23<00:07, 38.60it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  77%|███████▋  | 928/1200 [00:23<00:06, 39.08it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  79%|███████▊  | 944/1200 [00:23<00:06, 39.56it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  80%|████████  | 960/1200 [00:23<00:05, 40.05it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  81%|████████▏ | 976/1200 [00:24<00:05, 40.53it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Validating:  44%|████▍     | 176/400 [00:01<00:01, 142.70it/s]\u001b[A\n",
      "Epoch 0:  83%|████████▎ | 992/1200 [00:24<00:05, 41.01it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.49it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  85%|████████▌ | 1024/1200 [00:24<00:04, 41.97it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.45it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  88%|████████▊ | 1056/1200 [00:24<00:03, 42.88it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.34it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  91%|█████████ | 1088/1200 [00:24<00:02, 43.80it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.26it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  93%|█████████▎| 1120/1200 [00:25<00:01, 44.71it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.15it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.58it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.03it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.47it/s, loss=5.39, v_num=3, val_loss=4.750, val_acc=0.141]\n",
      "Epoch 0: 100%|██████████| 1200/1200 [00:25<00:00, 46.74it/s, loss=5.39, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  67%|██████▋   | 800/1200 [00:22<00:11, 36.01it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:53,  3.52it/s]\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 816/1200 [00:22<00:10, 36.03it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  69%|██████▉   | 832/1200 [00:22<00:10, 36.57it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  71%|███████   | 848/1200 [00:22<00:09, 37.10it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  72%|███████▏  | 864/1200 [00:22<00:08, 37.62it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  73%|███████▎  | 880/1200 [00:23<00:08, 38.14it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  75%|███████▍  | 896/1200 [00:23<00:07, 38.66it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  76%|███████▌  | 912/1200 [00:23<00:07, 39.16it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  77%|███████▋  | 928/1200 [00:23<00:06, 39.66it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  79%|███████▊  | 944/1200 [00:23<00:06, 40.16it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  80%|████████  | 960/1200 [00:23<00:05, 40.65it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  81%|████████▏ | 976/1200 [00:23<00:05, 41.13it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  83%|████████▎ | 992/1200 [00:23<00:05, 41.60it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  84%|████████▍ | 1008/1200 [00:23<00:04, 42.06it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.50it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.96it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.42it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.89it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  91%|█████████ | 1088/1200 [00:24<00:02, 44.33it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.78it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  93%|█████████▎| 1120/1200 [00:24<00:01, 45.24it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.69it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Validating:  84%|████████▍ | 336/400 [00:02<00:00, 146.43it/s]\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 1152/1200 [00:24<00:01, 46.10it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.54it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.96it/s, loss=5.02, v_num=3, val_loss=5.630, val_acc=0.043]\n",
      "Epoch 1: 100%|██████████| 1200/1200 [00:25<00:00, 47.20it/s, loss=5.02, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  67%|██████▋   | 800/1200 [00:22<00:11, 35.66it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:52,  3.54it/s]\u001b[A\n",
      "Epoch 2:  68%|██████▊   | 816/1200 [00:22<00:10, 35.67it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  69%|██████▉   | 832/1200 [00:22<00:10, 36.19it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  71%|███████   | 848/1200 [00:23<00:09, 36.72it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  72%|███████▏  | 864/1200 [00:23<00:09, 37.23it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  73%|███████▎  | 880/1200 [00:23<00:08, 37.73it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  75%|███████▍  | 896/1200 [00:23<00:07, 38.24it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  76%|███████▌  | 912/1200 [00:23<00:07, 38.74it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  77%|███████▋  | 928/1200 [00:23<00:06, 39.24it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  79%|███████▊  | 944/1200 [00:23<00:06, 39.73it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  80%|████████  | 960/1200 [00:23<00:05, 40.22it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  81%|████████▏ | 976/1200 [00:23<00:05, 40.70it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  83%|████████▎ | 992/1200 [00:24<00:05, 41.17it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.63it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.08it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.55it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.01it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.48it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Validating:  68%|██████▊   | 272/400 [00:02<00:00, 146.88it/s]\u001b[A\n",
      "Epoch 2:  91%|█████████ | 1088/1200 [00:24<00:02, 43.92it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.38it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  93%|█████████▎| 1120/1200 [00:24<00:01, 44.82it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.27it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.71it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.15it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.58it/s, loss=4.84, v_num=3, val_loss=5.530, val_acc=0.0522]\n",
      "Epoch 2: 100%|██████████| 1200/1200 [00:25<00:00, 46.84it/s, loss=4.84, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  67%|██████▋   | 800/1200 [00:22<00:11, 35.69it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  68%|██████▊   | 816/1200 [00:22<00:10, 35.73it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  69%|██████▉   | 832/1200 [00:22<00:10, 36.26it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Validating:   8%|▊         | 32/400 [00:00<00:04, 85.10it/s]\u001b[A\n",
      "Epoch 3:  71%|███████   | 848/1200 [00:23<00:09, 36.77it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  72%|███████▏  | 864/1200 [00:23<00:09, 37.27it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  73%|███████▎  | 880/1200 [00:23<00:08, 37.79it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  75%|███████▍  | 896/1200 [00:23<00:07, 38.30it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  76%|███████▌  | 912/1200 [00:23<00:07, 38.79it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  77%|███████▋  | 928/1200 [00:23<00:06, 39.28it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  79%|███████▊  | 944/1200 [00:23<00:06, 39.77it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  80%|████████  | 960/1200 [00:23<00:05, 40.24it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  81%|████████▏ | 976/1200 [00:23<00:05, 40.72it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  83%|████████▎ | 992/1200 [00:24<00:05, 41.19it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.68it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Validating:  52%|█████▏    | 208/400 [00:01<00:01, 143.58it/s]\u001b[A\n",
      "Epoch 3:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.13it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.60it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.06it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.52it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  91%|█████████ | 1088/1200 [00:24<00:02, 43.95it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.39it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  93%|█████████▎| 1120/1200 [00:24<00:01, 44.83it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.28it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.72it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.14it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.58it/s, loss=4.53, v_num=3, val_loss=5.660, val_acc=0.0566]\n",
      "Epoch 3: 100%|██████████| 1200/1200 [00:25<00:00, 46.87it/s, loss=4.53, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  67%|██████▋   | 800/1200 [00:22<00:11, 35.97it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:50,  3.60it/s]\u001b[A\n",
      "Epoch 4:  68%|██████▊   | 816/1200 [00:22<00:10, 35.97it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  69%|██████▉   | 832/1200 [00:22<00:10, 36.48it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  71%|███████   | 848/1200 [00:22<00:09, 37.02it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  72%|███████▏  | 864/1200 [00:23<00:08, 37.53it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  73%|███████▎  | 880/1200 [00:23<00:08, 38.05it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  75%|███████▍  | 896/1200 [00:23<00:07, 38.54it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  76%|███████▌  | 912/1200 [00:23<00:07, 39.05it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  77%|███████▋  | 928/1200 [00:23<00:06, 39.54it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  79%|███████▊  | 944/1200 [00:23<00:06, 40.03it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  80%|████████  | 960/1200 [00:23<00:05, 40.52it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Validating:  40%|████      | 160/400 [00:01<00:01, 139.54it/s]\u001b[A\n",
      "Epoch 4:  81%|████████▏ | 976/1200 [00:23<00:05, 40.99it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  83%|████████▎ | 992/1200 [00:23<00:05, 41.48it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.95it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.42it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.90it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.37it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.81it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  91%|█████████ | 1088/1200 [00:24<00:02, 44.28it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.74it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  93%|█████████▎| 1120/1200 [00:24<00:01, 45.18it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.62it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  96%|█████████▌| 1152/1200 [00:25<00:01, 46.06it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.50it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.94it/s, loss=4.35, v_num=3, val_loss=5.300, val_acc=0.0767]\n",
      "Epoch 4: 100%|██████████| 1200/1200 [00:25<00:00, 47.16it/s, loss=4.35, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  67%|██████▋   | 800/1200 [00:22<00:11, 35.65it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:50,  3.60it/s]\u001b[A\n",
      "Epoch 5:  68%|██████▊   | 816/1200 [00:22<00:10, 35.67it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  69%|██████▉   | 832/1200 [00:22<00:10, 36.20it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  71%|███████   | 848/1200 [00:23<00:09, 36.72it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  72%|███████▏  | 864/1200 [00:23<00:09, 37.25it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  73%|███████▎  | 880/1200 [00:23<00:08, 37.76it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  75%|███████▍  | 896/1200 [00:23<00:07, 38.27it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  76%|███████▌  | 912/1200 [00:23<00:07, 38.77it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  77%|███████▋  | 928/1200 [00:23<00:06, 39.27it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  79%|███████▊  | 944/1200 [00:23<00:06, 39.76it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  80%|████████  | 960/1200 [00:23<00:05, 40.26it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  81%|████████▏ | 976/1200 [00:23<00:05, 40.74it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  83%|████████▎ | 992/1200 [00:24<00:05, 41.23it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.71it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.18it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.64it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.11it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.58it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  91%|█████████ | 1088/1200 [00:24<00:02, 44.01it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.46it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  93%|█████████▎| 1120/1200 [00:24<00:01, 44.88it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Validating:  80%|████████  | 321/400 [00:02<00:00, 139.40it/s]\u001b[A\n",
      "Epoch 5:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.30it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.72it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.15it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.58it/s, loss=4.03, v_num=3, val_loss=5.360, val_acc=0.0846]\n",
      "Epoch 5: 100%|██████████| 1200/1200 [00:25<00:00, 46.83it/s, loss=4.03, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  67%|██████▋   | 800/1200 [00:22<00:11, 35.68it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  68%|██████▊   | 816/1200 [00:22<00:10, 35.74it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Validating:   4%|▍         | 16/400 [00:00<00:07, 53.50it/s]\u001b[A\n",
      "Epoch 6:  69%|██████▉   | 832/1200 [00:22<00:10, 36.26it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  71%|███████   | 848/1200 [00:23<00:09, 36.78it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  72%|███████▏  | 864/1200 [00:23<00:09, 37.30it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  73%|███████▎  | 880/1200 [00:23<00:08, 37.80it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  75%|███████▍  | 896/1200 [00:23<00:07, 38.31it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  76%|███████▌  | 912/1200 [00:23<00:07, 38.81it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  77%|███████▋  | 928/1200 [00:23<00:06, 39.31it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  79%|███████▊  | 944/1200 [00:23<00:06, 39.80it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  80%|████████  | 960/1200 [00:23<00:05, 40.30it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  81%|████████▏ | 976/1200 [00:23<00:05, 40.77it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  83%|████████▎ | 992/1200 [00:24<00:05, 41.24it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.73it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.19it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.63it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.07it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.54it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Validating:  68%|██████▊   | 272/400 [00:02<00:00, 137.49it/s]\u001b[A\n",
      "Epoch 6:  91%|█████████ | 1088/1200 [00:24<00:02, 43.96it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.42it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  93%|█████████▎| 1120/1200 [00:24<00:01, 44.87it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.32it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.76it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.18it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.62it/s, loss=3.88, v_num=3, val_loss=5.410, val_acc=0.0849]\n",
      "Epoch 6: 100%|██████████| 1200/1200 [00:25<00:00, 46.84it/s, loss=3.88, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  67%|██████▋   | 800/1200 [00:22<00:11, 35.87it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:56,  3.42it/s]\u001b[A\n",
      "Epoch 7:  68%|██████▊   | 816/1200 [00:22<00:10, 35.85it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  69%|██████▉   | 832/1200 [00:22<00:10, 36.38it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  71%|███████   | 848/1200 [00:22<00:09, 36.89it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  72%|███████▏  | 864/1200 [00:23<00:08, 37.41it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  73%|███████▎  | 880/1200 [00:23<00:08, 37.92it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  75%|███████▍  | 896/1200 [00:23<00:07, 38.43it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  76%|███████▌  | 912/1200 [00:23<00:07, 38.93it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  77%|███████▋  | 928/1200 [00:23<00:06, 39.43it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  79%|███████▊  | 944/1200 [00:23<00:06, 39.93it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  80%|████████  | 960/1200 [00:23<00:05, 40.41it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  81%|████████▏ | 976/1200 [00:23<00:05, 40.90it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  83%|████████▎ | 992/1200 [00:23<00:05, 41.37it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.85it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.31it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.77it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Validating:  60%|██████    | 240/400 [00:01<00:01, 143.38it/s]\u001b[A\n",
      "Epoch 7:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.22it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.67it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  91%|█████████ | 1088/1200 [00:24<00:02, 44.12it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.57it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  93%|█████████▎| 1120/1200 [00:24<00:01, 45.02it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.47it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.91it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.34it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.78it/s, loss=3.65, v_num=3, val_loss=5.450, val_acc=0.0953]\n",
      "Epoch 7: 100%|██████████| 1200/1200 [00:25<00:00, 47.05it/s, loss=3.65, v_num=3, val_loss=5.510, val_acc=0.098] \n",
      "Epoch 8:  67%|██████▋   | 800/1200 [00:22<00:11, 35.60it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  68%|██████▊   | 816/1200 [00:22<00:10, 35.65it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Validating:   4%|▍         | 16/400 [00:00<00:07, 52.94it/s]\u001b[A\n",
      "Epoch 8:  69%|██████▉   | 832/1200 [00:23<00:10, 36.15it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  71%|███████   | 848/1200 [00:23<00:09, 36.68it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  72%|███████▏  | 864/1200 [00:23<00:09, 37.18it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  73%|███████▎  | 880/1200 [00:23<00:08, 37.70it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  75%|███████▍  | 896/1200 [00:23<00:07, 38.21it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  76%|███████▌  | 912/1200 [00:23<00:07, 38.71it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  77%|███████▋  | 928/1200 [00:23<00:06, 39.19it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  79%|███████▊  | 944/1200 [00:23<00:06, 39.68it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  80%|████████  | 960/1200 [00:23<00:05, 40.17it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  81%|████████▏ | 976/1200 [00:24<00:05, 40.66it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  83%|████████▎ | 992/1200 [00:24<00:05, 41.12it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.57it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.04it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Validating:  56%|█████▌    | 224/400 [00:01<00:01, 137.66it/s]\u001b[A\n",
      "Epoch 8:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.49it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  88%|████████▊ | 1056/1200 [00:24<00:03, 42.94it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.41it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  91%|█████████ | 1088/1200 [00:24<00:02, 43.85it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.30it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  93%|█████████▎| 1120/1200 [00:25<00:01, 44.76it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.21it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.65it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.08it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.50it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]\n",
      "Epoch 8: 100%|██████████| 1200/1200 [00:25<00:00, 46.92it/s, loss=3.49, v_num=3, val_loss=5.510, val_acc=0.098]Epoch     9: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 8: 100%|██████████| 1200/1200 [00:25<00:00, 46.73it/s, loss=3.49, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  67%|██████▋   | 800/1200 [00:22<00:11, 35.72it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  68%|██████▊   | 816/1200 [00:22<00:10, 35.76it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  69%|██████▉   | 832/1200 [00:22<00:10, 36.29it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  71%|███████   | 848/1200 [00:23<00:09, 36.82it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  72%|███████▏  | 864/1200 [00:23<00:08, 37.34it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  73%|███████▎  | 880/1200 [00:23<00:08, 37.86it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  75%|███████▍  | 896/1200 [00:23<00:07, 38.37it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Validating:  24%|██▍       | 96/400 [00:00<00:02, 136.51it/s]\u001b[A\n",
      "Epoch 9:  76%|███████▌  | 912/1200 [00:23<00:07, 38.87it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  77%|███████▋  | 928/1200 [00:23<00:06, 39.36it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  79%|███████▊  | 944/1200 [00:23<00:06, 39.85it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  80%|████████  | 960/1200 [00:23<00:05, 40.34it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  81%|████████▏ | 976/1200 [00:23<00:05, 40.84it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  83%|████████▎ | 992/1200 [00:24<00:05, 41.32it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.80it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.26it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.72it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.17it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.64it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  91%|█████████ | 1088/1200 [00:24<00:02, 44.09it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.54it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  93%|█████████▎| 1120/1200 [00:24<00:01, 44.99it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.44it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.88it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.31it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.74it/s, loss=2.38, v_num=3, val_loss=5.730, val_acc=0.0936]\n",
      "Epoch 9: 100%|██████████| 1200/1200 [00:25<00:00, 46.94it/s, loss=2.38, v_num=3, val_loss=5.570, val_acc=0.132] \n",
      "Epoch 10:  67%|██████▋   | 800/1200 [00:22<00:11, 35.72it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:52,  3.56it/s]\u001b[A\n",
      "Epoch 10:  68%|██████▊   | 816/1200 [00:22<00:10, 35.73it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  69%|██████▉   | 832/1200 [00:22<00:10, 36.27it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  71%|███████   | 848/1200 [00:23<00:09, 36.79it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  72%|███████▏  | 864/1200 [00:23<00:09, 37.32it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  73%|███████▎  | 880/1200 [00:23<00:08, 37.82it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  75%|███████▍  | 896/1200 [00:23<00:07, 38.33it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  76%|███████▌  | 912/1200 [00:23<00:07, 38.83it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  77%|███████▋  | 928/1200 [00:23<00:06, 39.32it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  79%|███████▊  | 944/1200 [00:23<00:06, 39.82it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  80%|████████  | 960/1200 [00:23<00:05, 40.31it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  81%|████████▏ | 976/1200 [00:23<00:05, 40.80it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  83%|████████▎ | 992/1200 [00:24<00:05, 41.28it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.76it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.22it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.69it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.16it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.63it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  91%|█████████ | 1088/1200 [00:24<00:02, 44.09it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.54it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Validating:  76%|███████▌  | 304/400 [00:02<00:00, 149.61it/s]\u001b[A\n",
      "Epoch 10:  93%|█████████▎| 1120/1200 [00:24<00:01, 44.97it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.42it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.87it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.31it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.75it/s, loss=2.24, v_num=3, val_loss=5.570, val_acc=0.132]\n",
      "Epoch 10: 100%|██████████| 1200/1200 [00:25<00:00, 47.00it/s, loss=2.24, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  67%|██████▋   | 800/1200 [00:22<00:11, 35.84it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:47,  3.72it/s]\u001b[A\n",
      "Epoch 11:  68%|██████▊   | 816/1200 [00:22<00:10, 35.86it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  69%|██████▉   | 832/1200 [00:22<00:10, 36.39it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  71%|███████   | 848/1200 [00:22<00:09, 36.91it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  72%|███████▏  | 864/1200 [00:23<00:08, 37.43it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  73%|███████▎  | 880/1200 [00:23<00:08, 37.93it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  75%|███████▍  | 896/1200 [00:23<00:07, 38.44it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  76%|███████▌  | 912/1200 [00:23<00:07, 38.93it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  77%|███████▋  | 928/1200 [00:23<00:06, 39.43it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  79%|███████▊  | 944/1200 [00:23<00:06, 39.91it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  80%|████████  | 960/1200 [00:23<00:05, 40.38it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Validating:  40%|████      | 160/400 [00:01<00:01, 135.75it/s]\u001b[A\n",
      "Epoch 11:  81%|████████▏ | 976/1200 [00:23<00:05, 40.86it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  83%|████████▎ | 992/1200 [00:23<00:05, 41.35it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.83it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.31it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.78it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.24it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.71it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  91%|█████████ | 1088/1200 [00:24<00:02, 44.17it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.62it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  93%|█████████▎| 1120/1200 [00:24<00:01, 45.06it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.51it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.93it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.35it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.78it/s, loss=2.09, v_num=3, val_loss=5.720, val_acc=0.133]\n",
      "Epoch 11: 100%|██████████| 1200/1200 [00:25<00:00, 47.02it/s, loss=2.09, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  67%|██████▋   | 800/1200 [00:22<00:11, 35.69it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:47,  3.70it/s]\u001b[A\n",
      "Epoch 12:  68%|██████▊   | 816/1200 [00:22<00:10, 35.72it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  69%|██████▉   | 832/1200 [00:22<00:10, 36.25it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  71%|███████   | 848/1200 [00:23<00:09, 36.76it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  72%|███████▏  | 864/1200 [00:23<00:09, 37.25it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  73%|███████▎  | 880/1200 [00:23<00:08, 37.76it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  75%|███████▍  | 896/1200 [00:23<00:07, 38.28it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  76%|███████▌  | 912/1200 [00:23<00:07, 38.79it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  77%|███████▋  | 928/1200 [00:23<00:06, 39.28it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  79%|███████▊  | 944/1200 [00:23<00:06, 39.76it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  80%|████████  | 960/1200 [00:23<00:05, 40.25it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  81%|████████▏ | 976/1200 [00:23<00:05, 40.73it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  83%|████████▎ | 992/1200 [00:24<00:05, 41.21it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.66it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Validating:  52%|█████▏    | 208/400 [00:01<00:01, 138.80it/s]\u001b[A\n",
      "Epoch 12:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.13it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.58it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.05it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.49it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  91%|█████████ | 1088/1200 [00:24<00:02, 43.95it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.40it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  93%|█████████▎| 1120/1200 [00:24<00:01, 44.85it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.28it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.73it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.16it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.60it/s, loss=2.01, v_num=3, val_loss=5.810, val_acc=0.133]\n",
      "Epoch 12: 100%|██████████| 1200/1200 [00:25<00:00, 46.82it/s, loss=2.01, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  67%|██████▋   | 800/1200 [00:22<00:11, 35.80it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:50,  3.60it/s]\u001b[A\n",
      "Epoch 13:  68%|██████▊   | 816/1200 [00:22<00:10, 35.81it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  69%|██████▉   | 832/1200 [00:22<00:10, 36.33it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  71%|███████   | 848/1200 [00:23<00:09, 36.85it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  72%|███████▏  | 864/1200 [00:23<00:08, 37.36it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  73%|███████▎  | 880/1200 [00:23<00:08, 37.85it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  75%|███████▍  | 896/1200 [00:23<00:07, 38.37it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  76%|███████▌  | 912/1200 [00:23<00:07, 38.86it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  77%|███████▋  | 928/1200 [00:23<00:06, 39.37it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  79%|███████▊  | 944/1200 [00:23<00:06, 39.87it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  80%|████████  | 960/1200 [00:23<00:05, 40.37it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  81%|████████▏ | 976/1200 [00:23<00:05, 40.85it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  83%|████████▎ | 992/1200 [00:24<00:05, 41.32it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.78it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.26it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.74it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Validating:  60%|██████    | 240/400 [00:01<00:01, 146.48it/s]\u001b[A\n",
      "Epoch 13:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.20it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.65it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  91%|█████████ | 1088/1200 [00:24<00:02, 44.11it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.55it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  93%|█████████▎| 1120/1200 [00:24<00:01, 45.00it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.44it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.88it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.30it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.73it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]\n",
      "Epoch 13: 100%|██████████| 1200/1200 [00:25<00:00, 47.15it/s, loss=1.96, v_num=3, val_loss=5.930, val_acc=0.133]Epoch    14: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 13: 100%|██████████| 1200/1200 [00:25<00:00, 46.96it/s, loss=1.96, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  67%|██████▋   | 800/1200 [00:22<00:11, 35.38it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:49,  3.64it/s]\u001b[A\n",
      "Epoch 14:  68%|██████▊   | 816/1200 [00:23<00:10, 35.42it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  69%|██████▉   | 832/1200 [00:23<00:10, 35.94it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  71%|███████   | 848/1200 [00:23<00:09, 36.44it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  72%|███████▏  | 864/1200 [00:23<00:09, 36.95it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  73%|███████▎  | 880/1200 [00:23<00:08, 37.45it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  75%|███████▍  | 896/1200 [00:23<00:08, 37.96it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  76%|███████▌  | 912/1200 [00:23<00:07, 38.45it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  77%|███████▋  | 928/1200 [00:23<00:06, 38.94it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  79%|███████▊  | 944/1200 [00:23<00:06, 39.43it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  80%|████████  | 960/1200 [00:24<00:06, 39.93it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  81%|████████▏ | 976/1200 [00:24<00:05, 40.42it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  83%|████████▎ | 992/1200 [00:24<00:05, 40.87it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.35it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  85%|████████▌ | 1024/1200 [00:24<00:04, 41.82it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.28it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  88%|████████▊ | 1056/1200 [00:24<00:03, 42.74it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.20it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Validating:  68%|██████▊   | 272/400 [00:02<00:00, 147.24it/s]\u001b[A\n",
      "Epoch 14:  91%|█████████ | 1088/1200 [00:24<00:02, 43.65it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  92%|█████████▏| 1104/1200 [00:25<00:02, 44.10it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  93%|█████████▎| 1120/1200 [00:25<00:01, 44.54it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  95%|█████████▍| 1136/1200 [00:25<00:01, 44.97it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.41it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  97%|█████████▋| 1168/1200 [00:25<00:00, 45.84it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.27it/s, loss=1.76, v_num=3, val_loss=5.970, val_acc=0.133]\n",
      "Epoch 14: 100%|██████████| 1200/1200 [00:25<00:00, 46.55it/s, loss=1.76, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  67%|██████▋   | 800/1200 [00:22<00:11, 35.87it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  68%|██████▊   | 816/1200 [00:22<00:10, 35.92it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Validating:   4%|▍         | 16/400 [00:00<00:07, 51.59it/s]\u001b[A\n",
      "Epoch 15:  69%|██████▉   | 832/1200 [00:22<00:10, 36.42it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  71%|███████   | 848/1200 [00:22<00:09, 36.95it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  72%|███████▏  | 864/1200 [00:23<00:08, 37.47it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  73%|███████▎  | 880/1200 [00:23<00:08, 37.97it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  75%|███████▍  | 896/1200 [00:23<00:07, 38.48it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  76%|███████▌  | 912/1200 [00:23<00:07, 38.99it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  77%|███████▋  | 928/1200 [00:23<00:06, 39.49it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  79%|███████▊  | 944/1200 [00:23<00:06, 39.97it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  80%|████████  | 960/1200 [00:23<00:05, 40.46it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  81%|████████▏ | 976/1200 [00:23<00:05, 40.94it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  83%|████████▎ | 992/1200 [00:23<00:05, 41.42it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.90it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.38it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.85it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.33it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.79it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  91%|█████████ | 1088/1200 [00:24<00:02, 44.25it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.71it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  93%|█████████▎| 1120/1200 [00:24<00:01, 45.16it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.61it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  96%|█████████▌| 1152/1200 [00:25<00:01, 46.05it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.49it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.92it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15: 100%|██████████| 1200/1200 [00:25<00:00, 47.33it/s, loss=1.86, v_num=3, val_loss=6.020, val_acc=0.134]\n",
      "Epoch 15: 100%|██████████| 1200/1200 [00:25<00:00, 47.15it/s, loss=1.86, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  67%|██████▋   | 800/1200 [00:22<00:11, 35.64it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  68%|██████▊   | 816/1200 [00:22<00:10, 35.69it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Validating:   4%|▍         | 16/400 [00:00<00:07, 52.37it/s]\u001b[A\n",
      "Epoch 16:  69%|██████▉   | 832/1200 [00:22<00:10, 36.21it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  71%|███████   | 848/1200 [00:23<00:09, 36.73it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  72%|███████▏  | 864/1200 [00:23<00:09, 37.26it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  73%|███████▎  | 880/1200 [00:23<00:08, 37.77it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  75%|███████▍  | 896/1200 [00:23<00:07, 38.27it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  76%|███████▌  | 912/1200 [00:23<00:07, 38.77it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  77%|███████▋  | 928/1200 [00:23<00:06, 39.25it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  79%|███████▊  | 944/1200 [00:23<00:06, 39.72it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  80%|████████  | 960/1200 [00:23<00:05, 40.20it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  81%|████████▏ | 976/1200 [00:23<00:05, 40.68it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  83%|████████▎ | 992/1200 [00:24<00:05, 41.17it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.66it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.13it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.58it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.04it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.49it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  91%|█████████ | 1088/1200 [00:24<00:02, 43.96it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.41it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  93%|█████████▎| 1120/1200 [00:24<00:01, 44.86it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Validating:  80%|████████  | 320/400 [00:02<00:00, 149.21it/s]\u001b[A\n",
      "Epoch 16:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.29it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.73it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.17it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.59it/s, loss=1.82, v_num=3, val_loss=6.010, val_acc=0.132]\n",
      "Epoch 16: 100%|██████████| 1200/1200 [00:25<00:00, 46.83it/s, loss=1.82, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  67%|██████▋   | 800/1200 [00:22<00:11, 35.74it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:53,  3.53it/s]\u001b[A\n",
      "Epoch 17:  68%|██████▊   | 816/1200 [00:22<00:10, 35.76it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  69%|██████▉   | 832/1200 [00:22<00:10, 36.28it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  71%|███████   | 848/1200 [00:23<00:09, 36.81it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  72%|███████▏  | 864/1200 [00:23<00:09, 37.33it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  73%|███████▎  | 880/1200 [00:23<00:08, 37.84it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  75%|███████▍  | 896/1200 [00:23<00:07, 38.35it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  76%|███████▌  | 912/1200 [00:23<00:07, 38.84it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  77%|███████▋  | 928/1200 [00:23<00:06, 39.34it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  79%|███████▊  | 944/1200 [00:23<00:06, 39.82it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  80%|████████  | 960/1200 [00:23<00:05, 40.31it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  81%|████████▏ | 976/1200 [00:23<00:05, 40.80it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  83%|████████▎ | 992/1200 [00:24<00:05, 41.28it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.75it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.23it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.69it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.15it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.60it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  91%|█████████ | 1088/1200 [00:24<00:02, 44.06it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.50it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  93%|█████████▎| 1120/1200 [00:24<00:01, 44.95it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Validating:  80%|████████  | 320/400 [00:02<00:00, 146.21it/s]\u001b[A\n",
      "Epoch 17:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.39it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.83it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.28it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.72it/s, loss=1.77, v_num=3, val_loss=6.020, val_acc=0.133]\n",
      "Epoch 17: 100%|██████████| 1200/1200 [00:25<00:00, 46.99it/s, loss=1.77, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  67%|██████▋   | 800/1200 [00:22<00:11, 35.67it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  68%|██████▊   | 816/1200 [00:22<00:10, 35.71it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  69%|██████▉   | 832/1200 [00:22<00:10, 36.25it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Validating:   8%|▊         | 32/400 [00:00<00:04, 86.78it/s]\u001b[A\n",
      "Epoch 18:  71%|███████   | 848/1200 [00:23<00:09, 36.76it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  72%|███████▏  | 864/1200 [00:23<00:09, 37.27it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  73%|███████▎  | 880/1200 [00:23<00:08, 37.79it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  75%|███████▍  | 896/1200 [00:23<00:07, 38.30it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  76%|███████▌  | 912/1200 [00:23<00:07, 38.79it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  77%|███████▋  | 928/1200 [00:23<00:06, 39.29it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  79%|███████▊  | 944/1200 [00:23<00:06, 39.78it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  80%|████████  | 960/1200 [00:23<00:05, 40.27it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  81%|████████▏ | 976/1200 [00:23<00:05, 40.75it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  83%|████████▎ | 992/1200 [00:24<00:05, 41.23it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.70it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.16it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.63it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.10it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.56it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  91%|█████████ | 1088/1200 [00:24<00:02, 44.01it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.47it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Validating:  76%|███████▌  | 304/400 [00:02<00:00, 146.79it/s]\u001b[A\n",
      "Epoch 18:  93%|█████████▎| 1120/1200 [00:24<00:01, 44.91it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.34it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.79it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.23it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.67it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 18: 100%|██████████| 1200/1200 [00:25<00:00, 47.10it/s, loss=1.71, v_num=3, val_loss=6.030, val_acc=0.135]Epoch    19: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 18: 100%|██████████| 1200/1200 [00:25<00:00, 46.92it/s, loss=1.71, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  67%|██████▋   | 800/1200 [00:22<00:11, 35.78it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  68%|██████▊   | 816/1200 [00:22<00:10, 35.81it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Validating:   4%|▍         | 16/400 [00:00<00:07, 51.19it/s]\u001b[A\n",
      "Epoch 19:  69%|██████▉   | 832/1200 [00:22<00:10, 36.33it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  71%|███████   | 848/1200 [00:23<00:09, 36.85it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  72%|███████▏  | 864/1200 [00:23<00:08, 37.37it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  73%|███████▎  | 880/1200 [00:23<00:08, 37.88it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  75%|███████▍  | 896/1200 [00:23<00:07, 38.39it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  76%|███████▌  | 912/1200 [00:23<00:07, 38.89it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  77%|███████▋  | 928/1200 [00:23<00:06, 39.37it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  79%|███████▊  | 944/1200 [00:23<00:06, 39.85it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  80%|████████  | 960/1200 [00:23<00:05, 40.34it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  81%|████████▏ | 976/1200 [00:23<00:05, 40.83it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  83%|████████▎ | 992/1200 [00:24<00:05, 41.13it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.60it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.08it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.55it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.02it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.48it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  91%|█████████ | 1088/1200 [00:24<00:02, 43.93it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.38it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  93%|█████████▎| 1120/1200 [00:24<00:01, 44.83it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.28it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.73it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.17it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.61it/s, loss=1.88, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 19: 100%|██████████| 1200/1200 [00:25<00:00, 46.82it/s, loss=1.88, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  67%|██████▋   | 800/1200 [00:22<00:11, 35.97it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  68%|██████▊   | 816/1200 [00:22<00:10, 36.03it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Validating:   4%|▍         | 16/400 [00:00<00:07, 53.72it/s]\u001b[A\n",
      "Epoch 20:  69%|██████▉   | 832/1200 [00:22<00:10, 36.54it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  71%|███████   | 848/1200 [00:22<00:09, 37.06it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  72%|███████▏  | 864/1200 [00:22<00:08, 37.58it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  73%|███████▎  | 880/1200 [00:23<00:08, 38.09it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  75%|███████▍  | 896/1200 [00:23<00:07, 38.59it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  76%|███████▌  | 912/1200 [00:23<00:07, 39.11it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  77%|███████▋  | 928/1200 [00:23<00:06, 39.61it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  79%|███████▊  | 944/1200 [00:23<00:06, 40.11it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  80%|████████  | 960/1200 [00:23<00:05, 40.61it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  81%|████████▏ | 976/1200 [00:23<00:05, 41.09it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  83%|████████▎ | 992/1200 [00:23<00:05, 41.57it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  84%|████████▍ | 1008/1200 [00:23<00:04, 42.05it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.52it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  87%|████████▋ | 1040/1200 [00:24<00:03, 43.00it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.46it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.93it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  91%|█████████ | 1088/1200 [00:24<00:02, 44.37it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.83it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Validating:  76%|███████▌  | 304/400 [00:02<00:00, 147.11it/s]\u001b[A\n",
      "Epoch 20:  93%|█████████▎| 1120/1200 [00:24<00:01, 45.27it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.71it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  96%|█████████▌| 1152/1200 [00:24<00:01, 46.15it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.59it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20:  99%|█████████▊| 1184/1200 [00:25<00:00, 47.03it/s, loss=1.76, v_num=3, val_loss=6.060, val_acc=0.134]\n",
      "Epoch 20: 100%|██████████| 1200/1200 [00:25<00:00, 47.28it/s, loss=1.76, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  67%|██████▋   | 800/1200 [00:22<00:11, 35.61it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  68%|██████▊   | 816/1200 [00:22<00:10, 35.67it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  69%|██████▉   | 832/1200 [00:22<00:10, 36.19it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Validating:   8%|▊         | 32/400 [00:00<00:04, 84.46it/s]\u001b[A\n",
      "Epoch 21:  71%|███████   | 848/1200 [00:23<00:09, 36.70it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  72%|███████▏  | 864/1200 [00:23<00:09, 37.22it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  73%|███████▎  | 880/1200 [00:23<00:08, 37.73it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  75%|███████▍  | 896/1200 [00:23<00:07, 38.24it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  76%|███████▌  | 912/1200 [00:23<00:07, 38.74it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  77%|███████▋  | 928/1200 [00:23<00:06, 39.23it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  79%|███████▊  | 944/1200 [00:23<00:06, 39.71it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  80%|████████  | 960/1200 [00:23<00:05, 40.21it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  81%|████████▏ | 976/1200 [00:23<00:05, 40.71it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  83%|████████▎ | 992/1200 [00:24<00:05, 41.18it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.65it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.12it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.57it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.04it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.50it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  91%|█████████ | 1088/1200 [00:24<00:02, 43.95it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.41it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  93%|█████████▎| 1120/1200 [00:24<00:01, 44.86it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.31it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Validating:  84%|████████▍ | 336/400 [00:02<00:00, 144.84it/s]\u001b[A\n",
      "Epoch 21:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.71it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.15it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.59it/s, loss=1.72, v_num=3, val_loss=6.040, val_acc=0.135]\n",
      "Epoch 21: 100%|██████████| 1200/1200 [00:25<00:00, 46.85it/s, loss=1.72, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  67%|██████▋   | 800/1200 [00:22<00:11, 35.95it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:49,  3.64it/s]\u001b[A\n",
      "Epoch 22:  68%|██████▊   | 816/1200 [00:22<00:10, 35.99it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  69%|██████▉   | 832/1200 [00:22<00:10, 36.53it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  71%|███████   | 848/1200 [00:22<00:09, 37.05it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  72%|███████▏  | 864/1200 [00:22<00:08, 37.58it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  73%|███████▎  | 880/1200 [00:23<00:08, 38.11it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  75%|███████▍  | 896/1200 [00:23<00:07, 38.62it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  76%|███████▌  | 912/1200 [00:23<00:07, 39.13it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  77%|███████▋  | 928/1200 [00:23<00:06, 39.63it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  79%|███████▊  | 944/1200 [00:23<00:06, 40.11it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  80%|████████  | 960/1200 [00:23<00:05, 40.61it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  81%|████████▏ | 976/1200 [00:23<00:05, 41.08it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  83%|████████▎ | 992/1200 [00:23<00:05, 41.57it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  84%|████████▍ | 1008/1200 [00:23<00:04, 42.05it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.54it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  87%|████████▋ | 1040/1200 [00:24<00:03, 43.01it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.46it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.93it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  91%|█████████ | 1088/1200 [00:24<00:02, 44.39it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.84it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  93%|█████████▎| 1120/1200 [00:24<00:01, 45.29it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.73it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  96%|█████████▌| 1152/1200 [00:24<00:01, 46.17it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.61it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22:  99%|█████████▊| 1184/1200 [00:25<00:00, 47.06it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.135]\n",
      "Epoch 22: 100%|██████████| 1200/1200 [00:25<00:00, 47.27it/s, loss=1.82, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  67%|██████▋   | 800/1200 [00:22<00:11, 35.96it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:59,  3.34it/s]\u001b[A\n",
      "Epoch 23:  68%|██████▊   | 816/1200 [00:22<00:10, 35.92it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  69%|██████▉   | 832/1200 [00:22<00:10, 36.46it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  71%|███████   | 848/1200 [00:22<00:09, 36.98it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  72%|███████▏  | 864/1200 [00:23<00:08, 37.49it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  73%|███████▎  | 880/1200 [00:23<00:08, 38.00it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  75%|███████▍  | 896/1200 [00:23<00:07, 38.50it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  76%|███████▌  | 912/1200 [00:23<00:07, 38.99it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  77%|███████▋  | 928/1200 [00:23<00:06, 39.48it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  79%|███████▊  | 944/1200 [00:23<00:06, 39.97it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  80%|████████  | 960/1200 [00:23<00:05, 40.46it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  81%|████████▏ | 976/1200 [00:23<00:05, 40.96it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  83%|████████▎ | 992/1200 [00:23<00:05, 41.43it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.91it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.39it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Validating:  56%|█████▌    | 224/400 [00:01<00:01, 147.19it/s]\u001b[A\n",
      "Epoch 23:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.85it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.32it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.78it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  91%|█████████ | 1088/1200 [00:24<00:02, 44.22it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.67it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  93%|█████████▎| 1120/1200 [00:24<00:01, 45.11it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.55it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.98it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.41it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.84it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 23: 100%|██████████| 1200/1200 [00:25<00:00, 47.26it/s, loss=1.7, v_num=3, val_loss=6.060, val_acc=0.133]Epoch    24: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 23: 100%|██████████| 1200/1200 [00:25<00:00, 47.09it/s, loss=1.7, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  67%|██████▋   | 800/1200 [00:22<00:11, 35.97it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24:  68%|██████▊   | 816/1200 [00:22<00:10, 36.02it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Validating:   4%|▍         | 16/400 [00:00<00:07, 52.71it/s]\u001b[A\n",
      "Epoch 24:  69%|██████▉   | 832/1200 [00:22<00:10, 36.54it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  71%|███████   | 848/1200 [00:22<00:09, 37.05it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  72%|███████▏  | 864/1200 [00:22<00:08, 37.58it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  73%|███████▎  | 880/1200 [00:23<00:08, 38.09it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  75%|███████▍  | 896/1200 [00:23<00:07, 38.61it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  76%|███████▌  | 912/1200 [00:23<00:07, 39.11it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  77%|███████▋  | 928/1200 [00:23<00:06, 39.60it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  79%|███████▊  | 944/1200 [00:23<00:06, 40.09it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  80%|████████  | 960/1200 [00:23<00:05, 40.58it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  81%|████████▏ | 976/1200 [00:23<00:05, 41.06it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  83%|████████▎ | 992/1200 [00:23<00:05, 41.55it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  84%|████████▍ | 1008/1200 [00:23<00:04, 42.03it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.51it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.97it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.44it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.90it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  91%|█████████ | 1088/1200 [00:24<00:02, 44.36it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.82it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  93%|█████████▎| 1120/1200 [00:24<00:01, 45.27it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Validating:  80%|████████  | 320/400 [00:02<00:00, 146.23it/s]\u001b[A\n",
      "Epoch 24:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.69it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  96%|█████████▌| 1152/1200 [00:24<00:01, 46.13it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.56it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24:  99%|█████████▊| 1184/1200 [00:25<00:00, 47.00it/s, loss=1.74, v_num=3, val_loss=6.070, val_acc=0.136]\n",
      "Epoch 24: 100%|██████████| 1200/1200 [00:25<00:00, 47.23it/s, loss=1.74, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  67%|██████▋   | 800/1200 [00:22<00:11, 35.76it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:51,  3.59it/s]\u001b[A\n",
      "Epoch 25:  68%|██████▊   | 816/1200 [00:22<00:10, 35.64it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  69%|██████▉   | 832/1200 [00:23<00:10, 36.17it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  71%|███████   | 848/1200 [00:23<00:09, 36.69it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  72%|███████▏  | 864/1200 [00:23<00:09, 37.20it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  73%|███████▎  | 880/1200 [00:23<00:08, 37.72it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  75%|███████▍  | 896/1200 [00:23<00:07, 38.23it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  76%|███████▌  | 912/1200 [00:23<00:07, 38.73it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  77%|███████▋  | 928/1200 [00:23<00:06, 39.23it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  79%|███████▊  | 944/1200 [00:23<00:06, 39.71it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  80%|████████  | 960/1200 [00:23<00:05, 40.19it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  81%|████████▏ | 976/1200 [00:23<00:05, 40.68it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  83%|████████▎ | 992/1200 [00:24<00:05, 41.14it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.60it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.07it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.53it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.00it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.47it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Validating:  68%|██████▊   | 272/400 [00:02<00:00, 146.73it/s]\u001b[A\n",
      "Epoch 25:  91%|█████████ | 1088/1200 [00:24<00:02, 43.89it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.35it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  93%|█████████▎| 1120/1200 [00:25<00:01, 44.79it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.23it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.67it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.11it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.54it/s, loss=1.65, v_num=3, val_loss=6.030, val_acc=0.135]\n",
      "Epoch 25: 100%|██████████| 1200/1200 [00:25<00:00, 46.77it/s, loss=1.65, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  67%|██████▋   | 800/1200 [00:22<00:11, 35.91it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26:  68%|██████▊   | 816/1200 [00:22<00:10, 35.95it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  69%|██████▉   | 832/1200 [00:22<00:10, 36.48it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Validating:   8%|▊         | 32/400 [00:00<00:04, 85.66it/s]\u001b[A\n",
      "Epoch 26:  71%|███████   | 848/1200 [00:22<00:09, 37.00it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  72%|███████▏  | 864/1200 [00:23<00:08, 37.52it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  73%|███████▎  | 880/1200 [00:23<00:08, 38.03it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  75%|███████▍  | 896/1200 [00:23<00:07, 38.55it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  76%|███████▌  | 912/1200 [00:23<00:07, 39.06it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  77%|███████▋  | 928/1200 [00:23<00:06, 39.57it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  79%|███████▊  | 944/1200 [00:23<00:06, 40.03it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  80%|████████  | 960/1200 [00:23<00:05, 40.52it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  81%|████████▏ | 976/1200 [00:23<00:05, 41.01it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  83%|████████▎ | 992/1200 [00:23<00:05, 41.50it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.99it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.46it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.92it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.38it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.84it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  91%|█████████ | 1088/1200 [00:24<00:02, 44.30it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.74it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  93%|█████████▎| 1120/1200 [00:24<00:01, 45.17it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.61it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  96%|█████████▌| 1152/1200 [00:25<00:01, 46.05it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.49it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.93it/s, loss=1.67, v_num=3, val_loss=6.040, val_acc=0.134]\n",
      "Epoch 26: 100%|██████████| 1200/1200 [00:25<00:00, 47.15it/s, loss=1.67, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  67%|██████▋   | 800/1200 [00:22<00:11, 35.78it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:47,  3.71it/s]\u001b[A\n",
      "Epoch 27:  68%|██████▊   | 816/1200 [00:22<00:10, 35.82it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  69%|██████▉   | 832/1200 [00:22<00:10, 36.35it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  71%|███████   | 848/1200 [00:22<00:09, 36.88it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  72%|███████▏  | 864/1200 [00:23<00:08, 37.40it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  73%|███████▎  | 880/1200 [00:23<00:08, 37.89it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  75%|███████▍  | 896/1200 [00:23<00:07, 38.40it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  76%|███████▌  | 912/1200 [00:23<00:07, 38.90it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  77%|███████▋  | 928/1200 [00:23<00:06, 39.40it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  79%|███████▊  | 944/1200 [00:23<00:06, 39.90it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  80%|████████  | 960/1200 [00:23<00:05, 40.37it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  81%|████████▏ | 976/1200 [00:23<00:05, 40.85it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  83%|████████▎ | 992/1200 [00:24<00:05, 41.32it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.80it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.27it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.74it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Validating:  60%|██████    | 240/400 [00:01<00:01, 144.27it/s]\u001b[A\n",
      "Epoch 27:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.19it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.66it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  91%|█████████ | 1088/1200 [00:24<00:02, 44.12it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.58it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  93%|█████████▎| 1120/1200 [00:24<00:01, 44.99it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.44it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.88it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.31it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.74it/s, loss=1.81, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 27: 100%|██████████| 1200/1200 [00:25<00:00, 46.97it/s, loss=1.81, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  67%|██████▋   | 800/1200 [00:22<00:11, 35.86it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:52,  3.55it/s]\u001b[A\n",
      "Epoch 28:  68%|██████▊   | 816/1200 [00:22<00:10, 35.86it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  69%|██████▉   | 832/1200 [00:22<00:10, 36.36it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  71%|███████   | 848/1200 [00:22<00:09, 36.88it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  72%|███████▏  | 864/1200 [00:23<00:08, 37.38it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  73%|███████▎  | 880/1200 [00:23<00:08, 37.88it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  75%|███████▍  | 896/1200 [00:23<00:07, 38.38it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Validating:  24%|██▍       | 96/400 [00:01<00:02, 122.36it/s]\u001b[A\n",
      "Epoch 28:  76%|███████▌  | 912/1200 [00:23<00:07, 38.85it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  77%|███████▋  | 928/1200 [00:23<00:06, 39.35it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  79%|███████▊  | 944/1200 [00:23<00:06, 39.84it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  80%|████████  | 960/1200 [00:23<00:05, 40.33it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  81%|████████▏ | 976/1200 [00:23<00:05, 40.82it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  83%|████████▎ | 992/1200 [00:24<00:05, 41.30it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.78it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.26it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.74it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.20it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.65it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  91%|█████████ | 1088/1200 [00:24<00:02, 44.10it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.56it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  93%|█████████▎| 1120/1200 [00:24<00:01, 44.99it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.43it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.87it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.29it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.73it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Epoch 28: 100%|██████████| 1200/1200 [00:25<00:00, 47.14it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.135]\n",
      "Validating: 100%|██████████| 400/400 [00:03<00:00, 141.03it/s]\u001b[AEpoch    29: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 28: 100%|██████████| 1200/1200 [00:25<00:00, 46.94it/s, loss=1.87, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  67%|██████▋   | 800/1200 [00:22<00:11, 35.90it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:46,  3.75it/s]\u001b[A\n",
      "Epoch 29:  68%|██████▊   | 816/1200 [00:22<00:10, 35.92it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  69%|██████▉   | 832/1200 [00:22<00:10, 36.46it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  71%|███████   | 848/1200 [00:22<00:09, 36.98it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  72%|███████▏  | 864/1200 [00:23<00:08, 37.50it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  73%|███████▎  | 880/1200 [00:23<00:08, 38.00it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  75%|███████▍  | 896/1200 [00:23<00:07, 38.51it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  76%|███████▌  | 912/1200 [00:23<00:07, 39.01it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  77%|███████▋  | 928/1200 [00:23<00:06, 39.50it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  79%|███████▊  | 944/1200 [00:23<00:06, 40.00it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  80%|████████  | 960/1200 [00:23<00:05, 40.48it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  81%|████████▏ | 976/1200 [00:23<00:05, 40.97it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  83%|████████▎ | 992/1200 [00:23<00:05, 41.46it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.93it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.40it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.87it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Validating:  60%|██████    | 240/400 [00:01<00:01, 145.80it/s]\u001b[A\n",
      "Epoch 29:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.33it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.79it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  91%|█████████ | 1088/1200 [00:24<00:02, 44.26it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.69it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  93%|█████████▎| 1120/1200 [00:24<00:01, 45.13it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.57it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  96%|█████████▌| 1152/1200 [00:25<00:01, 46.01it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.45it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.88it/s, loss=1.78, v_num=3, val_loss=6.110, val_acc=0.133]\n",
      "Epoch 29: 100%|██████████| 1200/1200 [00:25<00:00, 47.07it/s, loss=1.78, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  67%|██████▋   | 800/1200 [00:22<00:11, 35.88it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:47,  3.70it/s]\u001b[A\n",
      "Epoch 30:  68%|██████▊   | 816/1200 [00:22<00:10, 35.92it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  69%|██████▉   | 832/1200 [00:22<00:10, 36.46it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  71%|███████   | 848/1200 [00:22<00:09, 37.00it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  72%|███████▏  | 864/1200 [00:23<00:08, 37.51it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  73%|███████▎  | 880/1200 [00:23<00:08, 38.04it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  75%|███████▍  | 896/1200 [00:23<00:07, 38.55it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  76%|███████▌  | 912/1200 [00:23<00:07, 39.06it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  77%|███████▋  | 928/1200 [00:23<00:06, 39.55it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  79%|███████▊  | 944/1200 [00:23<00:06, 40.04it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  80%|████████  | 960/1200 [00:23<00:05, 40.53it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  81%|████████▏ | 976/1200 [00:23<00:05, 41.02it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  83%|████████▎ | 992/1200 [00:23<00:05, 41.49it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.97it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.44it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.91it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.38it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.85it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  91%|█████████ | 1088/1200 [00:24<00:02, 44.31it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.76it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  93%|█████████▎| 1120/1200 [00:24<00:01, 45.18it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.62it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  96%|█████████▌| 1152/1200 [00:25<00:01, 46.07it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.50it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Epoch 30:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.92it/s, loss=1.72, v_num=3, val_loss=6.070, val_acc=0.134]\n",
      "Validating:  96%|█████████▌| 384/400 [00:02<00:00, 141.03it/s]\u001b[A\n",
      "Epoch 30: 100%|██████████| 1200/1200 [00:25<00:00, 47.14it/s, loss=1.72, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  67%|██████▋   | 800/1200 [00:22<00:11, 35.49it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:49,  3.64it/s]\u001b[A\n",
      "Epoch 31:  68%|██████▊   | 816/1200 [00:22<00:10, 35.50it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  69%|██████▉   | 832/1200 [00:23<00:10, 36.03it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  71%|███████   | 848/1200 [00:23<00:09, 36.54it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  72%|███████▏  | 864/1200 [00:23<00:09, 37.05it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  73%|███████▎  | 880/1200 [00:23<00:08, 37.55it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  75%|███████▍  | 896/1200 [00:23<00:07, 38.06it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  76%|███████▌  | 912/1200 [00:23<00:07, 38.56it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  77%|███████▋  | 928/1200 [00:23<00:06, 39.04it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  79%|███████▊  | 944/1200 [00:23<00:06, 39.51it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  80%|████████  | 960/1200 [00:23<00:05, 40.01it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  81%|████████▏ | 976/1200 [00:24<00:05, 40.48it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  83%|████████▎ | 992/1200 [00:24<00:05, 40.97it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.44it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  85%|████████▌ | 1024/1200 [00:24<00:04, 41.92it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Validating:  56%|█████▌    | 224/400 [00:01<00:01, 146.10it/s]\u001b[A\n",
      "Epoch 31:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.37it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  88%|████████▊ | 1056/1200 [00:24<00:03, 42.84it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.31it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  91%|█████████ | 1088/1200 [00:24<00:02, 43.76it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.21it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  93%|█████████▎| 1120/1200 [00:25<00:01, 44.66it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  95%|█████████▍| 1136/1200 [00:25<00:01, 45.10it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  96%|█████████▌| 1152/1200 [00:25<00:01, 45.53it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  97%|█████████▋| 1168/1200 [00:25<00:00, 45.97it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.41it/s, loss=1.73, v_num=3, val_loss=6.060, val_acc=0.133]\n",
      "Epoch 31: 100%|██████████| 1200/1200 [00:25<00:00, 46.65it/s, loss=1.73, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  67%|██████▋   | 800/1200 [00:22<00:11, 35.91it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 1/400 [00:00<01:49,  3.65it/s]\u001b[A\n",
      "Epoch 32:  68%|██████▊   | 816/1200 [00:22<00:10, 35.95it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  69%|██████▉   | 832/1200 [00:22<00:10, 36.47it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  71%|███████   | 848/1200 [00:22<00:09, 37.01it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  72%|███████▏  | 864/1200 [00:23<00:08, 37.51it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  73%|███████▎  | 880/1200 [00:23<00:08, 38.03it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  75%|███████▍  | 896/1200 [00:23<00:07, 38.53it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  76%|███████▌  | 912/1200 [00:23<00:07, 39.02it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  77%|███████▋  | 928/1200 [00:23<00:06, 39.52it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  79%|███████▊  | 944/1200 [00:23<00:06, 40.01it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  80%|████████  | 960/1200 [00:23<00:05, 40.50it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  81%|████████▏ | 976/1200 [00:23<00:05, 40.99it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  83%|████████▎ | 992/1200 [00:23<00:05, 41.46it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  84%|████████▍ | 1008/1200 [00:24<00:04, 41.93it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  85%|████████▌ | 1024/1200 [00:24<00:04, 42.41it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Validating:  56%|█████▌    | 224/400 [00:01<00:01, 143.89it/s]\u001b[A\n",
      "Epoch 32:  87%|████████▋ | 1040/1200 [00:24<00:03, 42.86it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  88%|████████▊ | 1056/1200 [00:24<00:03, 43.33it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  89%|████████▉ | 1072/1200 [00:24<00:02, 43.79it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  91%|█████████ | 1088/1200 [00:24<00:02, 44.25it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  92%|█████████▏| 1104/1200 [00:24<00:02, 44.70it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  93%|█████████▎| 1120/1200 [00:24<00:01, 45.15it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  95%|█████████▍| 1136/1200 [00:24<00:01, 45.59it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  96%|█████████▌| 1152/1200 [00:25<00:01, 46.03it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  97%|█████████▋| 1168/1200 [00:25<00:00, 46.44it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32:  99%|█████████▊| 1184/1200 [00:25<00:00, 46.84it/s, loss=1.75, v_num=3, val_loss=6.070, val_acc=0.135]\n",
      "Epoch 32: 100%|██████████| 1200/1200 [00:25<00:00, 47.08it/s, loss=1.75, v_num=3, val_loss=6.080, val_acc=0.133]\n",
      "Epoch 33:  44%|████▍     | 527/1200 [00:14<00:18, 35.69it/s, loss=1.77, v_num=3, val_loss=6.080, val_acc=0.133] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33:  44%|████▍     | 527/1200 [00:15<00:19, 34.92it/s, loss=1.77, v_num=3, val_loss=6.080, val_acc=0.133]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  868.06         \t|  100 %          \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  25.491         \t|34             \t|  866.68         \t|  99.841         \t|\n",
      "run_training_batch                 \t|  0.025641       \t|26928          \t|  690.46         \t|  79.541         \t|\n",
      "optimizer_step_and_closure_0       \t|  0.024795       \t|26928          \t|  667.67         \t|  76.915         \t|\n",
      "training_step_and_backward         \t|  0.015064       \t|26928          \t|  405.65         \t|  46.731         \t|\n",
      "model_backward                     \t|  0.0086394      \t|26928          \t|  232.64         \t|  26.8           \t|\n",
      "model_forward                      \t|  0.0056494      \t|26928          \t|  152.13         \t|  17.525         \t|\n",
      "training_step                      \t|  0.0054442      \t|26928          \t|  146.6          \t|  16.888         \t|\n",
      "evaluation_step_and_end            \t|  0.0064353      \t|13202          \t|  84.959         \t|  9.7872         \t|\n",
      "validation_step                    \t|  0.0063166      \t|13202          \t|  83.391         \t|  9.6066         \t|\n",
      "on_train_batch_end                 \t|  0.00088556     \t|26927          \t|  23.845         \t|  2.747          \t|\n",
      "get_train_batch                    \t|  0.00064359     \t|26928          \t|  17.331         \t|  1.9965         \t|\n",
      "on_validation_end                  \t|  0.25531        \t|34             \t|  8.6804         \t|  0.99997        \t|\n",
      "on_validation_batch_end            \t|  0.00026288     \t|13202          \t|  3.4706         \t|  0.39981        \t|\n",
      "cache_result                       \t|  1.5236e-05     \t|174618         \t|  2.6606         \t|  0.30649        \t|\n",
      "on_batch_start                     \t|  1.8396e-05     \t|26928          \t|  0.49537        \t|  0.057066       \t|\n",
      "on_before_zero_grad                \t|  1.241e-05      \t|26928          \t|  0.33417        \t|  0.038496       \t|\n",
      "on_batch_end                       \t|  1.1958e-05     \t|26927          \t|  0.32199        \t|  0.037093       \t|\n",
      "on_after_backward                  \t|  1.1891e-05     \t|26927          \t|  0.32018        \t|  0.036884       \t|\n",
      "on_train_batch_start               \t|  1.0137e-05     \t|26928          \t|  0.27298        \t|  0.031447       \t|\n",
      "training_step_end                  \t|  8.2993e-06     \t|26928          \t|  0.22348        \t|  0.025745       \t|\n",
      "on_validation_start                \t|  0.0037182      \t|34             \t|  0.12642        \t|  0.014563       \t|\n",
      "on_validation_batch_start          \t|  9.0784e-06     \t|13202          \t|  0.11985        \t|  0.013807       \t|\n",
      "validation_step_end                \t|  7.7485e-06     \t|13202          \t|  0.1023         \t|  0.011784       \t|\n",
      "on_train_epoch_start               \t|  0.001973       \t|34             \t|  0.067083       \t|  0.0077279      \t|\n",
      "on_train_end                       \t|  0.040752       \t|1              \t|  0.040752       \t|  0.0046946      \t|\n",
      "on_train_start                     \t|  0.0039563      \t|1              \t|  0.0039563      \t|  0.00045577     \t|\n",
      "on_epoch_start                     \t|  1.2793e-05     \t|68             \t|  0.00086995     \t|  0.00010022     \t|\n",
      "on_validation_epoch_end            \t|  2.3675e-05     \t|34             \t|  0.00080494     \t|  9.2728e-05     \t|\n",
      "on_epoch_end                       \t|  1.1912e-05     \t|67             \t|  0.00079808     \t|  9.1938e-05     \t|\n",
      "on_train_epoch_end                 \t|  1.5304e-05     \t|33             \t|  0.00050505     \t|  5.8181e-05     \t|\n",
      "on_validation_epoch_start          \t|  9.1018e-06     \t|34             \t|  0.00030946     \t|  3.565e-05      \t|\n",
      "on_fit_start                       \t|  1.8144e-05     \t|1              \t|  1.8144e-05     \t|  2.0902e-06     \t|\n",
      "on_before_accelerator_backend_setup\t|  1.0008e-05     \t|1              \t|  1.0008e-05     \t|  1.1529e-06     \t|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_trainer.fit(classifier, train_dataloader=data.train_dataloader(), val_dataloaders=data.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de7e1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), os.path.join(checkpoint_dir, 'classifier.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6311c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = ResNetClassifier()\n",
    "# net.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'classifier.pth')))\n",
    "net = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aab829a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 13.75%\n"
     ]
    }
   ],
   "source": [
    "net = net.cuda()\n",
    "\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in data.val_dataloader():\n",
    "        images, labels = batch\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0cef77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
