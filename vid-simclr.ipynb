{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c26f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "\n",
    "from PIL import Image\n",
    "from simclr import SimCLR\n",
    "from simclr.modules import NT_Xent, get_resnet\n",
    "from simclr.modules.transformations import TransformsSimCLR\n",
    "from simclr.modules.sync_batchnorm import convert_model\n",
    "from simclr.modules import LARS\n",
    "\n",
    "import resnet\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a220a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split, transform, limit=0):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            root: Location of the dataset folder, usually it is /dataset\n",
    "            split: The split you want to used, it should be one of train, val or unlabeled.\n",
    "            transform: the transform you want to applied to the images.\n",
    "        \"\"\"\n",
    "\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_dir = os.path.join(root, split)\n",
    "        label_path = os.path.join(root, f\"{split}_label_tensor.pt\")\n",
    "\n",
    "        if limit == 0:\n",
    "            self.num_images = len(os.listdir(self.image_dir))\n",
    "        else:\n",
    "            self.num_images = limit\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            self.labels = torch.load(label_path)\n",
    "        else:\n",
    "            self.labels = -1 * torch.ones(self.num_images, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(os.path.join(self.image_dir, f\"{idx}.png\"), 'rb') as f:\n",
    "            img = Image.open(f).convert('RGB')\n",
    "\n",
    "        return self.transform(img), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e242f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYUImageNetDataModule(pl.LightningDataModule):\n",
    "  \n",
    "    def train_dataloader(self):\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        trainset = CustomDataset(root='/dataset', split=\"train\", transform=train_transform)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        eval_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        evalset = CustomDataset(root='/dataset', split=\"val\", transform=eval_transform)\n",
    "        eval_loader = torch.utils.data.DataLoader(evalset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        return eval_loader\n",
    "    \n",
    "    def ssl_train_dataloader(self, batch_size):\n",
    "        unlabeled_dataset = CustomDataset(root='/dataset', split='unlabeled', transform=TransformsSimCLR(96))\n",
    "        unlabeled_dataloader = torch.utils.data.DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        return unlabeled_loader\n",
    "        \n",
    "    def ssl_val_dataloader(self, batch_size):\n",
    "        val_dataset = CustomDataset(root='/dataset', split='val', transform=TransformsSimCLR(96))\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63724ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLearning(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "#         self.hparams = args\n",
    "\n",
    "        # initialize ResNet\n",
    "        self.encoder = resnet.get_custom_resnet18()\n",
    "#         get_resnet(\"resnet18\", pretrained=False)\n",
    "        self.n_features = self.encoder.fc.in_features  # get dimensions of fc layer\n",
    "        self.model = SimCLR(self.encoder, 512, self.n_features)\n",
    "        self.criterion = NT_Xent(\n",
    "            BATCH_SIZE, 0.5, world_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        h_i, h_j, z_i, z_j = self.model(x_i, x_j)\n",
    "        loss = self.criterion(z_i, z_j)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop. It is independent of forward\n",
    "        (x_i, x_j), _ = batch\n",
    "        loss = self.forward(x_i, x_j)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop. It is independent of forward\n",
    "        (x_i, x_j), _ = batch\n",
    "        loss = self.forward(x_i, x_j)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return { 'loss' : loss }\n",
    "\n",
    "    def configure_criterion(self):\n",
    "        criterion = NT_Xent(BATCH_SIZE, 0.5)\n",
    "        return criterion\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        scheduler = None\n",
    "#       \"Adam\":\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=3e-4)\n",
    "    \n",
    "#       \"LARS\"\n",
    "        # optimized using LARS with linear learning rate scaling\n",
    "        # (i.e. LearningRate = 0.3 × BatchSize/256) and weight decay of 10−6.\n",
    "        learning_rate = 0.3 * BATCH_SIZE / 256\n",
    "        optimizer = LARS(\n",
    "            self.model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=0.000001,\n",
    "            exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",
    "        )\n",
    "\n",
    "        # \"decay the learning rate with the cosine decay schedule without restarts\"\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, EPOCHS, eta_min=0, last_epoch=-1\n",
    "        )\n",
    "\n",
    "        if scheduler:\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "        else:\n",
    "            return {\"optimizer\": optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd8684fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b89534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataset = CustomDataset(root='/dataset', split='unlabeled', transform=TransformsSimCLR(96))\n",
    "unlabeled_dataloader = torch.utils.data.DataLoader(unlabeled_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5e14eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CustomDataset(root='/dataset', split='val', transform=TransformsSimCLR(96))\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c8c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = NYUImageNetDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "767dd7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr = ContrastiveLearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "184980af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', save_last=True)\n",
    "\n",
    "trainer = Trainer(gpus=1,deterministic=True, max_epochs=EPOCHS, default_root_dir='/scratch/vvb238/simclr', profiler=\"simple\",\n",
    "                     limit_val_batches= 5, precision=16, benchmark=True, callbacks=[checkpoint_callback], fast_dev_run=False)\n",
    "trainer.sync_batchnorm=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee856451",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | encoder   | ResNet  | 11.2 M\n",
      "1 | model     | SimCLR  | 11.7 M\n",
      "2 | criterion | NT_Xent | 0     \n",
      "--------------------------------------\n",
      "11.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.7 M    Total params\n",
      "46.772    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 2000/2005 [13:51<00:02,  2.41it/s, loss=4.95, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 2002/2005 [13:52<00:01,  2.41it/s, loss=4.95, v_num=1]\n",
      "Validating:  40%|████      | 2/5 [00:01<00:01,  1.56it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 2004/2005 [13:52<00:00,  2.41it/s, loss=4.95, v_num=1]\n",
      "Validating:  80%|████████  | 4/5 [00:01<00:00,  3.44it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2005/2005 [13:54<00:00,  2.40it/s, loss=4.95, v_num=1]\n",
      "Epoch 1: 100%|█████████▉| 2000/2005 [13:57<00:02,  2.39it/s, loss=4.85, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 2002/2005 [13:58<00:01,  2.39it/s, loss=4.85, v_num=1]\n",
      "Validating:  40%|████      | 2/5 [00:01<00:02,  1.34it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 2004/2005 [13:59<00:00,  2.39it/s, loss=4.85, v_num=1]\n",
      "Validating:  80%|████████  | 4/5 [00:01<00:00,  3.08it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2005/2005 [14:00<00:00,  2.39it/s, loss=4.85, v_num=1]\n",
      "Epoch 2: 100%|█████████▉| 2000/2005 [13:57<00:02,  2.39it/s, loss=4.82, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 2002/2005 [13:59<00:01,  2.38it/s, loss=4.82, v_num=1]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:02,  1.17it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 2004/2005 [13:59<00:00,  2.39it/s, loss=4.82, v_num=1]\n",
      "Validating:  80%|████████  | 4/5 [00:02<00:00,  2.80it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2005/2005 [14:00<00:00,  2.38it/s, loss=4.82, v_num=1]\n",
      "Epoch 3: 100%|█████████▉| 2000/2005 [13:58<00:02,  2.39it/s, loss=4.79, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 2002/2005 [13:59<00:01,  2.38it/s, loss=4.79, v_num=1]\n",
      "Validating:  40%|████      | 2/5 [00:01<00:01,  1.57it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 2004/2005 [13:59<00:00,  2.39it/s, loss=4.79, v_num=1]\n",
      "Validating:  80%|████████  | 4/5 [00:01<00:00,  3.10it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 2005/2005 [14:01<00:00,  2.38it/s, loss=4.79, v_num=1]\n",
      "Epoch 4: 100%|█████████▉| 2000/2005 [13:58<00:02,  2.39it/s, loss=4.77, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 2002/2005 [13:59<00:01,  2.38it/s, loss=4.77, v_num=1]\n",
      "Validating:  40%|████      | 2/5 [00:01<00:02,  1.46it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 2004/2005 [13:59<00:00,  2.39it/s, loss=4.77, v_num=1]\n",
      "Validating:  80%|████████  | 4/5 [00:01<00:00,  3.09it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 2005/2005 [14:01<00:00,  2.38it/s, loss=4.77, v_num=1]\n",
      "Epoch 5: 100%|█████████▉| 2000/2005 [13:58<00:02,  2.39it/s, loss=4.76, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 2002/2005 [14:00<00:01,  2.38it/s, loss=4.76, v_num=1]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:02,  1.15it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 2004/2005 [14:00<00:00,  2.38it/s, loss=4.76, v_num=1]\n",
      "Validating:  80%|████████  | 4/5 [00:02<00:00,  2.77it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 2005/2005 [14:01<00:00,  2.38it/s, loss=4.76, v_num=1]\n",
      "Epoch 6: 100%|█████████▉| 2000/2005 [13:57<00:02,  2.39it/s, loss=4.75, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 2002/2005 [13:59<00:01,  2.39it/s, loss=4.75, v_num=1]\n",
      "Validating:  40%|████      | 2/5 [00:01<00:02,  1.45it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 2004/2005 [13:59<00:00,  2.39it/s, loss=4.75, v_num=1]\n",
      "Validating:  80%|████████  | 4/5 [00:01<00:00,  3.24it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 2005/2005 [14:00<00:00,  2.38it/s, loss=4.75, v_num=1]\n",
      "Epoch 7: 100%|█████████▉| 2000/2005 [13:58<00:02,  2.39it/s, loss=4.74, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 2002/2005 [14:00<00:01,  2.38it/s, loss=4.74, v_num=1]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:02,  1.14it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 2004/2005 [14:00<00:00,  2.38it/s, loss=4.74, v_num=1]\n",
      "Validating:  80%|████████  | 4/5 [00:02<00:00,  2.75it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 2005/2005 [14:01<00:00,  2.38it/s, loss=4.74, v_num=1]\n",
      "Epoch 8: 100%|█████████▉| 2000/2005 [13:58<00:02,  2.39it/s, loss=4.73, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 2002/2005 [13:59<00:01,  2.39it/s, loss=4.73, v_num=1]\n",
      "Validating:  40%|████      | 2/5 [00:01<00:02,  1.05it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 2004/2005 [14:00<00:00,  2.39it/s, loss=4.73, v_num=1]\n",
      "Validating:  80%|████████  | 4/5 [00:02<00:00,  2.57it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 2005/2005 [14:01<00:00,  2.38it/s, loss=4.73, v_num=1]\n",
      "Epoch 9: 100%|█████████▉| 2000/2005 [13:58<00:02,  2.38it/s, loss=4.73, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 2002/2005 [13:59<00:01,  2.38it/s, loss=4.73, v_num=1]\n",
      "Validating:  40%|████      | 2/5 [00:01<00:01,  1.70it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 2004/2005 [14:00<00:00,  2.39it/s, loss=4.73, v_num=1]\n",
      "Validating:  80%|████████  | 4/5 [00:01<00:00,  3.72it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 2005/2005 [14:01<00:00,  2.38it/s, loss=4.73, v_num=1]\n",
      "Epoch 9: 100%|██████████| 2005/2005 [14:01<00:00,  2.38it/s, loss=4.73, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  8408.5         \t|  100 %          \t|\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  840.5          \t|10             \t|  8405.0         \t|  99.958         \t|\n",
      "run_training_batch                 \t|  0.41557        \t|20000          \t|  8311.4         \t|  98.845         \t|\n",
      "optimizer_step_and_closure_0       \t|  0.41526        \t|20000          \t|  8305.1         \t|  98.771         \t|\n",
      "training_step_and_backward         \t|  0.12684        \t|20000          \t|  2536.8         \t|  30.169         \t|\n",
      "model_forward                      \t|  0.11646        \t|20000          \t|  2329.2         \t|  27.701         \t|\n",
      "training_step                      \t|  0.11626        \t|20000          \t|  2325.2         \t|  27.653         \t|\n",
      "model_backward                     \t|  0.0096459      \t|20000          \t|  192.92         \t|  2.2943         \t|\n",
      "on_train_batch_end                 \t|  0.0018061      \t|20000          \t|  36.123         \t|  0.4296         \t|\n",
      "get_train_batch                    \t|  0.0010671      \t|20000          \t|  21.343         \t|  0.25383        \t|\n",
      "evaluation_step_and_end            \t|  0.11237        \t|52             \t|  5.8433         \t|  0.069493       \t|\n",
      "validation_step                    \t|  0.11222        \t|52             \t|  5.8354         \t|  0.069399       \t|\n",
      "cache_result                       \t|  7.123e-06      \t|100276         \t|  0.71426        \t|  0.0084946      \t|\n",
      "on_validation_batch_end            \t|  0.010855       \t|52             \t|  0.56447        \t|  0.0067132      \t|\n",
      "on_batch_start                     \t|  1.5094e-05     \t|20000          \t|  0.30187        \t|  0.0035901      \t|\n",
      "on_before_zero_grad                \t|  1.2696e-05     \t|20000          \t|  0.25391        \t|  0.0030198      \t|\n",
      "on_batch_end                       \t|  1.2524e-05     \t|20000          \t|  0.25048        \t|  0.0029789      \t|\n",
      "on_after_backward                  \t|  1.0903e-05     \t|20000          \t|  0.21807        \t|  0.0025934      \t|\n",
      "training_step_end                  \t|  9.5066e-06     \t|20000          \t|  0.19013        \t|  0.0022612      \t|\n",
      "on_train_batch_start               \t|  8.2197e-06     \t|20000          \t|  0.16439        \t|  0.0019551      \t|\n",
      "on_train_end                       \t|  0.075907       \t|1              \t|  0.075907       \t|  0.00090275     \t|\n",
      "on_validation_end                  \t|  0.0021039      \t|11             \t|  0.023143       \t|  0.00027524     \t|\n",
      "on_validation_start                \t|  0.001962       \t|11             \t|  0.021582       \t|  0.00025667     \t|\n",
      "on_train_epoch_start               \t|  0.00087424     \t|10             \t|  0.0087424      \t|  0.00010397     \t|\n",
      "on_train_start                     \t|  0.0021265      \t|1              \t|  0.0021265      \t|  2.5291e-05     \t|\n",
      "on_validation_batch_start          \t|  2.5017e-05     \t|52             \t|  0.0013009      \t|  1.5471e-05     \t|\n",
      "validation_step_end                \t|  1.0008e-05     \t|52             \t|  0.00052041     \t|  6.1892e-06     \t|\n",
      "on_epoch_start                     \t|  1.1067e-05     \t|21             \t|  0.00023241     \t|  2.764e-06      \t|\n",
      "on_epoch_end                       \t|  1.0041e-05     \t|21             \t|  0.00021086     \t|  2.5077e-06     \t|\n",
      "on_validation_epoch_end            \t|  1.5379e-05     \t|11             \t|  0.00016917     \t|  2.0119e-06     \t|\n",
      "on_train_epoch_end                 \t|  1.6462e-05     \t|10             \t|  0.00016462     \t|  1.9578e-06     \t|\n",
      "on_validation_epoch_start          \t|  8.9283e-06     \t|11             \t|  9.8212e-05     \t|  1.168e-06      \t|\n",
      "on_fit_start                       \t|  1.5456e-05     \t|1              \t|  1.5456e-05     \t|  1.8382e-07     \t|\n",
      "on_before_accelerator_backend_setup\t|  7.8436e-06     \t|1              \t|  7.8436e-06     \t|  9.3282e-08     \t|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(simclr, train_dataloader=data.ssl_train_dataloader(BATCH_SIZE), val_dataloaders=data.ssl_val_dataloader(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9b0f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"/scratch/vvb238/simclr/simclr.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6323fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"/scratch/vvb238/simclr\"\n",
    "torch.save(simclr.model.encoder.state_dict(), os.path.join(checkpoint_dir, 'simclr_encoder.pth'))\n",
    "torch.save(simclr.model.projector.state_dict(), os.path.join(checkpoint_dir, 'simclr_projector.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e717eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning on labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32b88f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = resnet.get_custom_resnet18()\n",
    "        self.encoder.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'simclr_encoder.pth')))\n",
    "        self.lastLayer = torch.nn.Linear(512, 800)\n",
    "        self.criterion=torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.lastLayer(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, label = batch\n",
    "        classProbs = self.forward(x)\n",
    "        loss = self.criterion(classProbs, label)\n",
    "        return { 'loss' : loss, 'prediction' : classProbs, 'target' : label }\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        data, label = batch\n",
    "        classProbs = self.forward(x)\n",
    "        loss = self.criterion(classProbs, label)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return { 'loss' : loss, 'prediction' : classProbs, 'target' : label }\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n",
    "        return ({'optimizer': optimizer, 'lr_scheduler': scheduler})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86e619cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"fc.weight\", \"fc.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9d1d94d5ce8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-70492921613c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_custom_resnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'simclr_encoder.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastLayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/envs/dev/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1224\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"fc.weight\", \"fc.bias\". "
     ]
    }
   ],
   "source": [
    "classifier = ResNetClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 60\n",
    "trainer = Trainer(gpus=1,deterministic=True, max_epochs=EPOCHS, default_root_dir='/scratch/vvb238/simclr', profiler=\"simple\",\n",
    "                     limit_val_batches= 5, precision=16, benchmark=True, callbacks=[checkpoint_callback], fast_dev_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(classifier, train_dataloader=data.train_dataloader(), val_dataloaders=data.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce9b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), os.path.join(checkpoint_dir, 'classifier.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNetClassifier()\n",
    "net.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'classifier.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()\n",
    "\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in evalloader:\n",
    "        images, labels = data\n",
    "\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {(100 * correct / total):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
